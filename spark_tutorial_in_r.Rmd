# Spark Tutorial in R

Yunhan Jin and Zeyu Jin

Why using Spark?

When working with small-scale datasets, within the memory limit, we can perform all those steps from R, without using Spark. However, when data does not fit in memory or computation is simply too slow, we can slightly modify this approach by incorporating Spark.

Spark is a parallel computation engine that works at a large scale and provides a SQL engine and modeling libraries. It can perform operations including data selection, transformation, and modeling. Spark also includes tools for performing specialized computational work like graph analysis, stream processing, and many others.

### Getting started with Spark
1. Load required packages
```{r}
library(sparklyr)
library(DBI)
library(dplyr)
library(ggplot2)
library(corrr)
library(dbplot)
library(rmarkdown)
library(ggmosaic)
library(forcats)
library(FactoMineR)
library(tidyr)
```

2. Install Spark and connect to the local cluster
```{r}
# install Spark, these two lines should only run once!!!
spark_install("3.0")
spark_available_versions()

# connect the spark to local cluster
sc <- spark_connect(master="local", version="3.0")
```

3. Use spark to load dataset from R
```{r}
# example - mtcars
head(mtcars)
# copy mtcars dataset into Apache Spark by using copy_to()
cars <- copy_to(sc, mtcars)
head(cars)
```

4. Load Spark web interface for additional information
```{r}
# most spark commands are run from R, but monitoring and analyzing execution is through Spark's interface, which you can access here
spark_web(sc)
```
In the web interface, you can see the memory of a fully cached dataset we just loaded under storage. Executors tab provides a view of your cluster resources. Environment ists all of the settings for this Spark application.

5. Analysis using Spark from R
```{r}
# Can use SQL (from "DBI" package) / dplyr to analyze dataset in Spark from R

# look at how many records are there in the cars dataset using SQL
dbGetQuery(sc, "SELECT count(*) FROM mtcars")

# look at how many records are there in the cars dataset using dplyr, whic is easier and more compact
count(cars)
```
```{r}
# Simple visualization by selecting, sampline, and plotting the "cars" dataset in Spark
select(cars, hp, mpg) %>%
  sample_n(100) %>%
  collect() %>%
  plot(main="horsepower versus miles per gallon")
```

6. Modeling
```{r}
# linear model for relationship between fuel efficiency and horsepower
linear_model <- ml_linear_regression(cars, mpg~hp)
linear_model
```
```{r}
# we can use this linear model to predict values beyond the original dataset.
# for example, plotting horsepower beyond 250
linear_model %>%
  ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %>%
  transmute(hp = hp, mpg = prediction) %>%
  full_join(select(cars, hp, mpg)) %>%
  collect() %>%
  plot(main="Horsepower versus miles per gallon with predictions")
```

### Analysis in Spark
1. Wrangle

Data wrangling in R is often performed with `dplyr` library. In the following, we provide a quick reference to the most frequently used dplyr functions:

Frequently used `dplyr` functions:
* `summarise()`: apply a **summary** function to a column and return one value
    * summarise(data, new_colname=f(colname))
    * example: 
        * summarise(mtcars, avg=mean(mpg))
        * mtcars %>% summarise(avg=mean(mpg))
    * summary functions:
        * n(): # of values/rows of the input column
        * n_distinct(): # of uniques values of the input column
        * mean(): mean value of the input column
        * median(): median value of the input column
        * sum(): sum of the input column
        * first(): first value of the input column
        * last(): last value of the input column
        * nth(): value in nth location of the input column
        * quantile(): nth quantile of the input column
        * min(): minimum value of the input column
        * max(): maximum value of the input column
        * IQR(): Inter Quantile Range of the input column
        * mad(): medium absolute deviation of the input column
        * sd(): standard deviation of the input column
        * var(): variance of the input column

* `mutate()`: apply a **vectorized** function to a column and return a new column with the same length
    * mutate(data, new_colname=f(colname))
    * example:
        * mutate(mtcars, gpm=1/mpg)
        * mtcars %>% mutate(gpm=1/mgp)
    * vectorized function:
        * lag(): offset elements by 1
        * lead(): offset elements by -1
        * cummax(): cumulative max()
        * cummin(): cummulative min()
        * cummean(): cummulative mean()
        * cumsum(): cummulative sum()

* `filter()`: extract rows that meet logical criteria
    * filter(data, logical_criteria)
    * example:
        * filter(mtcars, mpg>20)
        * mtcars %>% filter(mpg>20)

* `select()`: extract columns that meet logical criteria
    * select(data, logical criteria)
    * example:
        * select(mtcars, starts_with("m"))
        * mtcars %>% select(starts_with("m"))

* `arrange()`: order rows by values of a column (default: low to high), use with desc() to order from high to low
    * arrange(data, colname)
    * example:
        * arrange(mtcars, mpg)
        * mtcars %>% arrange(mpg)
        * arrange(mtcars, desc(mpg))
        * mtcars %>% arrange(desc(mpg))

* `group_by()`: create a "grouped" copy of a table grouped by columns, and then apply summarise on each group (summary functions will manipulate each "group" separately and then combine the results)
    * data %>% group_by(colname) %>% summarise(summary_function)
    * example:
        * mtcars %>$ group_by(cyl) %>% summarise(avg=mean(mpg))

With the above cheatsheet, we can continue to our data wrangling procedures.

```{r}
# can find out the mean of all columns uwing sumarize_all()
summarize_all(cars, mean, na.rm=TRUE)

# group the cars by transmission type
cars %>%
  mutate(transmission = ifelse(am == 0, "automatic", "manual")) %>%
  group_by(transmission) %>%
  summarise_all(mean)
```
```{r}
# there are built-in functions from Hive's SQL that are not in R, but incorporated into Spark SQL
# for example, you can extract percentiles using the function below, and expand them using explode
summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75))) %>%
  mutate(mpg_percentile = explode(mpg_percentile))
```
```{r}
# correlations using Spark on the entire dataset
ml_corr(cars)

# or you can use correlate
correlate(cars, use = "pairwise.complete.obs", method = "pearson") 
```
```{r}
# can pipe the results into other functions. eg. shave() turns all duplicated values into NAs.
correlate(cars, use = "pairwise.complete.obs", method = "pearson") %>%
  shave() %>%
  rplot()
```

2. Visualize
```{r}
# let's first plot inside R
ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col()+ coord_flip() + ggtitle("Plot inside R")

# plot using Spark: group_by and summarise are run inside Spark, and the results are brung back into R for collection
car_group <- cars %>%
  group_by(cyl) %>%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
  collect()

ggplot(aes(as.factor(cyl), mpg), data = car_group) + 
  geom_col() + coord_flip() + ggtitle("Transform using Spark and plot with R")
```
```{r}
# dplot package provides helper functions for plotting with remote data, and it can be translated into Spark

# single var visualization with histogram
cars %>%
dbplot_histogram(mpg, binwidth = 3) +
labs(title = "MPG Distribution",
     subtitle = "Histogram over miles per gallon")
```
```{r}
# two continuous var with scatter plot in R
ggplot(aes(mpg, wt), data = mtcars) + 
  geom_point() +
  labs(title = "Weight over MPG",
     subtitle = "a scatterplot visualizing car weight over miles per gallon")
```
```{r}
# Spark alternative using dbplot is the raster plot with grids of x/y positions
dbplot_raster(cars, mpg, wt, resolution = 20)
```

3. Model in Analysis
```{r}
# linear model summary
cars %>% 
  ml_linear_regression(mpg ~ hp + cyl) %>%
  summary()
```
```{r}
# generalized linear model summary
cars %>% 
  ml_generalized_linear_regression(mpg ~ hp + cyl) %>%
  summary()
```
```{r}
# when working with large dataset, may want to save the transformed results in a new table loaded in Spark memory
cached_cars <- cars %>% 
  mutate(cyl = paste0("cyl_", cyl)) %>%
  compute("cached_cars")

# run the linear regression summary
cached_cars %>%
  ml_linear_regression(mpg ~ .) %>%
  summary()
```

### Modeling in Spark
1. Load example dataset OkCupid, which is a dataset consists of user profile data from an online dating site and contains a diverse set of features, including biographical characteristics such as gender and profession, as well as free text fields related to personal interests.
```{r}
download.file(
  "https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip",
  "okcupid.zip")

unzip("okcupid.zip", exdir = "data")
unlink("okcupid.zip")
```
```{r}
# load the downloaded file
profiles <- read.csv("data/profiles.csv")
head(profiles)
```

Consider the problem: Predict whether someone is actively working—that is, not retired, a student, or unemployed.

2. EDA
```{r}
sc <- spark_connect(master = "local", version = "3.0")

# specify escape = "\"" and options = list(multiline = TRUE) here to accommodate embedded quote characters and newlines in the essay fields.
# convert the height and income columns to numeric types and recode missing values in the string columns.
okc <- spark_read_csv(
  sc, 
  "data/profiles.csv", 
  escape = "\"", 
  memory = FALSE,
  options = list(multiline = TRUE)
) %>%
  mutate(
    height = as.numeric(height),
    income = ifelse(income == "-1", NA, as.numeric(income))
  ) %>%
  mutate(sex = ifelse(is.na(sex), "missing", sex)) %>%
  mutate(drinks = ifelse(is.na(drinks), "missing", drinks)) %>%
  mutate(drugs = ifelse(is.na(drugs), "missing", drugs)) %>%
  mutate(job = ifelse(is.na(job), "missing", job))
```
```{r}
# look at our data
head(okc)
```
```{r}
# now let's add our response variable and look at its distribution
okc <- okc %>%
  mutate(
    not_working = ifelse(job %in% c("student", "unemployed", "retired"), 1 , 0)
  )

okc %>% 
  group_by(not_working) %>% 
  tally()
```

```{r}
# split our data into training and testing
data_splits <- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42)
okc_train <- data_splits$training
okc_test <- data_splits$testing
```
```{r}
# look at distribution of our response variable in the training data
okc_train %>%
  group_by(not_working) %>%
  tally() %>%
  mutate(frac = n / sum(n))
```
```{r}
# explore relationship between 'religion' and the response variable in the training set
prop_data <- okc_train %>%
  mutate(religion = regexp_extract(religion, "^\\\\w+", 0)) %>% 
  group_by(religion, not_working) %>%
  tally() %>%
  group_by(religion) %>%
  summarize(
    count = sum(n),
    prop = sum(not_working * n) / sum(n)
  ) %>%
  mutate(se = sqrt(prop * (1 - prop) / count)) %>%
  collect()

prop_data
```
```{r}
# visualize the distribution using ggplot
prop_data %>%
  ggplot(aes(x = religion, y = prop)) + geom_point(size = 2) +
  geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se),
                width = .1) +
  geom_hline(yintercept = sum(prop_data$prop * prop_data$count) /
                              sum(prop_data$count)) +
  ggtitle("Proportion of individuals not currently employed, by religion")

```
```{r}
# Now, let's look at the relatinoship between predictors: here we choose to look at alcohol use and drug use.
# compute a contingency table using sdf_crosstab
contingency_tbl <- okc_train %>% 
  sdf_crosstab("drinks", "drugs") %>%
  collect()

contingency_tbl
```
```{r, fig.width=5, fig.height=5}
# visualize contingency table using a mosaic plot
contingency_tbl %>%
  rename(drinks = drinks_drugs) %>%
  gather("drugs", "count", missing:sometimes) %>%
  mutate(
    drinks = as_factor(drinks) %>% 
      fct_relevel("missing", "not at all", "rarely", "socially", 
                  "very often", "desperately"),
    drugs = as_factor(drugs) %>%
      fct_relevel("missing", "never", "sometimes", "often")
  ) %>%
  ggplot() +
  geom_mosaic(aes(x = product(drinks, drugs), fill = drinks, 
                  weight = count))
```
```{r}
# we can also perform correspondence analysis using the 'FactoMineR' package, which summarizes the relationship between the high-dimensional factor levels by mapping each level to a point on the plane.
dd_obj <- contingency_tbl %>% 
  tibble::column_to_rownames(var = "drinks_drugs") %>%
  FactoMineR::CA(graph = FALSE)

dd_drugs <-
  dd_obj$row$coord %>%
  as.data.frame() %>%
  mutate(
    label = gsub("_", " ", rownames(dd_obj$row$coord)),
    Variable = "Drugs"
  )

dd_drinks <-
  dd_obj$col$coord %>%
  as.data.frame() %>%
  mutate(
    label = gsub("_", " ", rownames(dd_obj$col$coord)),
    Variable = "Alcohol"
  )
  
ca_coord <- rbind(dd_drugs, dd_drinks)
  
ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, 
                     col = Variable)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_text(aes(label = label)) +
  coord_equal() +
  ggtitle("Correspondence analysis principal coordinates for drug and alcohol use")
```

3. Feature Engineering
```{r}
# normalize the inputs, eg. 'age'
scale_values <- okc_train %>%
  summarize(
    mean_age = mean(age),
    sd_age = sd(age)
  ) %>%
  collect()

scale_values
```
```{r}
# transform and plot the histogram
okc_train <- okc_train %>%
  mutate(scaled_age = (age - !!scale_values$mean_age) /
           !!scale_values$sd_age)

dbplot_histogram(okc_train, scaled_age) + ggtitle("Distribution of scaled age")
```
```{r}
# for features that could be multi-selected, we can use dummy variables. eg. race of a person
ethnicities <- c("asian", "middle eastern", "black", "native american", "indian", 
                 "pacific islander", "hispanic / latin", "white", "other")
ethnicity_vars <- ethnicities %>% 
  purrr::map(~ expr(ifelse(like(ethnicity, !!.x), 1, 0))) %>%
  purrr::set_names(paste0("ethnicity_", gsub("\\s|/", "", ethnicities)))
okc_train <- mutate(okc_train, !!!ethnicity_vars)
okc_train %>% 
  select(starts_with("ethnicity_")) %>%
  glimpse()
```
```{r}
# for free text fields, a  way to extract features is counting the total number of characters.
# store the train dataset in Spark’s memory with compute() to speed up computation.
okc_train <- okc_train %>%
  mutate(
    essay_length = char_length(paste(!!!syms(paste0("essay", 0:9))))
  ) %>% compute()

dbplot_histogram(okc_train, essay_length, bins = 100) + ggtitle("Distribution of essay length")
```

4. Supvervised Learning
Since we are dealing with a binary classification problem, the metrics we can use include accuracy, precision, sensitivity, and area under the receiver operating characteristic curve (ROC AUC), among others. For this exercise, we will focus on the ROC AUC.

```{r}
# perform 10-fold cross-validation
vfolds <- sdf_random_split(
  okc_train,
  weights = purrr::set_names(rep(0.1, 10), paste0("fold", 1:10)),
  seed = 42
)
```
```{r}
# first analysis/assessment split
analysis_set <- do.call(rbind, vfolds[2:10])
assessment_set <- vfolds[[1]]

# scale on the analysis set only and apply the same transformation to both sets
make_scale_age <- function(analysis_data) {
  scale_values <- analysis_data %>%
    summarize(
      mean_age = mean(age),
      sd_age = sd(age)
    ) %>%
    collect()

  function(data) {
    data %>%
      mutate(scaled_age = (age - !!scale_values$mean_age) / !!scale_values$sd_age)
  }
}

scale_age <- make_scale_age(analysis_set)
train_set <- scale_age(analysis_set)
validation_set <- scale_age(assessment_set)
```
```{r}
# use logistic regression
lr <- ml_logistic_regression(
  analysis_set, not_working ~ scaled_age + sex + drinks + drugs + essay_length
)
lr
```
```{r}
# summary of performance on the assessment set
validation_summary <- ml_evaluate(lr, assessment_set)
validation_summary
```
```{r}
# plot ROC curve
roc <- validation_summary$roc() %>%
  collect()

ggplot(roc, aes(x = FPR, y = TPR)) +
  geom_line() + geom_abline(lty = "dashed") +
  ggtitle("ROC curve for the logistic regression model")
```
The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1–specificity) for varying values of the classification threshold. The AUC is a summary measure for determining the quality of a model, and we can compute it by calling the area_under_roc() function.
```{r}
# AUC value
validation_summary$area_under_roc()
```
```{r}
# we just did one fold in the previous part. Now, we will apply it to each analysis/assessment split
cv_results <- purrr::map_df(1:10, function(v) {
  analysis_set <- do.call(rbind, vfolds[setdiff(1:10, v)]) %>% compute()
  assessment_set <- vfolds[[v]]
  
  scale_age <- make_scale_age(analysis_set)
  train_set <- scale_age(analysis_set)
  validation_set <- scale_age(assessment_set)
  
  model <- ml_logistic_regression(
    analysis_set, not_working ~ scaled_age + sex + drinks + drugs + essay_length
  )
  s <- ml_evaluate(model, assessment_set)
  roc_df <- s$roc() %>% 
    collect()
  auc <- s$area_under_roc()
  
  tibble(
    Resample = paste0("Fold", stringr::str_pad(v, width = 2, pad = "0")),
    roc_df = list(roc_df),
    auc = auc
  )
})
```
```{r}
# and we have 10 ROC curves from the 10 folds
unnest(cv_results, roc_df) %>%
  ggplot(aes(x = FPR, y = TPR, color = Resample)) +
  geom_line() + geom_abline(lty = "dashed")
```
```{r}
# mean value for AUC metric
mean(cv_results$auc)
```

Generalized Linear Model
```{r}
# fit a logistic regression via the generalized linear regression interface by specifying family = "binomial"
glr <- ml_generalized_linear_regression(
  analysis_set, 
  not_working ~ scaled_age + sex + drinks + drugs, 
  family = "binomial"
)

tidy_glr <- tidy(glr)

# create a coefficient plot
tidy_glr %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, 
       ymax = estimate + 1.96 * std.error, width = .1)
  ) +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  ggtitle("Coefficient estimates with 95% confidence intervals")
```

To fit a neural network model we can run the following:
```{r}
# feedforward neural network model with two hidden layers of 64 nodes each
nn <- ml_multilayer_perceptron_classifier(
  analysis_set,
  not_working ~ scaled_age + sex + drinks + drugs + essay_length, 
  layers = c(12, 64, 64, 2)
)
```
```{r}
# obtain predictions on a validation set using ml_predict()
predictions <- ml_predict(nn, assessment_set)

# compute the AUC via ml_binary_classification_evaluator()
ml_binary_classification_evaluator(predictions)
```

5. Unsupervised Learning
```{r}
# look at freeform text that the users entered into their dating profiles.
essay_cols <- paste0("essay", 0:9)
essays <- okc %>%
  select(!!essay_cols)
essays %>% 
  glimpse()
```
```{r}
# remove HTML tags, special characters, and missing character fields
essays <- essays %>%
  # Replace `missing` with empty string.
  mutate_all(list(~ ifelse(. == "missing", "", .))) %>%
  # Concatenate the columns.
  mutate(essay = paste(!!!syms(essay_cols))) %>%
  # Remove miscellaneous characters and HTML tags
  mutate(words = regexp_replace(essay, "\\n|&nbsp;|<[^>]*>|[^A-Za-z|']", " "))
```

Latent Dirichlet Allocation (LDA) is a type of topic model for identifying abstract “topics” in a set of documents. A typical use case for topic models involves categorizing many documents, for which the large number of documents renders manual approaches infeasible.
```{r}
# fit an LDA model with ml_lda()
stop_words <- ml_default_stop_words(sc) %>%
  c(
    "like", "love", "good", "music", "friends", "people", "life",
    "time", "things", "food", "really", "also", "movies"
  )

lda_model <-  ml_lda(essays, ~ words, k = 6, max_iter = 1, min_token_length = 4, 
                     stop_words = stop_words, min_df = 5)
```
```{r}
# use the tidy() function to extract the associated betas, which are the per-topic-per-word probabilities, from the model.
betas <- tidy(lda_model)
head(betas)
```
```{r}
# visualize this output by looking at word probabilities by topic, at 1 iteration
betas %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip() +
    ggtitle("The most common terms per topic in the first iteration")
```

disconnect from Spark
```{r}
spark_disconnect(sc)
```

6. References:
* "Mastering Spark with R": https://therinspark.com/analysis.html#wrangle
* dplyr documentation: https://dplyr.tidyverse.org/
