[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Everyone: let’s add content welcome page.Please edit file submit pull request .","code":""},{"path":"community-contribution.html","id":"community-contribution","chapter":"2 Community Contribution","heading":"2 Community Contribution","text":"fairly open-ended assignment provides opportunity receive credit contributing collective learning class, perhaps beyond. reflect minimum 3 hours work. complete assignment must submit short description contribution. appropriate, attach relevant files.many ways can contribute:organize lead workshop particular topic (date may assignment due date need schedule )help students find final project partnersgive well-rehearsed 5 minute lightning talk class datavis topic (theory tool) (email set date – may assignment due date need schedule )create video tutorial (length)create cheatsheet resourcewrite tutorial tool ’s well documentedbuild viz product (ex. htmlwidget RStudio add-) class use[idea](Note: translations allowed)may draw expand existing resources. , critical cite sources.","code":""},{"path":"community-contribution.html","id":"important-logistics","chapter":"2 Community Contribution","heading":"2.1 IMPORTANT LOGISTICS","text":"","code":""},{"path":"community-contribution.html","id":"groups","chapter":"2 Community Contribution","heading":"2.1.1 Groups","text":"may work partner choosing. work alone, need join group 1, simply submit work CourseWorks solo assignment.work partner, add group CC page People tab. Ed Discussion can used find partners similar interests.","code":""},{"path":"community-contribution.html","id":"what-to-submit","chapter":"2 Community Contribution","heading":"2.1.2 What to submit","text":"cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)","code":""},{"path":"community-contribution.html","id":"submitting-your-assignment","chapter":"2 Community Contribution","heading":"2.1.3 Submitting your assignment","text":"must submit assignment twice: CourseWorks (can graded) class, details follow.CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .Class (GitHub) submission: detail provided separate assignment.Class (GitHub) submission: detail provided separate assignment.","code":""},{"path":"community-contribution.html","id":"grading","chapter":"2 Community Contribution","heading":"2.1.4 Grading","text":"graded quality work, originality, effort invested. sources used must cited.","code":""},{"path":"github-submission-instructions.html","id":"github-submission-instructions","chapter":"3 GitHub submission instructions","heading":"3 GitHub submission instructions","text":"chapter gives information need upload community contribution. Please read entire document carefully making submission. particular note fact bookdown requires different .Rmd format ’re used , must make changes beginning file described submitting.","code":""},{"path":"github-submission-instructions.html","id":"background","chapter":"3 GitHub submission instructions","heading":"3.1 Background","text":"web site makes use bookdown package render collection .Rmd files nicely formatted online book chapters subchapters. job submit slightly modified version community contribution .Rmd file GitHub repository source files web site stored. backend, admins divide chapters book sections order .community contribution different format, create short .Rmd file explains , includes links relevant files, slides, etc. can post GitHub repo (another online site.)","code":""},{"path":"github-submission-instructions.html","id":"preparing-your-.rmd-file","chapter":"3 GitHub submission instructions","heading":"3.2 Preparing your .Rmd file","text":"submit ONE Rmd file.completing modifications, .Rmd look like sample .Rmd.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.second line blank, followed name(s):\n# Base R vs. ggplot2\n\nAaron Burr Alexander Hamilton\n\ncontent starts . second line blank, followed name(s):project requires data, please use built-dataset read directly URL, :\ndf <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.project requires data, please use built-dataset read directly URL, :df <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:\n{r, include=FALSE}\ninstead :\n{r setup, include=FALSE}included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:instead :project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:\n\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must installed sourceIf project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:developed .Rmd file moving library() statements rest file content, highly recommended knit review document . may change namespace available section code development, causing function work exhibit unexpected behavior.file contain getwd() / setwd() calls (never use scripts anyway!) write statements.Want get fancy? See optional tweaks section .","code":"# Base R vs. ggplot2\n\nAaron Burr and Alexander Hamilton\n\nYour content starts here. {r, include=FALSE}{r setup, include=FALSE}\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must be installed from source"},{"path":"github-submission-instructions.html","id":"submission-steps","chapter":"3 GitHub submission instructions","heading":"3.3 Submission steps","text":"submit work, following “Workflow #4” – submitting pull request someone else’s repository write access. Instructions available lecture slides topic well tutorial. repeated abbreviated form, specific instructions naming conventions, content information, important details.Fork cc21fall2 repo (repo) GitHub account.Fork cc21fall2 repo (repo) GitHub account.Clone/download forked repo local computer.Clone/download forked repo local computer.Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:\n![Test Photo](resources/sample_project/pumpkins.jpg)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:![Test Photo](resources/sample_project/pumpkins.jpg)ready submit project, push branch remote repo. Follow tutorial create pull request.ready submit project, push branch remote repo. Follow tutorial create pull request.point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.","code":""},{"path":"github-submission-instructions.html","id":"optional-tweaks","chapter":"3 GitHub submission instructions","heading":"3.4 Optional tweaks","text":"prefer links chapter open new tabs, add {target=\"_blank\"} link, :\n[edav.info](edav.info){target=\"_blank\"}prefer links chapter open new tabs, add {target=\"_blank\"} link, :[edav.info](edav.info){target=\"_blank\"}Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.","code":""},{"path":"github-submission-instructions.html","id":"faq","chapter":"3 GitHub submission instructions","heading":"3.5 FAQ","text":"","code":""},{"path":"github-submission-instructions.html","id":"what-should-i-expect-after-creating-a-pull-request","chapter":"3 GitHub submission instructions","heading":"3.5.1 What should I expect after creating a pull request?","text":"Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.take time can process pull requests, long see pull request repo, don’t worry.take time can process pull requests, long see pull request repo, don’t worry.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-before-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.2 What if I catch mistakes before my pull request is merged?","text":"Just make changes branch, commit push GitHub. automatically added pull request.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-after-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.3 What if I catch mistakes after my pull request is merged?","text":"may submit additional pull requests fix material site. edits small, fixing typos, easiest make edits directly GitHub, following instructions. merge first pull requests edits, please patient.","code":""},{"path":"github-submission-instructions.html","id":"other-questions","chapter":"3 GitHub submission instructions","heading":"3.5.4 Other questions","text":"additional questions, please ask Discussions section respond.Thank contributions!","code":""},{"path":"sample-project.html","id":"sample-project","chapter":"4 Sample project","heading":"4 Sample project","text":"","code":""},{"path":"sample-project.html","id":"happy-halloween","chapter":"4 Sample project","heading":"4.1 HAPPY HALLOWEEN!","text":"Test Photo","code":""},{"path":"statistics-in-r-vs-python.html","id":"statistics-in-r-vs-python","chapter":"5 Statistics in R vs Python","heading":"5 Statistics in R vs Python","text":"Weiyi Jiang Sicheng LiPython R two popular coding languages Data Science. project, propose comparison Python R different kinds scenarios, including generating random numbers, time series analysis basic machine learning. Furthermore, visualize comparizon cheat sheet reference. project, conducted following R Python:\n1. Several random number generating methods including generating normal distribution uniform distribution.\n2. Time Series Analysis tools including ARIMA model, acf/pacf parameters, adfuller test Granger Causality test.\n3. Machine Learning tools including Linear regression, Lasso, Ridge, SVM, Decision Tree Random Forest.link:https://github.com/serendipitylee/cc_group1","code":""},{"path":"data-visualization-with-seaborn.html","id":"data-visualization-with-seaborn","chapter":"6 Data Visualization with Seaborn","heading":"6 Data Visualization with Seaborn","text":"Xinyu Huang (xh2511)created cheat sheet data visualization Seaborn, package python. Including data preparation, plot aesthetics, categorical data plotting, statistical relationships plotting customizations.See two pdf files ‘https://github.com/Elliot1h/EDAV-cc’ cheat sheet citations. can also found ‘resources/python_seaborn_cheatsheet’.","code":""},{"path":"how-to-make-your-own-r-package.html","id":"how-to-make-your-own-r-package","chapter":"7 How to Make Your Own R package","heading":"7 How to Make Your Own R package","text":"Yuren DongIn project introduced procedures integrating functions create packages. knowledge, able save time integrating commonly used data processing workflow packages,share analytical pipelines collaborators. Thus, think great opportunity share classmates strategy.link: https://github.com/yunyuntian/make_r_package_cheatsheet/","code":""},{"path":"rchess-package-cheatsheet.html","id":"rchess-package-cheatsheet","chapter":"8 RChess package cheatsheet","heading":"8 RChess package cheatsheet","text":"Ju Hyun JeonHow can one analyze chess games R? several packages help users understand chess positions R many resources. cheatsheet made help people use RChess.\nRChess approachable convenient way processing Chess data R. Furthermore, can used conjunction ggplot2 plot chess positions. cheatsheet contains functions example positions help understand package. 4 major uses Rchess functions. 1. importing positions, 2. identifying positions, 3. modifying positions, 4. visualizing positions. study can conducted derive methods process annotated PGNs PGNs clock time.cheatsheet can accessed following link.\nhttps://github.com/witchhead/RChessCheatsheet/blob/main/RchessCheatSheet.pdf","code":""},{"path":"hypothesis-testing-cheatsheet.html","id":"hypothesis-testing-cheatsheet","chapter":"9 Hypothesis testing cheatsheet","heading":"9 Hypothesis testing cheatsheet","text":"Weisheng ChenThis PDF version cheat sheet hypothesis testing including key concepts, steps conducting hypothesis testing comparison different tests.Check cheat sheet clicking following Github link:https://github.com/SteveChen2751/GR5702-EDAV/blob/main/Hypothesis_Testing_Cheatsheet.pdf","code":""},{"path":"interactive-web-visualizations-for-r-cheatsheet.html","id":"interactive-web-visualizations-for-r-cheatsheet","chapter":"10 Interactive web visualizations for R cheatsheet\"","heading":"10 Interactive web visualizations for R cheatsheet\"","text":"Tianhang CuiThe content cheatsheet interactive web visualizations R, form PDF. includes make adjust general type interactive web visualization plotly, create special types visualization.many libraries can used R make useful nice-looking interactive visualization. However, easiest best tools different purposes often library (example:3D scatterplot can done ‘plotly’ using ‘threejs’ much easier effective), search internet see package commonly used different kinds interactive visualization. cheat sheets internet specific packages detail, none contains commonly used packages interactive visualization. Thus, think helpful cheat sheet include commonly used packages, statisticians/programmers can easily use needs make interactive visualization based purposes.addresses need statisticians/programmers want quickly check make interactive visualization data given requirement. statistics/programmers often just need simple use make interactive graph (tooltips, mouse-click events, etc.) interactive visualization packages contains function default, need include basic use package used. functions needed, user can also quickly find ways checking package document.Page1Page2To check cheatsheet pdf form, click link :\nhttp://tianhangcui.com/cheatsheet_vis_for_R/Cheatsheet_Interactive_web_visualizations_for_R.pdf","code":""},{"path":"ggforce.html","id":"ggforce","chapter":"11 ggforce","heading":"11 ggforce","text":"Rona Xuggforce ‘ggplot2’ extension aims aid data visualization. cheat sheet created includes several major features\npackage functions found helpful.Link: https://github.com/rona-x/ggforce_cheatsheet.git","code":""},{"path":"gganimate-cheatsheet.html","id":"gganimate-cheatsheet","chapter":"12 GGanimate cheatsheet","heading":"12 GGanimate cheatsheet","text":"Siyu Li","code":"\nlibrary(ggplot2)\n# remotes::install_github('thomasp85/gganimate')\n# remotes::install_github('thomasp85/transformr')\nlibrary(gganimate)# must be installed from source"},{"path":"gganimate-cheatsheet.html","id":"pdf-cheatsheet","chapter":"12 GGanimate cheatsheet","heading":"12.1 PDF cheatsheet","text":"Please click link:\ngganimate_cheatsheet","code":""},{"path":"gganimate-cheatsheet.html","id":"introduction","chapter":"12 GGanimate cheatsheet","heading":"12.2 Introduction","text":"gganimate extends grammar graphics \nimplemented ggplot2 include description animation. providing \nrange new grammar classes can added\nplot object order customise \nchange time.","code":""},{"path":"gganimate-cheatsheet.html","id":"installation","chapter":"12 GGanimate cheatsheet","heading":"12.3 Installation","text":"NOTE: May also need install ‘gifski’ ‘av’ package.","code":"install.packages('devtools')\ndevtools::install_github('thomasp85/gganimate')\ndevtools::install_github('thomasp85/transformr')"},{"path":"gganimate-cheatsheet.html","id":"basic-function","chapter":"12 GGanimate cheatsheet","heading":"12.4 Basic function","text":"• transition_∗():\ndefines data spread \nrelates across time.• view_∗():\ndefines positional scales \nchange along animation.• shadow_∗():\ndefines data points time\npresented given point time.• enter_∗()/exit_∗():\ndefines new data appear \nold data disappear \ncourse animation.• ease_aes():\ndefines different aesthetics \neased transitions.• animate():\nrender gganimate object.• anim_save():\nsave animation file.","code":"p <- ggplot()+\n   geom_point()+//or other kinds of graph\n   transition_states(states, transition_length,\nstate_length)+\n   view_follow(fixed_x,fixed_y)+\n   shadow_wake(wake_length,size,alpha)\n   enter_fade()+\n   exit_shrink()+\n   ease_aes(default=’linear’)\nanimate(p)\nanim_save(filename,path)"},{"path":"gganimate-cheatsheet.html","id":"transition","chapter":"12 GGanimate cheatsheet","heading":"12.4.1 Transition","text":"","code":""},{"path":"gganimate-cheatsheet.html","id":"transition_states","chapter":"12 GGanimate cheatsheet","heading":"12.4.1.1 transition_states","text":"Transition several distinct stages data","code":"\nanim<-ggplot(iris, aes(Sepal.Width, Petal.Width)) +\ngeom_point() +\nlabs(title = \"{closest_state}\") +\ntransition_states(Species, transition_length = 3, state_length = 1)\nanimate(anim)"},{"path":"gganimate-cheatsheet.html","id":"transition_filter","chapter":"12 GGanimate cheatsheet","heading":"12.4.1.2 transition_filter","text":"Transition different filters","code":"\nanim<-ggplot(iris,aes(Petal.Width,Petal.Length,\ncolour=Species))+\ngeom_point()+\ntransition_filter(\ntransition_length=2,filter_length = 1,\nSetosa=Species=='setosa',\nLong = Petal.Length>4,\nWide = Petal.Width>2)\nanimate(anim)"},{"path":"gganimate-cheatsheet.html","id":"transition_layers","chapter":"12 GGanimate cheatsheet","heading":"12.4.1.3 transition_layers","text":"Build plot layer layer","code":"\nggplot(mtcars, aes(mpg, disp)) +\ngeom_point() +\ngeom_smooth(colour = 'grey', se = FALSE) +\ngeom_smooth(aes(colour = factor(gear))) +\ntransition_layers(layer_length = 1, transition_length = 2,\nfrom_blank = FALSE, layer_order = c(3, 1, 2)) +\nenter_fade() + enter_grow()"},{"path":"gganimate-cheatsheet.html","id":"transition_revealtransition_time","chapter":"12 GGanimate cheatsheet","heading":"12.4.1.4 transition_reveal&transition_time","text":"","code":"\nanim<-ggplot(airquality,aes(Day,Temp,group=\nMonth))+geom_line()+\ngeom_point(aes(group=seq_along(Day)),size\n=3,color='red')+\ntransition_reveal(Day)\nanimate(anim)\nanim<-ggplot(airquality,aes(Day,Temp))+\ngeom_point(aes(colour=factor(Month)))+\ntransition_time(Day)\nanimate(anim)"},{"path":"gganimate-cheatsheet.html","id":"shadow","chapter":"12 GGanimate cheatsheet","heading":"12.4.2 Shadow","text":"","code":""},{"path":"gganimate-cheatsheet.html","id":"shadow_mark","chapter":"12 GGanimate cheatsheet","heading":"12.4.2.1 shadow_mark","text":"Show original data background","code":"\nanim <- ggplot(airquality,aes(Day,Temp,colour = factor(Month))) +\ngeom_point() +\ntransition_time(Day)\nanim1<-anim+shadow_mark(colour='black',size = 0.75,past = TRUE, future = FALSE)\nanimate(anim1)"},{"path":"gganimate-cheatsheet.html","id":"shadow_trail","chapter":"12 GGanimate cheatsheet","heading":"12.4.2.2 shadow_trail","text":"trail evenly spaced old frames","code":"\nanim2 <- anim +\nshadow_trail(distance=0.4,alpha = 0.3, shape = 2)\nanimate(anim2)"},{"path":"gganimate-cheatsheet.html","id":"shadow_weak","chapter":"12 GGanimate cheatsheet","heading":"12.4.2.3 shadow_weak","text":"Show preceding frames gradual falloff","code":"\nanim3<-anim+shadow_wake(wake_length=0.1,size=2,alpha =FALSE,colour='grey92')\nanimate(anim3)"},{"path":"gganimate-cheatsheet.html","id":"view","chapter":"12 GGanimate cheatsheet","heading":"12.4.3 View","text":"","code":""},{"path":"gganimate-cheatsheet.html","id":"view_follow","chapter":"12 GGanimate cheatsheet","heading":"12.4.3.1 view_follow","text":"Let view follow data","code":"\nanim<-ggplot(iris,aes(Sepal.Length, Sepal.Width))+\ngeom_point()+labs(title = \"closest_state\")+\ntransition_states(Species,transition_length=4\n,state_length=1)\nanim1<-anim+view_follow(fixed_x=TRUE,\nfixed_y=FALSE)\nanim2<-anim+view_follow(fixed_x=c(4,NA),\nfixed_y=c(2,NA))\nanimate(anim1)\nanimate(anim2)"},{"path":"gganimate-cheatsheet.html","id":"view_step","chapter":"12 GGanimate cheatsheet","heading":"12.4.3.2 view_step","text":"Follow data steps.NOTE: use view_step relative transition_states. transition doesn’t wrap, view shouldn’t either.","code":"\nanim<-ggplot(iris,aes(Petal.Length,Petal.Width))+\ngeom_point()+\ntransition_states(Species, transition_length=1)+\nview_step(pause_length=2,step_length=1,\nnsteps =3,pause_first=TRUE)\nanimate(anim)"},{"path":"gganimate-cheatsheet.html","id":"animation","chapter":"12 GGanimate cheatsheet","heading":"12.4.4 Animation","text":"","code":"animate(plot, nframes, fps, height, width, duration, detail, renderer, device, ref_frame, start_pause,\nend_pause, rewind,...)\nanim_save(filename, animation=last_animation(), path=NULL, ...)"},{"path":"gganimate-cheatsheet.html","id":"example","chapter":"12 GGanimate cheatsheet","heading":"12.5 Example","text":"","code":"\nggplot(mtcars, aes(factor(cyl), mpg)) +\ngeom_boxplot() +\ntransition_states(\ngear,\ntransition_length = 2,\nstate_length = 1\n) +\nenter_fade() +\nexit_shrink() +\nease_aes('sine-in-out')\nlibrary(gapminder)\nggplot(gapminder, aes(gdpPercap, lifeExp, size =\npop, colour = country)) +\ngeom_point(alpha=0.7,show.legend=FALSE)+\nscale_colour_manual(values=country_colors)+\nscale_size(range = c(2, 12))+scale_x_log10()+\nfacet_wrap(~continent) +\nlabs(title='Year: frame_time', x='GDP per\ncapita', y='life expectancy') +\ntransition_time(year) +\nease_aes('linear')"},{"path":"gganimate-cheatsheet.html","id":"references","chapter":"12 GGanimate cheatsheet","heading":"12.6 References","text":"official githubofficial introduction pdf","code":""},{"path":"statistical-tests-and-parameter-estimations-in-r-cheatsheet.html","id":"statistical-tests-and-parameter-estimations-in-r-cheatsheet","chapter":"13 Statistical Tests and Parameter Estimations in R Cheatsheet","heading":"13 Statistical Tests and Parameter Estimations in R Cheatsheet","text":"Candong ChenThis project includes pdf version cheatsheet Statistical Tests Parameter Estimations R. includes moment estimation, MLE, EM algorithm, distribution estimation, parametric tests, bivariate correlation test, nonparametric tests.Click following link check cheatsheet:\nhttps://github.com/Kimtanyo/EDAV/blob/master/latex/statistical_test_and_parameter_test_in_r.pdf","code":""},{"path":"ggalluvial-cheat-sheet.html","id":"ggalluvial-cheat-sheet","chapter":"14 ggalluvial-cheat-sheet","heading":"14 ggalluvial-cheat-sheet","text":"Qingyi Zhang (qz2419)chosen ggalluvial package topic.First, described ggalluvial function .introduced two aspect since two Format alluvial graph: Wide format Lodes(Long) Format.introducing basic information code, also make changes basic graph can help analyst efficiently analyze data different aspects.cheat sheet link: https://github.com/Seleven711/ggalluvial-cheat-sheet/blob/5bcefce6b22d94e6877ca74726312d4d4d334e77/qz2419cc.pdfif link work, use link instead: https://cheatography.com//seleven/cheat-sheets/ggalluvial/pdf/","code":""},{"path":"introduction-to-plotly.html","id":"introduction-to-plotly","chapter":"15 Introduction to plotly","heading":"15 Introduction to plotly","text":"Moya Zhu Yunshu Cai","code":"\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(vcd)"},{"path":"introduction-to-plotly.html","id":"introduction-1","chapter":"15 Introduction to plotly","heading":"15.1 Introduction","text":"cheat sheet help find proper visualization demostration purpose using plotly. plots interactive documented variable types. package use ‘Plotly’, ‘dplyr’","code":""},{"path":"introduction-to-plotly.html","id":"prepare-install-load","chapter":"15 Introduction to plotly","heading":"15.1.1 Prepare: Install & Load","text":"","code":"\n# After install\nlibrary(plotly)"},{"path":"introduction-to-plotly.html","id":"comparision","chapter":"15 Introduction to plotly","heading":"15.1.2 Comparision","text":"","code":""},{"path":"introduction-to-plotly.html","id":"comparing-over-items-bar-charts","chapter":"15 Introduction to plotly","heading":"15.1.2.1 Comparing over items – Bar Charts","text":"Bar charts aimed show distribution data points see specific group values behave comparing groups.Vertical bar chartsHorizontal bar chartsCustomizing Individual Bar Widths pictorial representation grouped dataCustomizing Individual Bar Colors focusing particular item comparing othersCustomizing Individual Bar Base comparing groups relatively opposite values (ex. income expense, restocking sales)","code":"\nfig <- plot_ly(\n  x = c(\"giraffes\", \"orangutans\", \"monkeys\"),\n  y = c(20, 14, 23),\n  name = \"SF Zoo\",\n  type = \"bar\"\n)\n\nfig\nfig <- plot_ly(x = c(20, 14, 23), y = c('giraffes', 'orangutans', 'monkeys'), type = 'bar', orientation = 'h')\n\nfig\nx= c(1, 2, 3, 5.5, 10)\ny= c(10, 8, 6, 4, 2)\nwidth = c(0.8, 0.8, 0.8, 3.5, 4)\ndata <- data.frame(x, y, width)\n\nfig <- plot_ly(data)\nfig <- fig %>% add_bars(\n    x= ~x,\n    y= ~y,\n    width = ~width\n  )\n\nfig\nx <- c('item A', 'item B', 'item C', 'item D', 'item E')\ny <- c(5, 17, 20, 12, 15)\ndata <- data.frame(x, y)\n\nfig <- plot_ly(data, x = ~x, y = ~y, type = 'bar',\n        marker = list(color = c('rgba(204,204,204,1)', 'rgba(204,204,204,1)',\n                                'rgba(204,204,204,1)', 'rgba(199,77,120,0.8)',\n                                'rgba(204,204,204,1)')))\nfig <- fig %>% layout(title = \"Least Used Features\",\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\"))\n\nfig\nfig <- plot_ly()\nfig <- fig %>% add_bars(\n    x = c(\"2016\", \"2017\", \"2018\"),\n    y = c(500,600,700),\n    base = c(-500,-600,-700),\n    marker = list(\n      color = 'lightpink'\n    ),\n    name = 'expenses'\n  )\nfig <- fig %>% add_bars(\n    x = c(\"2016\", \"2017\", \"2018\"),\n    y = c(300,400,700),\n    base = 0,\n    marker = list(\n      color = 'lightblue'\n    ),\n    name = 'revenue'\n  )\n\nfig"},{"path":"introduction-to-plotly.html","id":"comparing-over-time-line-charts-area-plots","chapter":"15 Introduction to plotly","heading":"15.1.2.2 Comparing over time – Line Charts, Area Plots","text":"Line charts aimed show changes data either short long periods time, compare trend different groups data.Area charts aimed also indicate total value compare differences among variables.Line chart single variable(Styled) Line chart one variablesArea chart single variable(Custom colored) Area chart one variables(Stacked) Area chart one variables suitable displaying part--whole relations showing constituent parts whole one .","code":"\nx <- c(1:100)\nrandom_y <- rnorm(100, mean = 0)\ndata <- data.frame(x, random_y)\n\nfig <- plot_ly(data, x = ~x, y = ~random_y, type = 'scatter', mode = 'lines')\n\nfig\nmonth <- c('January', 'February', 'March', 'April', 'May', 'June', 'July',\n         'August', 'September', 'October', 'November', 'December')\nhigh_2000 <- c(32.5, 37.6, 49.9, 53.0, 69.1, 75.4, 76.5, 76.6, 70.7, 60.6, 45.1, 29.3)\nlow_2000 <- c(13.8, 22.3, 32.5, 37.2, 49.9, 56.1, 57.7, 58.3, 51.2, 42.8, 31.6, 15.9)\nhigh_2007 <- c(36.5, 26.6, 43.6, 52.3, 71.5, 81.4, 80.5, 82.2, 76.0, 67.3, 46.1, 35.0)\nlow_2007 <- c(23.6, 14.0, 27.0, 36.8, 47.6, 57.7, 58.9, 61.2, 53.3, 48.5, 31.0, 23.6)\nhigh_2014 <- c(28.8, 28.5, 37.0, 56.8, 69.7, 79.7, 78.5, 77.8, 74.1, 62.6, 45.3, 39.9)\nlow_2014 <- c(12.7, 14.3, 18.6, 35.5, 49.9, 58.0, 60.0, 58.6, 51.7, 45.2, 32.2, 29.1)\n\ndata <- data.frame(month, high_2000, low_2000, high_2007, low_2007, high_2014, low_2014)\n\n#The default order will be alphabetized unless specified as below:\ndata$month <- factor(data$month, levels = data[[\"month\"]])\n\nfig <- plot_ly(data, x = ~month, y = ~high_2014, name = 'High 2014', type = 'scatter', mode = 'lines',\n        line = list(color = 'rgb(205, 12, 24)', width = 4)) \nfig <- fig %>% add_trace(y = ~low_2014, name = 'Low 2014', line = list(color = 'rgb(22, 96, 167)', width = 4)) \nfig <- fig %>% add_trace(y = ~high_2007, name = 'High 2007', line = list(color = 'rgb(205, 12, 24)', width = 4, dash = 'dash')) \nfig <- fig %>% add_trace(y = ~low_2007, name = 'Low 2007', line = list(color = 'rgb(22, 96, 167)', width = 4, dash = 'dash')) \nfig <- fig %>% add_trace(y = ~high_2000, name = 'High 2000', line = list(color = 'rgb(205, 12, 24)', width = 4, dash = 'dot')) \nfig <- fig %>% add_trace(y = ~low_2000, name = 'Low 2000', line = list(color = 'rgb(22, 96, 167)', width = 4, dash = 'dot')) \nfig <- fig %>% layout(title = \"Average High and Low Temperatures in New York\",\n         xaxis = list(title = \"Months\"),\n         yaxis = list (title = \"Temperature (degrees F)\"))\n\nfig\ndensity <- density(diamonds$carat)\n\nfig <- plot_ly(x = ~density$x, y = ~density$y, type = 'scatter', mode = 'lines', fill = 'tozeroy')\nfig <- fig %>% layout(xaxis = list(title = 'Carat'),\n         yaxis = list(title = 'Density'))\n\nfig\ndiamonds1 <- diamonds[which(diamonds$cut == \"Fair\"),]\ndensity1 <- density(diamonds1$carat)\n\ndiamonds2 <- diamonds[which(diamonds$cut == \"Ideal\"),]\ndensity2 <- density(diamonds2$carat)\n\nfig <- plot_ly(x = ~density1$x, y = ~density1$y, type = 'scatter', mode = 'lines', name = 'Fair cut', fill = 'tozeroy',\n        fillcolor = 'rgba(168, 216, 234, 0.5)',\n        line = list(width = 0.5))\nfig <- fig %>% add_trace(x = ~density2$x, y = ~density2$y, name = 'Ideal cut', fill = 'tozeroy',\n            fillcolor = 'rgba(255, 212, 96, 0.5)')\nfig <- fig %>% layout(xaxis = list(title = 'Carat'),\n         yaxis = list(title = 'Density'))\n\nfig\ndata <- t(USPersonalExpenditure)\ndata <- data.frame(\"year\"=rownames(data), data)\n\nfig <- plot_ly(data, x = ~year, y = ~Food.and.Tobacco, name = 'Food and Tobacco', type = 'scatter', mode = 'none', stackgroup = 'one', fillcolor = '#F5FF8D')\nfig <- fig %>% add_trace(y = ~Household.Operation, name = 'Household Operation', fillcolor = '#50CB86')\nfig <- fig %>% add_trace(y = ~Medical.and.Health, name = 'Medical and Health', fillcolor = '#4C74C9')\nfig <- fig %>% add_trace(y = ~Personal.Care, name = 'Personal Care', fillcolor = '#700961')\nfig <- fig %>% add_trace(y = ~Private.Education, name = 'Private Education', fillcolor = '#312F44')\nfig <- fig %>% layout(title = 'United States Personal Expenditures by Categories',\n         xaxis = list(title = \"\",\n                      showgrid = FALSE),\n         yaxis = list(title = \"Expenditures (in billions of dollars)\",\n                      showgrid = FALSE))\n\nfig"},{"path":"introduction-to-plotly.html","id":"distribution","chapter":"15 Introduction to plotly","heading":"15.1.3 Distribution","text":"One coutinuous variable histogram:One continuous variable histogram density line:multi-continuous variable histogram overlaid:","code":"\nfig <- plot_ly(data = iris, x = ~Sepal.Length,type=\"histogram\")\nfig\ndens<-density(iris$Sepal.Length)\n\nfig <- plot_ly(data = iris,x = ~Sepal.Length,type=\"histogram\", name = \"Histogram\")%>%\n  add_trace(x=dens$x,y=dens$y,mode = \"lines\",type='scatter', fill = \"tozeroy\", yaxis = \"y2\", name = \"Density\") %>% \n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\"))\nfig\nfig <- plot_ly(data=iris,alpha = 0.6)\nfig <- fig %>% add_histogram(x = ~Sepal.Length,name='sepal length')\nfig <- fig %>% add_histogram(x = ~Petal.Length,name='petalvlength')\nfig <- fig %>% add_histogram(x = ~Petal.Width,name='petal width')\nfig <- fig %>% layout(barmode = \"overlay\")\n\nfig"},{"path":"introduction-to-plotly.html","id":"relationship","chapter":"15 Introduction to plotly","heading":"15.1.4 Relationship","text":"","code":""},{"path":"introduction-to-plotly.html","id":"between-two-continuous-variables","chapter":"15 Introduction to plotly","heading":"15.1.4.1 Between two continuous variables:","text":"Scatter plot","code":"\nfig <- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length)\nfig\n#with color:\nfig_color <- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species)\nfig_color"},{"path":"introduction-to-plotly.html","id":"multiple-variables","chapter":"15 Introduction to plotly","heading":"15.1.4.2 Multiple variables:","text":"Bubble size scatter plot indicating relationship 3 variables text hover","code":"\ndata <- read.csv(\"https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv\")\n\nfig <- plot_ly(data, x = ~Women, y = ~Men, text = ~School, type = 'scatter', mode = 'markers',\n        marker = list(size = ~Gap, opacity = 0.5, color = 'rgb(255, 65, 54)'))\nfig <- fig %>% layout(title = 'Gender Gap in Earnings per University',\n         xaxis = list(showgrid = FALSE),\n         yaxis = list(showgrid = FALSE))\nfig"},{"path":"introduction-to-plotly.html","id":"composition","chapter":"15 Introduction to plotly","heading":"15.1.5 Composition","text":"","code":""},{"path":"introduction-to-plotly.html","id":"changing-over-time","chapter":"15 Introduction to plotly","heading":"15.1.5.1 Changing over time","text":"Cumulative values:\nStacked area chart","code":"\ndata <- t(USPersonalExpenditure)\ndata <- data.frame(\"year\"=rownames(data), data)\n\nfig <- plot_ly(data, x = ~year, y = ~Food.and.Tobacco, name = 'Food and Tobacco', type = 'scatter', mode = 'none', stackgroup = 'one', groupnorm = 'percent', fillcolor = '#F5FF8D')\nfig <- fig %>% add_trace(y = ~Household.Operation, name = 'Household Operation', fillcolor = '#50CB86')\nfig <- fig %>% add_trace(y = ~Medical.and.Health, name = 'Medical and Health', fillcolor = '#4C74C9')\nfig <- fig %>% add_trace(y = ~Personal.Care, name = 'Personal Care', fillcolor = '#700961')\nfig <- fig %>% add_trace(y = ~Private.Education, name = 'Private Education', fillcolor = '#312F44')\nfig <- fig %>% layout(title = 'United States Personal Expenditures by Categories',\n         xaxis = list(title = \"\",\n                      showgrid = FALSE),\n         yaxis = list(title = \"Proportion from the Total Expenditures\",\n                      showgrid = FALSE,\n                      ticksuffix = '%'))\n\nfig"},{"path":"introduction-to-plotly.html","id":"static","chapter":"15 Introduction to plotly","heading":"15.1.5.2 Static","text":"Single categorical variable:\npie chart:Multi-categorical variables:\nStacked bar plot:Hierarchical data:\nMulti-categorical:\nTree map displays hierarchical data sets nested rectangles. represent branches dimension data.sequential data\nwaterfall chart helps understanding cumulative effect sequential values. represent accumulation subtraction total. ’s great representation lots financial data.","code":"\nfig <- plot_ly()\nfig <- fig %>% add_pie(data = count(iris,Species), labels = ~Species, values = ~n,\n                         name = \"Species\", domain = list(row = 0, column = 0))\nfig\ndata(Arthritis)\nuniq<-unique(Arthritis$Treatment)\nArthritis%>% group_by(Treatment) %>%arrange(Improved)%>%\n  plot_ly( x = ~Treatment, y = ~Improved,color = ~ Improved,type = 'bar')%>%\nlayout(yaxis = list(title = 'value'), barmode = 'stack')\ndf1 = read.csv('https://raw.githubusercontent.com/plotly/datasets/718417069ead87650b90472464c7565dc8c2cb1c/sunburst-coffee-flavors-complete.csv')\n\nfig <- plot_ly(\n  type='treemap',\n  ids=df1$ids,\n  labels=df1$labels,\n  parents=df1$parents,\n  domain=list(column=0))\nfig\nx= list(\"Sales\", \"Consulting\", \"Net revenue\", \"Purchases\", \"Other expenses\", \"Profit before tax\")\nmeasure= c(\"relative\", \"relative\", \"total\", \"relative\", \"relative\", \"total\")\ntext= c(\"+60\", \"+80\", \"\", \"-40\", \"-20\", \"Total\")\ny= c(60, 80, 0, -40, -20, 0)\ndata = data.frame(x=factor(x,levels=x),measure,text,y)\n\nfig <- plot_ly(\n  data, name = \"20\", type = \"waterfall\", measure = ~measure,\n  x = ~x, textposition = \"outside\", y= ~y, text =~text,\n  connector = list(line = list(color= \"rgb(63, 63, 63)\"))) \nfig <- fig %>%\n  layout(title = \"Profit and loss statement 2018\",\n        xaxis = list(title = \"\"),\n        yaxis = list(title = \"\"),\n        autosize = TRUE,\n        showlegend = TRUE)\n\nfig"},{"path":"introduction-to-plotly.html","id":"references-1","chapter":"15 Introduction to plotly","heading":"15.2 References","text":"https://plotly.com/r/bar-charts/\nhttps://plotly.com/r/horizontal-bar-charts/\nhttps://plotly.com/r/line-charts/\nhttps://www.fusioncharts.com/blog/line-charts-vs-area-charts/\nhttps://plotly.com/r/filled-area-plots/\nhttps://plotly.com/r/treemaps/\nhttps://plotly.com/python/waterfall-charts/\nhttps://plotly.com/r/histograms/\nhttps://www.qlik.com/blog/third-pillar--mapping-data--visualizations-usage","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"draw-graphs-using-python-and-r","chapter":"16 draw graphs using python and R","heading":"16 draw graphs using python and R","text":"Shuyue Xu","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"things-that-need-to-be-done-before-fitting-models","chapter":"16 draw graphs using python and R","heading":"16.1 things that need to be done before fitting models","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"check-unique-of-the-values","chapter":"16 draw graphs using python and R","heading":"16.1.0.0.1 check unique of the values","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"check-null-values-consider-to-drop-values-or-adding-a-missing-indicator","chapter":"16 draw graphs using python and R","heading":"16.1.0.0.2 check null values, consider to drop values or adding a missing indicator","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"normalize-numerical-features","chapter":"16 draw graphs using python and R","heading":"16.1.0.0.3 normalize numerical features","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"handle-categorical-feature-using-label-encoding-or-onehotencoding-or-other","chapter":"16 draw graphs using python and R","heading":"16.1.0.0.4 handle categorical feature using Label encoding or OneHotencoding or other…","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"continuous-variables","chapter":"16 draw graphs using python and R","heading":"16.2 Continuous Variables","text":"always care asymmetry, outliers, multimodality, gaps, heaping, errors\n","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"graphs-that-usually-used-for-continuous-variables","chapter":"16 draw graphs using python and R","heading":"16.2.1 Graphs that usually used for continuous Variables:","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"histograms-r-base-codehistx","chapter":"16 draw graphs using python and R","heading":"16.2.1.1 Histograms: R base code:hist(x, …)","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"different-bin-boundaries-different-graphs.","chapter":"16 draw graphs using python and R","heading":"16.2.1.1.1 different bin boundaries different graphs.","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"shape","chapter":"16 draw graphs using python and R","heading":"16.2.1.1.2 shape:","text":"","code":"           skew to left means mean < median < mode\n           skew to right means mean > median > mode"},{"path":"draw-graphs-using-python-and-r.html","id":"boxplot-r-base-code-boxplotx-data","chapter":"16 draw graphs using python and R","heading":"16.2.1.2 Boxplot: R base code: boxplot(x, data=),","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"multiple-boxplots-usually-reorder-by-median-from-high-to-low","chapter":"16 draw graphs using python and R","heading":"16.2.1.2.1 multiple boxplots usually reorder by median from high to low","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"will-show-outliers-on-the-graph.-outliers-are","chapter":"16 draw graphs using python and R","heading":"16.2.1.2.2 will show outliers on the graph. outliers are","text":"","code":"           1.5 x hinge or fourth spread above upper-hinge\n           1.5 x hinge or fourth spread below lower-hinge<br/> "},{"path":"draw-graphs-using-python-and-r.html","id":"ways-to-check-normal-distribution-of-the-data","chapter":"16 draw graphs using python and R","heading":"16.2.2 Ways to check normal distribution of the data","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"qqplot-check-normal-distribution","chapter":"16 draw graphs using python and R","heading":"16.2.2.1 qqplot: check normal distribution","text":"","code":"         qqnorm(): produces a normal QQ plot of the variable\n         qqline(): adds a reference line"},{"path":"draw-graphs-using-python-and-r.html","id":"if-the-data-is-normally-distributed-the-points-in-the-qq-normal-plot-lie-on-a-straight-diagonal-line.","chapter":"16 draw graphs using python and R","heading":"16.2.2.1.1 If the data is normally distributed, the points in the QQ-normal plot lie on a straight diagonal line.","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"density-curve-line","chapter":"16 draw graphs using python and R","heading":"16.2.2.2 density curve line:","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"compare-density-curve-line-with-normal-curve-line-can-also-check-normal-distribution","chapter":"16 draw graphs using python and R","heading":"16.2.2.2.1 compare density curve line with normal curve line can also check normal distribution","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"shapiro-wilk-test-codeshapiro.testx","chapter":"16 draw graphs using python and R","heading":"16.2.2.3 Shapiro Wilk test: code:shapiro.test(x)","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"if-the-p-value-is-smaller-than-alpha-level-we-should-reject-the-null-hypothesis-that-data-is-normally-distributed-and-make-the-conclusion-that-data-is-not-normally-distributed.","chapter":"16 draw graphs using python and R","heading":"16.2.2.3.1 If the p-value is smaller than alpha level, we should reject the null hypothesis that data is normally distributed and make the conclusion that data is not normally distributed.","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"multivariate-categorical-data","chapter":"16 draw graphs using python and R","heading":"16.3 Multivariate categorical data","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"nominal-vs.-ordinal-ordinal-vs.-discrete","chapter":"16 draw graphs using python and R","heading":"16.3.0.1 nominal vs. ordinal, ordinal vs. discrete,…","text":"\n### Frequency\n#### Bar plot Basic R code: barplot(x)\n##### Sort logical order categories (level1, level2, lever3..)(Ordinal data)\n##### Sort highest lowest count (Nominal data)\n","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"cleveland-dot-plot","chapter":"16 draw graphs using python and R","heading":"16.3.0.2 Cleveland dot plot","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"r-codes-for-factor-data","chapter":"16 draw graphs using python and R","heading":"16.3.0.3 R codes for factor data","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"dont-use-levelsx-only-use-levelx","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.1 don’t use\" “levels(x) =”, only use “level(x)”","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"use-fct_reorder-to-assign-new-factor-levels","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.2 use “fct_reorder()” to assign new factor levels","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"use-fct_inorder-to-set-level-order-to-row-order","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.3 use “fct_inorder()” to set level order to row order","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"use-fct_relevel-to-move-levels-to-beginning-to-change-the-level-order","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.4 use “fct_relevel()” to move levels to beginning to change the level order","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"use-fct_infreq-to-order-the-levels-by-decreasing-frequency","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.5 use “fct_infreq()” to order the levels by decreasing frequency","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"use-fct_rev-to-reverse-the-order-of-factor-levels","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.6 use “fct_rev()” to reverse the order of factor levels","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"use-fct_explicit_na-to-turn-nas-into-a-real-factor-level","chapter":"16 draw graphs using python and R","heading":"16.3.0.3.7 use “fct_explicit_na()” to turn NAs into a real factor level","text":"\n###Proportion / Association\n#### Mosaic plots Code: mosaic(y~x)\n##### check association different variables\n##### Chi Square Test: p-value smaller alpha level, reject null hypothesis two variables independent conclusion depend .","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"fluctation-diagrams","chapter":"16 draw graphs using python and R","heading":"16.3.0.4 Fluctation diagrams","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"tidy","chapter":"16 draw graphs using python and R","heading":"16.3.0.5 Tidy","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"tidy-data-means-1-variable-per-column-and-1-observation-per-row","chapter":"16 draw graphs using python and R","heading":"16.3.0.5.1 Tidy data means 1 variable per column and 1 observation per row","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"find-relations-between-two-variables","chapter":"16 draw graphs using python and R","heading":"16.4 Find relations between two variables","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"scatter-plot-base-r-code-plotxy","chapter":"16 draw graphs using python and R","heading":"16.4.0.1 Scatter Plot: Base R code: plot(x,y)","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"show-gaps-clusters-outliers-boundaries-conditional-relationships-associations","chapter":"16 draw graphs using python and R","heading":"16.4.0.1.1 show gaps, clusters, outliers, boundaries, conditional relationships, associations","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"sometimes-transform-to-log-scale-square-heatmap-of-bin-counts-add-density-estimate-contour-lines","chapter":"16 draw graphs using python and R","heading":"16.4.0.1.2 sometimes transform to log scale, square heatmap of bin counts, add density estimate contour lines","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"continuous-categorical-variables","chapter":"16 draw graphs using python and R","heading":"16.5 Continuous Categorical Variables","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"parallel-coordinates-plot","chapter":"16 draw graphs using python and R","heading":"16.5.0.1 parallel coordinates plot","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"df-parcoordsrownames-f-brushmode-1d-axesreorderable-truecolor-listcolorby-regioncolorscale-scaleordinalcolorschemeschemecategory10withd3-truewidth-1500height-800","chapter":"16 draw graphs using python and R","heading":"16.5.0.1.1 df %>% parcoords(rownames = F, brushMode = ‘1D-axes’,reorderable = TRUE,color = list(colorBy = ‘Region’,colorScale = “scaleOrdinal”,colorScheme=“schemeCategory10”),withD3 = TRUE,width = 1500,height = 800)","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"heatmaps","chapter":"16 draw graphs using python and R","heading":"16.5.1 Heatmaps","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"can-be-used-for-continuous-or-categorical-data","chapter":"16 draw graphs using python and R","heading":"16.5.1.1 can be used for continuous or categorical data","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"show-frequency-counts-2d-histogram-or-value-of-a-third-variable","chapter":"16 draw graphs using python and R","heading":"16.5.1.2 show frequency counts (2D histogram) or value of a third variable","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"alluvial-diagrams","chapter":"16 draw graphs using python and R","heading":"16.5.2 Alluvial diagrams","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"shows-flow-of-changes-over-time","chapter":"16 draw graphs using python and R","heading":"16.5.2.1 shows flow of changes over time","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"color-by-first-variable-or-color-by-last-variable-or","chapter":"16 draw graphs using python and R","heading":"16.5.2.2 color by first variable or color by last variable or","text":"","code":""},{"path":"draw-graphs-using-python-and-r.html","id":"different-codes-to-plot-alluvail-diagram","chapter":"16 draw graphs using python and R","heading":"16.5.2.3 different codes to plot alluvail diagram:","text":"","code":"ggplot(df, aes(axis1, axis2, y = Freq)) +\n  geom_alluvium(color = \"blue\") +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) "},{"path":"draw-graphs-using-python-and-r.html","id":"or-change-data-to-lodes-form-first","chapter":"16 draw graphs using python and R","heading":"16.5.2.3.1 or change data to lodes form first","text":"","code":"dfl <- to_lodes_form(df, axes = 1:2)\nggplot(dfl, aes(alluvium = alluvium, x = x, stratum = stratum, y = Freq)) +\n  geom_alluvium(color = \"blue\") +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum))))"},{"path":"draw-graphs-using-python-and-r.html","id":"some-useful-packages-to-plot-in-python","chapter":"16 draw graphs using python and R","heading":"16.5.3 some useful packages to plot in Python","text":"","code":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure"},{"path":"draw-graphs-using-python-and-r.html","id":"make-plot-in-python","chapter":"16 draw graphs using python and R","heading":"16.5.4 make plot in python","text":"Cheat sheet, reviewed slides collected important materials think may used EDA. think important materials help students preparing final exams give hints start fitting models data. Cheat sheet includes lots different types graphs can used see distributions data first future studies like classifications predictions. Also, add explanations graphs information can get graph. think can help students better analyze graphs.\nadd basic codes make plots Python gives examples students didn’t learn Python . codes include examples basic graphs bar chart, pie chart, histograms, scatter plot . \nmaking cheat sheet, reviewed things learned class benefits deep understanding course. memory consolidation helps deep impression knowledge. Also, another machine learning class asked us data analysis. trying fit models data, also need preprocessing. never forget data cleaning course. Machine learning course asked us used Python instead R codes, think Cheat sheet can save time future projects improve efficient. \nNext time maybe provide detailed Python codes kind package Python can thing ‘dplyr’. add codes can select rows columns dataframe, change ordering rows data manipulations.","code":"##scattler plot\nplt.scatter(x,y,marker='o')\nplt.ylabel()\nplt.xlabel()##histogram\nplt.hist(x)##pie chart\nplt.pie(wf[\"Count\"],labels = wf[\"Weak Foot\"])##boxplot\nsns.boxplot(x=,y=)##violin plot\nsns.violinplot(x= ,y = )##barplot\nsns.barplot(x=, y=, data=)##small multiple of bar plots\nfig,axs = plt.subplots(3,1,figsize = (15,8))\nsns.barplot(ax=axs[0],data=s1, x='workclass', hue='target', y='Count')\nsns.barplot(ax=axs[1],data=s2, x='education', hue='target', y='Count')\nsns.barplot(ax=axs[2],data=s3, x='sex', hue='target', y='Count')##another way of small multiple of scatter plots\nplt.subplot(3, 1, 1)\nplt.scatter(auto_mpg_X['displacement'],auto_mpg_y )\n\nplt.subplot(3,1,2)\nplt.scatter(auto_mpg_X['horsepower'],auto_mpg_y )\n\nplt.subplot(3,1,3)\nplt.scatter(auto_mpg_X['weight'],auto_mpg_y )## some codes to preprocess data\ndf.groupby()\ndf.dropna(subset=)"},{"path":"data-visualization-packages-in-r.html","id":"data-visualization-packages-in-r","chapter":"17 Data Visualization Packages in R","heading":"17 Data Visualization Packages in R","text":"Hari Prasad Renganathan & Karveandhan PalanisamyData Visualization Packages RHerewith prepared cheat sheet data visualization using R.common packages:ggplot2LeafletPlotlyLatticeRColorBrewerHighchartersunburstRRGLEsquisseDygraphs","code":""},{"path":"esquisse-cheat-sheet.html","id":"esquisse-cheat-sheet","chapter":"18 Esquisse Cheat sheet","heading":"18 Esquisse Cheat sheet","text":"Hongtao Jiang - hj2554My community contribution project cheat sheet R package Esquisse. Esquisse allows interactively explore data visualizing ggplot2 package. allows draw bar plots, curves, scatter plots, histograms, boxplot, sf objects, export graph retrieve code reproduce graph, similar Tableau.Esquisse Cheatsheet","code":""},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"review-sheet-for-r-code-and-data-transformation","chapter":"19 review sheet for r code and data transformation","heading":"19 review sheet for r code and data transformation","text":"Mengchen Xu","code":""},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"contribution-explaination","chapter":"19 review sheet for r code and data transformation","heading":"19.0.1 Contribution Explaination","text":"student many experiences R, found difficult write R code. way learning EDVA, spent lot time searching useful R code online deal problems data transformation, data cleaning, etc. Thus, wanted make review sheet everyone included useful functions helpful learning materials Course. materials think important learning course. want make review sheet, others also review future. review sheet, wrote R code explanations using datasets use Problem Set. using new dataset, practiced ability write R code different situations helps know course material better. also added personal thoughts graphs course concepts review sheet. glad chose make review sheet Class Contribution assignment learned new knowledge way writing review sheet. example, R functions think good command , however, writing review sheet, found R functions can used differently different situations (Pivot_longer pivot_wider better use variables categorical data). use R functions really depends features dataset (“str_detect” variables string types, different functions chose filtering rows string type variables, integer type variables). R functions better categorical data, functions better numerical data.Also, reviewed lecture slides first one recent one assignment. way reviewing lectures notes, reflected learned tried find way present important concepts assignment. example, decided compare graphs created using different R functions sheet others can understand exactly functions work.better command types graphs learned, mosaic plot, alluvial plot, assignment. might choose review sheet cheatsheet next time well since found can learn much review sheet. However, probably focus course concepts instead R code next time.\n(code review sheet written , datasets used R built-datasets “mtcars” “Titanic”)","code":""},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"section-one-important-types-of-graphs-we-learned-in-edva.","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2 Section one: Important types of graphs we learned in EDVA.","text":"(use mtcars dataset example)","code":""},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"a-histograms","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2.1 (a) Histograms:","text":"creating frequency Histograms R using ggplot2 package, function need use “geom_histogram()”, need specify variable used X-axis, R figure frequency Y-axis. can specify binwidth center histogram. highly encouraged everyone use “color” “fill” parameters drawing histograms, makes graph clear. “Color” specifies outline color, “fill” specifies actual color histgram bins. example: (Note, “0” X-axis disappears case, thus, avoid situations like example real practice)(2)“binwidth” means width bins histgram, parameter “center” means values want center bins lay graph. exmaple , center 100, values 100,200,300, etc. center bins X-axis. example , “center” 50, values 50,150,250, etc. center bins X-axis. two parameters help locate position histgram graph canvas.“scale_x_continuous(breaks = seq(start, end, step)) ”scale_y_continuous(breaks = seq(start, end, step))\" two useful functions well. example, “scale_x_continuous(breaks = seq(0, 500,50))” set X limits breaks 0,50, 100, … 500. However, R show x_tick X=25 x label well. want x_tick = 100 show , can add line “theme(panel.grid.minor = element_blank())” eliminate minor x_ticks lines graph.(4)However, R show x_tick X=100 x label well. want x_tick = 100 show , can add line “theme(panel.grid.minor = element_blank())” eliminate minor x_ticks lines graph. example:","code":"\nmtcars %>% \n    ggplot(aes(x = disp)) + \n    geom_histogram(color = \"black\", fill = \"lightblue\",\n                   binwidth = 100, center = 100)\nmtcars %>% \n    ggplot(aes(x = disp)) + \n    geom_histogram(color = \"black\", fill = \"lightblue\",\n                   binwidth = 100, center = 50)\nmtcars %>% \n    ggplot(aes(x = disp)) + \n    geom_histogram(color = \"black\", fill = \"lightblue\",\n                   binwidth = 100, center = 50) +\n  scale_x_continuous(breaks = seq(0, 500, 50))\nmtcars %>% \n    ggplot(aes(x = disp)) + \n    geom_histogram(color = \"black\", fill = \"lightblue\",\n                   binwidth = 100, center = 50) +\n  scale_x_continuous(breaks = seq(0, 500, 50)) +\n  theme(panel.grid.minor = element_blank())"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"b-boxplot","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2.2 (b) boxplot:","text":"(use mtcars dataset example)\n(1) drawing boxplot, need specify x-axis y-axis. can add x,y cubtitles using “labs()”. Also, can adjust positions labs using “theme()”. “hjust = 0.5” move subtitle center. example:can reorder boxplots specific conditions. example, can reorder graph median CYL level median value MPG. can reorder graph mean, values well.easy way flip coordinates use “coord_flip()”. example:","code":"\nmtcars %>%\nggplot(aes(x=as.factor(cyl), y=mpg)) + \n    geom_boxplot() +\n    labs(x = \"CYL\", y = \"MPG\", \n    subtitle = \"Boxplots of MPG vs CYL\")+\n    theme(axis.text.x   = element_text(size = 12),\n        axis.text.y   = element_text(size = 12),\n        plot.subtitle = element_text(size = 15, face = \"bold\", hjust = 0.5))\nmtcars %>%\nggplot(aes(x=reorder(as.factor(cyl),mpg,median), y=mpg)) + \n    geom_boxplot() +\n    labs(x = \"CYL\", y = \"MPG\", \n    subtitle = \"Boxplots of MPG vs CYL\")+\n    theme(axis.text.x   = element_text(size = 12),\n        axis.text.y   = element_text(size = 12),\n        plot.subtitle = element_text(size = 15, face = \"bold\", hjust = 0.5))\nmtcars %>%\nggplot(aes(x=reorder(as.factor(cyl),mpg,median), y=mpg)) + \n    geom_boxplot() +\n    labs(x = \"CYL\", y = \"MPG\", \n    subtitle = \"Boxplots of CYL vs MPG\")+\n    theme(axis.text.x   = element_text(size = 11),\n        axis.text.y   = element_text(size = 11),\n        plot.subtitle = element_text(size = 15, face = \"bold\", hjust = 0.5)) +\n        coord_flip()"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"c-bar-plot","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2.3 (c) Bar Plot","text":"drawing frequency bar plot ggplot2, need specify dataset, x-axis.(2)Sometimes, meaning values X-axis clear (see example , 4,6,8 make much sense look ), therefore, need rename values something understandable, leave category names numbers. can use “fct_recode” solve problem. See example :","code":"\nmtcars %>%\nggplot(aes(x=as.factor(cyl))) + \n  labs(x = \"Number of Cylinders\") +\n  geom_bar(fill=\"lightblue\",color=\"black\")\nmtcars_new <- mtcars\nmtcars_new$cyl <- factor(mtcars_new$cyl, levels = c(4,6,8))\nmtcars_new$cyl <- fct_recode(mtcars_new$cyl,\"Car Models with 4 Cylinders\" = \"4\", \"Car Models with 6 Cylinders\" = \"6\",\"Car Models with 8 Cylinders\" = \"8\")\nmtcars_new %>%\nggplot(aes(x=as.factor(cyl))) + \n  labs(x = \"Number of Cylinders\") +\n  geom_bar(fill=\"lightblue\",color=\"black\")"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"d-parallel-coordinates-plot","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2.4 (d) Parallel Coordinates Plot","text":"(Use moderndive dataset examples)can use mtcars dataset draw parallel coordinate graph. drawing graph, can modify parameters create best graph analyzing. lines direction can show corelation bewteen variables. example, lines two variables twists, two variables negatively correlated. lines two variables parallel, matter slope lines negative positive, two variables positively correlated (graph , cyl disp positively correlated.)can make changes parallel coordinates plot, make graph clear. can use “scale” rescale graph “globalminmax” “uniminmax”. data different variable varies much, rescaling can help us see lines graph clearly. Also, can see smmoth graph changing “splineFactor” different numbers. can also add title graph simply add “title = …” function.","code":"\nmtcars_new_1 <- mtcars\nggparcoord(mtcars_new_1,columns=1:5,scale='globalminmax')\nggparcoord(mtcars_new_1,columns=1:5,scale='uniminmax',alphaLines=.3, splineFactor=10,title='mtcars parallel coordinates')"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"e-mosaic-plot","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2.5 (e) Mosaic Plot","text":"mosaic plot, can find correlation different variables well. proportion one variable distributed evenly different levels varaible, say two variables correlated, since change levels one variable influence variable. Note, need put dependent variable leftside “~”, independent variables right side. Also, need split dependent variable horizontally.","code":"\nmosaic(cyl~gear,direction = c(\"v\", \"h\") ,mtcars,highlighting_fill=brewer.pal(3,'Blues'))"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"f-alluvial","chapter":"19 review sheet for r code and data transformation","heading":"19.0.2.6 (f) Alluvial","text":"can draw alluvial find data flow. Take Titanic dataset example, can found people’s personal identity, result survived . good way track data. think data good data size large, want know data flow.personally think Alluvial interesting. fun graph can track flow data.","code":"\nggplot(as.data.frame(Titanic),\n       aes(axis1 = Class,axis3 = Age,axis2 = Sex, axis4 = Survived)) +\n  geom_alluvium(aes(fill = Class),color='blue') +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_x_continuous(breaks = 1:4, labels = c(\"Class\", \"Sex\", \"Age\",\"Survived\")) +\n  ggtitle(\"Titanic Alluvial\")"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"section-two-data-transformation-skills","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3 Section Two: Data Transformation Skills","text":"Data transformation important data visualization. reality, dataset always messy us visualize. Almost times, need change data order us visualize easily. useful method think useful data transformation. good us remember method mind. (use “mtcars” dataset “Titanic” dataset examples .)","code":""},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"a-mutate","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3.1 (a) mutate","text":"()“mutate” works adding new column dataset. can add column dateset using conditions data visualization later.can also add column data set using filter mutate, talk later.can see column called “disAndCyl” added dataset.","code":"\nmtcars_modified <- mtcars %>% \n  mutate(disAndCyl = disp/cyl)\n\nmtcars_modified##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n##                     disAndCyl\n## Mazda RX4            26.66667\n## Mazda RX4 Wag        26.66667\n## Datsun 710           27.00000\n## Hornet 4 Drive       43.00000\n## Hornet Sportabout    45.00000\n## Valiant              37.50000\n## Duster 360           45.00000\n## Merc 240D            36.67500\n## Merc 230             35.20000\n## Merc 280             27.93333\n## Merc 280C            27.93333\n## Merc 450SE           34.47500\n## Merc 450SL           34.47500\n## Merc 450SLC          34.47500\n## Cadillac Fleetwood   59.00000\n## Lincoln Continental  57.50000\n## Chrysler Imperial    55.00000\n## Fiat 128             19.67500\n## Honda Civic          18.92500\n## Toyota Corolla       17.77500\n## Toyota Corona        30.02500\n## Dodge Challenger     39.75000\n## AMC Javelin          38.00000\n## Camaro Z28           43.75000\n## Pontiac Firebird     50.00000\n## Fiat X1-9            19.75000\n## Porsche 914-2        30.07500\n## Lotus Europa         23.77500\n## Ford Pantera L       43.87500\n## Ferrari Dino         24.16667\n## Maserati Bora        37.62500\n## Volvo 142E           30.25000"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"b-filter","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3.2 (b) filter","text":"Filter means select conditions. example, want rows dataset, can use “filter”. can use boolean expressions, “XOR” select data want. filtering data want, can create new column store filtered data. example:","code":"\nmtcars_modified <- mtcars %>% \n  filter(xor(cyl==6,mpg==21.0)) %>% \n  mutate(carType = \"new type\")\n\nmtcars_modified##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb  carType\n## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 new type\n## Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 new type\n## Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 new type\n## Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 new type\n## Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 new type"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"c-fct_grid","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3.3 (c) fct_grid","text":"“fct_grid” useful need create plots variable different levels can compare different levels variable different data features.","code":"\nmtcars %>%\n  ggplot(aes(x = mpg)) +\n  geom_histogram(color = \"black\", fill = \"lightblue\") +\n  facet_grid(~ cyl)"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"d-sapply","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3.4 (d) sapply","text":"Sapply commonly used . comes data visualization, always need check data type variables. “sapply” can list variables names levels within variable dataset. useful.","code":"\ntitanic <-as.data.frame(Titanic)\ntitanic %>% \n     sapply(levels)## $Class\n## [1] \"1st\"  \"2nd\"  \"3rd\"  \"Crew\"\n## \n## $Sex\n## [1] \"Male\"   \"Female\"\n## \n## $Age\n## [1] \"Child\" \"Adult\"\n## \n## $Survived\n## [1] \"No\"  \"Yes\"\n## \n## $Freq\n## NULL"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"epivot_wider","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3.5 (e)pivot_wider","text":"Pivot_wider useful need transform dataset “wider” form. example, Titanic dataset, pivot_wider can extract levels variable, Sex, turns levels columns dataset. extremely useful visualize frequency different levels vriable dataset.","code":"\ntitanic##    Class    Sex   Age Survived Freq\n## 1    1st   Male Child       No    0\n## 2    2nd   Male Child       No    0\n## 3    3rd   Male Child       No   35\n## 4   Crew   Male Child       No    0\n## 5    1st Female Child       No    0\n## 6    2nd Female Child       No    0\n## 7    3rd Female Child       No   17\n## 8   Crew Female Child       No    0\n## 9    1st   Male Adult       No  118\n## 10   2nd   Male Adult       No  154\n## 11   3rd   Male Adult       No  387\n## 12  Crew   Male Adult       No  670\n## 13   1st Female Adult       No    4\n## 14   2nd Female Adult       No   13\n## 15   3rd Female Adult       No   89\n## 16  Crew Female Adult       No    3\n## 17   1st   Male Child      Yes    5\n## 18   2nd   Male Child      Yes   11\n## 19   3rd   Male Child      Yes   13\n## 20  Crew   Male Child      Yes    0\n## 21   1st Female Child      Yes    1\n## 22   2nd Female Child      Yes   13\n## 23   3rd Female Child      Yes   14\n## 24  Crew Female Child      Yes    0\n## 25   1st   Male Adult      Yes   57\n## 26   2nd   Male Adult      Yes   14\n## 27   3rd   Male Adult      Yes   75\n## 28  Crew   Male Adult      Yes  192\n## 29   1st Female Adult      Yes  140\n## 30   2nd Female Adult      Yes   80\n## 31   3rd Female Adult      Yes   76\n## 32  Crew Female Adult      Yes   20\ntitanic_new <- titanic %>% \n  count(Class,Sex,sort = TRUE,name =\"Freq\") %>%\n  pivot_wider(id_cols = Class, names_from = Sex,values_from = Freq) %>%\n  rowwise() \n\ntitanic_new## # A tibble: 4 × 3\n## # Rowwise: \n##   Class  Male Female\n##   <fct> <int>  <int>\n## 1 1st       4      4\n## 2 2nd       4      4\n## 3 3rd       4      4\n## 4 Crew      4      4"},{"path":"review-sheet-for-r-code-and-data-transformation.html","id":"fpivot_longer","chapter":"19 review sheet for r code and data transformation","heading":"19.0.3.6 (f)pivot_longer","text":"Similar pivot_wider, pivot_longer can transform dataset “longer” form. example, Titanic dataset, pivot_longer can combine several columns dataset together one column, makes dateset “longer”. can compare dataset original Titanic, titanic_new, titanic_new_1 see pivot_longer pivot_wider work.","code":"\ntitanic_new_1<-titanic_new %>% \n    pivot_longer(cols = !Class,names_to = \"Sex\",values_to = \"frequency\")\ntitanic_new_1## # A tibble: 8 × 3\n##   Class Sex    frequency\n##   <fct> <chr>      <int>\n## 1 1st   Male           4\n## 2 1st   Female         4\n## 3 2nd   Male           4\n## 4 2nd   Female         4\n## 5 3rd   Male           4\n## 6 3rd   Female         4\n## 7 Crew  Male           4\n## 8 Crew  Female         4"},{"path":"scrape-twitter-data-using-r.html","id":"scrape-twitter-data-using-r","chapter":"20 Scrape Twitter data using R","heading":"20 Scrape Twitter data using R","text":"Senqi ZhangSocial media emerging field study researchers. pandemic, lots studies carried analysis using Twitter data. tutorial, want introduce basics scraping Twitter data using R. introduce two libraries rtweet twitteR. Note two libraries complement . rtweet actually contains functionality twitteR . updated maintained recently, always suggest using rtweet collecting Twitter data. However, tutorial, still introduce twitteR.","code":"\n# install.packages(\"twitteR\")\n# install.packages(\"rtweet\")\nlibrary(dplyr)\nlibrary(twitteR) #Note there is only one big capital R at the end\nlibrary(rtweet)"},{"path":"scrape-twitter-data-using-r.html","id":"preparation","chapter":"20 Scrape Twitter data using R","heading":"20.1 Preparation","text":"","code":""},{"path":"scrape-twitter-data-using-r.html","id":"apply-for-developer-account","chapter":"20 Scrape Twitter data using R","heading":"20.1.1 Apply for developer account","text":"start, need Twitter account (valid phone number associated).order gain access Twitter data, apply developer account.can apply using https://developer.twitter.com/en.applying process requires fill academic project information. ’s relative easy process dive deep .","code":""},{"path":"scrape-twitter-data-using-r.html","id":"set-up-the-app-and-generate-tokens","chapter":"20 Scrape Twitter data using R","heading":"20.1.2 Set up the App and generate tokens","text":"application goes , need set “App”. Select Project & Apps choose Create AppAfter successfully creating App, click Keys tokens, Generate. One page pop contains information need, including API key, API secret, Access Token, Access Token Secret. need keep record .","code":""},{"path":"scrape-twitter-data-using-r.html","id":"libraries","chapter":"20 Scrape Twitter data using R","heading":"20.1.3 Libraries","text":"use following two libraries: twitteR rtweet.","code":""},{"path":"scrape-twitter-data-using-r.html","id":"twitter","chapter":"20 Scrape Twitter data using R","heading":"20.2 twitteR","text":"","code":""},{"path":"scrape-twitter-data-using-r.html","id":"connection","chapter":"20 Scrape Twitter data using R","heading":"20.2.1 Connection","text":"first introduce library TwitterR. Note keys revoked finished tutorial, try use .start, need establish connection. need enter “1” console authorize connection.","code":"\nAPI_key = \"viQ7dbfTQrjCXq7MGc33C5KId\" \nAPI_secret = \"K6Xa9ZqBzjofXc8YpISMR1rfsv0PuwWHBXgTLYeziar3zIUg3Y\"\nAccess_token = \"1238492669338419200-8PM1QodMzH89YW99rClzdNEaw1VdMK\"\nAccess_secret = \"oWAZqJiw2V0LE8rI6OPV7htKWD9UTeU9o2cTiCMbUZ5Cq\"\nsetup_twitter_oauth(API_key,API_secret,Access_token,Access_secret)"},{"path":"scrape-twitter-data-using-r.html","id":"search","chapter":"20 Scrape Twitter data using R","heading":"20.2.2 Search","text":"first introduce searchTwitter, basic function find tweets using multiple criteria. example set keyword “Covid-19” return three tweets.Notice returned result texts contain Twitter username followed content, contents English . might well observe texts seem complete discuss later.\ncan also limit scope English give time restriction.\ncan convert returned tweets data frame using function twListToDF. Moreover, notice example tweets RT beginning. implies results actually retweets filter retweets using function strip_retweetsFor purpose tidiness, present several noteworthy features. fact, single collected tweet, far features twitteR presents. One important feature extended tweet, contains complete content tweet. Unfortunately, unable retrieve information using tweetR. Incomplete text can meaningless carry analysis. solve problem, introduce library rtweet.","code":"\nsearchTwitter(\"Covid-19\", n=3)\nsearchTwitter(\"Covid-19\", n=3, lang=\"en\", since=\"2021-11-01\", until=\"2021-11-02\" )\ncovid_raw <- searchTwitter(\"Covid-19\", n=500, lang=\"en\", since=\"2021-11-01\", until=\"2021-11-02\" )\ncovid_df <- covid_raw %>% \n  strip_retweets() %>%\n  twListToDF()\n\ncovid_df <- covid_df %>%\n  select(text,favoriteCount,created,truncated,longitude,latitude)\n\nknitr::kable(covid_df[1:3,], format=\"markdown\")"},{"path":"scrape-twitter-data-using-r.html","id":"rtweet","chapter":"20 Scrape Twitter data using R","heading":"20.3 rtweet","text":"","code":""},{"path":"scrape-twitter-data-using-r.html","id":"connection-1","chapter":"20 Scrape Twitter data using R","heading":"20.3.1 Connection","text":"establish connection, use function like search_tweets(\"Covid-19\")console. , signed Twitter develop account, page pop-, asking authorize API usage. Agree connection established.","code":""},{"path":"scrape-twitter-data-using-r.html","id":"search-1","chapter":"20 Scrape Twitter data using R","heading":"20.3.2 Search","text":"search function rtweet resembles twitteR, changes. can specify multiple keywords using blank space. result return tweets contain three words. can filter retweets setting include_rts FALSE.can observe returning data frame actually 90 features compared 16 features twitteR data frame. text feature seem complete parts show due display length. can write data frame csv file observe text feature complete.","code":"\ncovid_df2 <- search_tweets(\"Covid-19 vaccine flu\",n=50, include_rts=FALSE, lang=\"en\")\n#write_as_csv(covid_df2, \"covid_df\", na =\"NA\")"},{"path":"scrape-twitter-data-using-r.html","id":"streaming","chapter":"20 Scrape Twitter data using R","heading":"20.3.3 Streaming","text":"search function limitations. search_tweets, returns results past 6-9 days. resort streaming function collect real-time tweets.can stream random sample without specifying keywords. timeout parameter defines time period searching. want stream indefinitely, set timeout = FALSE. function return 1% tweets posted time.can specify language results display multiple languages.can also specify keywords write data json file. data collection purpose, recommended method. can settimeout=FALSE collect continuous period. collected data “covid.json”.","code":"\nrand_tweet <- stream_tweets(\"\", timeout= 3)\n#covid_tweet <- stream_tweets(\"Covid-19\", timeout=3, file_name = \"covid.json\", langangue=\"en\")"},{"path":"scrape-twitter-data-using-r.html","id":"other-functions","chapter":"20 Scrape Twitter data using R","heading":"20.3.4 Other functions","text":"can use get_timeline retreive tweets posted target users.can use following find certain groups users. example, get_friends returns ids Elon Musk follows. use lookup_users find one specific user. (Creator Dogecoin)can use built ts_plot plot time series showing tweets posted time stamp. set interval 5 minutes.functions allow manipulate Twitter status, like posting tweets, following certain Twitter users, etc.. leave readers find . Refer https://www.rdocumentation.org/packages/rtweet/versions/0.7.0 full list functions use.","code":"\npotus_tweet <- get_timeline(\"@POTUS\",n=5) #Collecting tweets posted by the president.\nem_favorite <- get_favorites(\"@elonmusk\",n=5)\nem_friends <- get_friends(\"@elonmusk\",n=5)\nem_follower <- get_followers(\"@elonmusk\",n=5)\nfriend <- lookup_users(30699048)\ncovid_df3 <- search_tweets(\"Covid-19\",n=1000, include_rts=FALSE, lang=\"en\")\nts_plot(covid_df3,by = \"5 minutes\")"},{"path":"scrape-twitter-data-using-r.html","id":"wrap-up","chapter":"20 Scrape Twitter data using R","heading":"20.4 Wrap up","text":"","code":""},{"path":"scrape-twitter-data-using-r.html","id":"api-limitations","chapter":"20 Scrape Twitter data using R","heading":"20.4.1 API limitations","text":"libraries actually built based Twitter API, Twitter standard API poses fundamental restrictions.\nexample, search generally allows search back seven days. reason, suggest always using streaming method want collect large amount data. limits, can refer : https://developer.twitter.com/en/docs/twitter-api/early-access.","code":""},{"path":"scrape-twitter-data-using-r.html","id":"python-equivalent-library","chapter":"20 Scrape Twitter data using R","heading":"20.4.2 Python equivalent library","text":"can also refer Tweepy library Python, functionality collecting tweets, https://docs.tweepy.org/en/stable/.","code":""},{"path":"scrape-twitter-data-using-r.html","id":"reference","chapter":"20 Scrape Twitter data using R","heading":"20.5 Reference","text":"https://www.rdocumentation.org/packages/rtweet/versions/0.7.0https://www.rdocumentation.org/packages/twitteR/versions/1.1.9","code":""},{"path":"r-note-and-mathematics-in-rmd-cheatsheet.html","id":"r-note-and-mathematics-in-rmd-cheatsheet","chapter":"21 R note and Mathematics in Rmd cheatsheet","heading":"21 R note and Mathematics in Rmd cheatsheet","text":"Yuning Ding Xingcheng RongThis R note briefly mention R features think important keep mind write R.\nAlso, covers two main aspects R, Basic operation mathematical statistics R.\nbasic operation includes running debugging r, preparing, data processing, data view, drawing, edit.\nknow, R powerful language make data analysis, R learner, also need learn use deal statistics problems. also include mathematical statistics part R covers normal distribution, hypothesis tests etc.Mathematics Rmd cheatsheet aims help Rmd user write mathematical formulas. cheatsheet can also used different document types like Latex.\nAlthough EDAV class, just analyze graphs. find graphs related statistical models, might need explained mathematical formulas help someone mathematical knowledge familiar statistical models comprehend easily.\nthink cheatsheet useful, since rarely see kind Rmd cheatsheet Internet, help user write beautiful mathematical formulas deduce conclusion. finishing mathematics Rmd cheatsheet, familiar writing formulas Rmd.R note link: https://github.com/RubyRong/edav_CC27/blob/master/cc2.pdf\nMathematics Rmd cheatsheet link: https://github.com/RubyRong/edav_CC27/blob/master/mathmd_cheatsheet.pdf","code":""},{"path":"textbook-notes.html","id":"textbook-notes","chapter":"22 Textbook Notes","heading":"22 Textbook Notes","text":"Xianmeng WangThis summary note course textbook “Graphical Data Analysis”, course EDAV . Many contents overlapped two still number significant difference. textbook note covers contents Chapter 3 Chapter 11 since Chapter 1 2 introductions contents Chapter 12 likely summaries introduction R, covered file.Link:https://github.com/wxmkevin/edav_cc","code":""},{"path":"ggplot2-cheatsheet.html","id":"ggplot2-cheatsheet","chapter":"23 ggplot2 cheatsheet","heading":"23 ggplot2 cheatsheet","text":"Ruoxi Liu (rl3155) Ziyu Fang (zf2253)ggplot2 cheatsheet uploaded github, link: https://github.com/rul124/Stats5702-Community-Contribution","code":""},{"path":"rstudio-cheatsheet.html","id":"rstudio-cheatsheet","chapter":"24 RStudio Cheatsheet","heading":"24 RStudio Cheatsheet","text":"Mendel BranoverThis document serves cheatsheet guide using Studio IDE. includes brief explanations useful features RStudio.\ndocument can used introduction new Rstudio users. can also used quick start guide RStudio user wanting brief breakdown shortcuts.URL document : https://github.com/mbran1/GR5702-EDAV","code":""},{"path":"d3-visualization-in-r-cheatsheet.html","id":"d3-visualization-in-r-cheatsheet","chapter":"25 D3 Visualization in R Cheatsheet","heading":"25 D3 Visualization in R Cheatsheet","text":"Qiran Li Zheng WuThis LaTeX generated cheat sheet help go data visualization using R providing comprehensive shortcut examples utilize D3 features R.cheatsheet now maintained following link Qiran Zheng collectively:\nhttps://github.com/Kitatine/D3-Visualization--R-Cheatsheet/blob/e36d84105daddf2c23388c17d78be7c9533bd01c/D3CheatSheet.pdf","code":""},{"path":"colors.html","id":"colors","chapter":"26 Colors","heading":"26 Colors","text":"Ziheng Ru","code":""},{"path":"r-data-sets.html","id":"r-data-sets","chapter":"27 R data sets","heading":"27 R data sets","text":"Daoxing ZhangThis cheatsheet summarized R built-data sets help people find interested data sets play . data set, cheatsheet listed attributes brief explanations.Access cheat sheet:\nresources/R_datasets/R_datasets_Cheatsheet.pdf","code":""},{"path":"introduction-to-plotly-1.html","id":"introduction-to-plotly-1","chapter":"28 “Introduction to Plotly”","heading":"28 “Introduction to Plotly”","text":"Siqi HeI create cheat sheet plotly give examples rmarkdown file can viewed github. link cheatsheet:https://github.com/siqihe41099/Community-Contribution","code":""},{"path":"tidyverse-cheatsheet.html","id":"tidyverse-cheatsheet","chapter":"29 Tidyverse Cheatsheet","heading":"29 Tidyverse Cheatsheet","text":"Huaizhi Ge","code":"\nlibrary(tidyverse)"},{"path":"tidyverse-cheatsheet.html","id":"introduction-2","chapter":"29 Tidyverse Cheatsheet","heading":"29.1 Introduction","text":"Usually, plot data, need preprocess data first. use tidyverse process data commonly. find can remember tidyverse functions deal data. create cheat sheet includes commonly used functions tidyverse process data.reference:\nhttps://www.rdocumentation.org/,\nhttps://dplyr.tidyverse.org/index.html","code":""},{"path":"tidyverse-cheatsheet.html","id":"select","chapter":"29 Tidyverse Cheatsheet","heading":"29.2 select()","text":"select columns data, need add names columns Select() function.\n(Use dataset “mpg” example dataset.)Also, can use start_col:end_col.can select columns structure using starts_with(),end_with() contains().select_if() can used select columns specific data type. example, select_if(.character) can select character type columns. Similarly, can use .numeric, .integer, .factor .select_if() can select columns meet conditions.","code":"\nmpg %>% \n  select(year,manufacturer,model)## # A tibble: 234 × 3\n##     year manufacturer model     \n##    <int> <chr>        <chr>     \n##  1  1999 audi         a4        \n##  2  1999 audi         a4        \n##  3  2008 audi         a4        \n##  4  2008 audi         a4        \n##  5  1999 audi         a4        \n##  6  1999 audi         a4        \n##  7  2008 audi         a4        \n##  8  1999 audi         a4 quattro\n##  9  1999 audi         a4 quattro\n## 10  2008 audi         a4 quattro\n## # … with 224 more rows\nmpg %>% \n  select(year:drv)## # A tibble: 234 × 4\n##     year   cyl trans      drv  \n##    <int> <int> <chr>      <chr>\n##  1  1999     4 auto(l5)   f    \n##  2  1999     4 manual(m5) f    \n##  3  2008     4 manual(m6) f    \n##  4  2008     4 auto(av)   f    \n##  5  1999     6 auto(l5)   f    \n##  6  1999     6 manual(m5) f    \n##  7  2008     6 auto(av)   f    \n##  8  1999     4 manual(m5) 4    \n##  9  1999     4 auto(l5)   4    \n## 10  2008     4 manual(m6) 4    \n## # … with 224 more rows\nmpg %>% \n  select(starts_with(\"m\"),contains(\"c\"),ends_with(\"y\"))## # A tibble: 234 × 6\n##    manufacturer model        cyl   cty class     hwy\n##    <chr>        <chr>      <int> <int> <chr>   <int>\n##  1 audi         a4             4    18 compact    29\n##  2 audi         a4             4    21 compact    29\n##  3 audi         a4             4    20 compact    31\n##  4 audi         a4             4    21 compact    30\n##  5 audi         a4             6    16 compact    26\n##  6 audi         a4             6    18 compact    26\n##  7 audi         a4             6    18 compact    27\n##  8 audi         a4 quattro     4    18 compact    26\n##  9 audi         a4 quattro     4    16 compact    25\n## 10 audi         a4 quattro     4    20 compact    28\n## # … with 224 more rows\nmpg %>% \n  select_if(is.numeric)## # A tibble: 234 × 5\n##    displ  year   cyl   cty   hwy\n##    <dbl> <int> <int> <int> <int>\n##  1   1.8  1999     4    18    29\n##  2   1.8  1999     4    21    29\n##  3   2    2008     4    20    31\n##  4   2    2008     4    21    30\n##  5   2.8  1999     6    16    26\n##  6   2.8  1999     6    18    26\n##  7   3.1  2008     6    18    27\n##  8   1.8  1999     4    18    26\n##  9   1.8  1999     4    16    25\n## 10   2    2008     4    20    28\n## # … with 224 more rows\nmpg %>% select(where(is.numeric)) %>% \n  select(where(~mean(.)<100))## # A tibble: 234 × 4\n##    displ   cyl   cty   hwy\n##    <dbl> <int> <int> <int>\n##  1   1.8     4    18    29\n##  2   1.8     4    21    29\n##  3   2       4    20    31\n##  4   2       4    21    30\n##  5   2.8     6    16    26\n##  6   2.8     6    18    26\n##  7   3.1     6    18    27\n##  8   1.8     4    18    26\n##  9   1.8     4    16    25\n## 10   2       4    20    28\n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"filter","chapter":"29 Tidyverse Cheatsheet","heading":"29.3 filter()","text":"can use filter() choose certain rows.filter(condition1,condition2) return rows meet two conditions.filter(condition1|condition2) return rows meet condition1 condition2.filter(xor(condition1,condition2)) return rows meet one condition.filter_all() filter columns.filter_if() first find columns meet condition filter.filter_at() need select columns vars().","code":"\nmpg %>% \n  filter(year==1999)## # A tibble: 117 × 11\n##    manufacturer model     displ  year   cyl trans  drv     cty   hwy fl    class\n##    <chr>        <chr>     <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n##  1 audi         a4          1.8  1999     4 auto(… f        18    29 p     comp…\n##  2 audi         a4          1.8  1999     4 manua… f        21    29 p     comp…\n##  3 audi         a4          2.8  1999     6 auto(… f        16    26 p     comp…\n##  4 audi         a4          2.8  1999     6 manua… f        18    26 p     comp…\n##  5 audi         a4 quatt…   1.8  1999     4 manua… 4        18    26 p     comp…\n##  6 audi         a4 quatt…   1.8  1999     4 auto(… 4        16    25 p     comp…\n##  7 audi         a4 quatt…   2.8  1999     6 auto(… 4        15    25 p     comp…\n##  8 audi         a4 quatt…   2.8  1999     6 manua… 4        17    25 p     comp…\n##  9 audi         a6 quatt…   2.8  1999     6 auto(… 4        15    24 p     mids…\n## 10 chevrolet    c1500 su…   5.7  1999     8 auto(… r        13    17 r     suv  \n## # … with 107 more rows\nmpg %>% \n  filter(year!=1999)## # A tibble: 117 × 11\n##    manufacturer model     displ  year   cyl trans  drv     cty   hwy fl    class\n##    <chr>        <chr>     <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n##  1 audi         a4          2    2008     4 manua… f        20    31 p     comp…\n##  2 audi         a4          2    2008     4 auto(… f        21    30 p     comp…\n##  3 audi         a4          3.1  2008     6 auto(… f        18    27 p     comp…\n##  4 audi         a4 quatt…   2    2008     4 manua… 4        20    28 p     comp…\n##  5 audi         a4 quatt…   2    2008     4 auto(… 4        19    27 p     comp…\n##  6 audi         a4 quatt…   3.1  2008     6 auto(… 4        17    25 p     comp…\n##  7 audi         a4 quatt…   3.1  2008     6 manua… 4        15    25 p     comp…\n##  8 audi         a6 quatt…   3.1  2008     6 auto(… 4        17    25 p     mids…\n##  9 audi         a6 quatt…   4.2  2008     8 auto(… 4        16    23 p     mids…\n## 10 chevrolet    c1500 su…   5.3  2008     8 auto(… r        14    20 r     suv  \n## # … with 107 more rows\nmpg %>% \n  filter(year==1999,cyl==4)## # A tibble: 45 × 11\n##    manufacturer model   displ  year   cyl trans   drv     cty   hwy fl    class \n##    <chr>        <chr>   <dbl> <int> <int> <chr>   <chr> <int> <int> <chr> <chr> \n##  1 audi         a4        1.8  1999     4 auto(l… f        18    29 p     compa…\n##  2 audi         a4        1.8  1999     4 manual… f        21    29 p     compa…\n##  3 audi         a4 qua…   1.8  1999     4 manual… 4        18    26 p     compa…\n##  4 audi         a4 qua…   1.8  1999     4 auto(l… 4        16    25 p     compa…\n##  5 chevrolet    malibu    2.4  1999     4 auto(l… f        19    27 r     midsi…\n##  6 dodge        carava…   2.4  1999     4 auto(l… f        18    24 r     miniv…\n##  7 honda        civic     1.6  1999     4 manual… f        28    33 r     subco…\n##  8 honda        civic     1.6  1999     4 auto(l… f        24    32 r     subco…\n##  9 honda        civic     1.6  1999     4 manual… f        25    32 r     subco…\n## 10 honda        civic     1.6  1999     4 manual… f        23    29 p     subco…\n## # … with 35 more rows\nmpg %>% \n  filter(year==1999|cyl==4)## # A tibble: 153 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n##  9 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## 10 audi         a4 quattro   2    2008     4 auto… 4        19    27 p     comp…\n## # … with 143 more rows\nmpg %>% \n  filter(xor(year==1999,cyl==4))## # A tibble: 108 × 11\n##    manufacturer model     displ  year   cyl trans  drv     cty   hwy fl    class\n##    <chr>        <chr>     <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n##  1 audi         a4          2    2008     4 manua… f        20    31 p     comp…\n##  2 audi         a4          2    2008     4 auto(… f        21    30 p     comp…\n##  3 audi         a4          2.8  1999     6 auto(… f        16    26 p     comp…\n##  4 audi         a4          2.8  1999     6 manua… f        18    26 p     comp…\n##  5 audi         a4 quatt…   2    2008     4 manua… 4        20    28 p     comp…\n##  6 audi         a4 quatt…   2    2008     4 auto(… 4        19    27 p     comp…\n##  7 audi         a4 quatt…   2.8  1999     6 auto(… 4        15    25 p     comp…\n##  8 audi         a4 quatt…   2.8  1999     6 manua… 4        17    25 p     comp…\n##  9 audi         a6 quatt…   2.8  1999     6 auto(… 4        15    24 p     mids…\n## 10 chevrolet    c1500 su…   5.7  1999     8 auto(… r        13    17 r     suv  \n## # … with 98 more rows\nmpg %>% \n  filter_all(any_vars(.==4))## # A tibble: 164 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  6 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n##  7 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n##  8 audi         a4 quattro   2    2008     4 auto… 4        19    27 p     comp…\n##  9 audi         a4 quattro   2.8  1999     6 auto… 4        15    25 p     comp…\n## 10 audi         a4 quattro   2.8  1999     6 manu… 4        17    25 p     comp…\n## # … with 154 more rows\nmpg %>% \n  filter_if(is.character,any_vars(.==4))## # A tibble: 103 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  2 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n##  3 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n##  4 audi         a4 quattro   2    2008     4 auto… 4        19    27 p     comp…\n##  5 audi         a4 quattro   2.8  1999     6 auto… 4        15    25 p     comp…\n##  6 audi         a4 quattro   2.8  1999     6 manu… 4        17    25 p     comp…\n##  7 audi         a4 quattro   3.1  2008     6 auto… 4        17    25 p     comp…\n##  8 audi         a4 quattro   3.1  2008     6 manu… 4        15    25 p     comp…\n##  9 audi         a6 quattro   2.8  1999     6 auto… 4        15    24 p     mids…\n## 10 audi         a6 quattro   3.1  2008     6 auto… 4        17    25 p     mids…\n## # … with 93 more rows\nmpg %>% \n  filter_at(vars(displ),all_vars(.>3))## # A tibble: 126 × 11\n##    manufacturer model     displ  year   cyl trans  drv     cty   hwy fl    class\n##    <chr>        <chr>     <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n##  1 audi         a4          3.1  2008     6 auto(… f        18    27 p     comp…\n##  2 audi         a4 quatt…   3.1  2008     6 auto(… 4        17    25 p     comp…\n##  3 audi         a4 quatt…   3.1  2008     6 manua… 4        15    25 p     comp…\n##  4 audi         a6 quatt…   3.1  2008     6 auto(… 4        17    25 p     mids…\n##  5 audi         a6 quatt…   4.2  2008     8 auto(… 4        16    23 p     mids…\n##  6 chevrolet    c1500 su…   5.3  2008     8 auto(… r        14    20 r     suv  \n##  7 chevrolet    c1500 su…   5.3  2008     8 auto(… r        11    15 e     suv  \n##  8 chevrolet    c1500 su…   5.3  2008     8 auto(… r        14    20 r     suv  \n##  9 chevrolet    c1500 su…   5.7  1999     8 auto(… r        13    17 r     suv  \n## 10 chevrolet    c1500 su…   6    2008     8 auto(… r        12    17 r     suv  \n## # … with 116 more rows"},{"path":"tidyverse-cheatsheet.html","id":"mutate","chapter":"29 Tidyverse Cheatsheet","heading":"29.4 mutate()","text":"mutate() can add new column based existing column.can use ifelse() choose data.mutate_all() can change columns.mutate_if() can select certain columns mutate.mutate_at() need select columns vars().case_when() can create categorical data.","code":"\nmpg %>% \n  select(manufacturer,model,year) %>% \n  mutate(year2=year-2000)## # A tibble: 234 × 4\n##    manufacturer model       year year2\n##    <chr>        <chr>      <int> <dbl>\n##  1 audi         a4          1999    -1\n##  2 audi         a4          1999    -1\n##  3 audi         a4          2008     8\n##  4 audi         a4          2008     8\n##  5 audi         a4          1999    -1\n##  6 audi         a4          1999    -1\n##  7 audi         a4          2008     8\n##  8 audi         a4 quattro  1999    -1\n##  9 audi         a4 quattro  1999    -1\n## 10 audi         a4 quattro  2008     8\n## # … with 224 more rows\nmpg %>% \n  select(manufacturer,model,year) %>% \n  mutate(year2=ifelse(year>2000,0,NA))## # A tibble: 234 × 4\n##    manufacturer model       year year2\n##    <chr>        <chr>      <int> <dbl>\n##  1 audi         a4          1999    NA\n##  2 audi         a4          1999    NA\n##  3 audi         a4          2008     0\n##  4 audi         a4          2008     0\n##  5 audi         a4          1999    NA\n##  6 audi         a4          1999    NA\n##  7 audi         a4          2008     0\n##  8 audi         a4 quattro  1999    NA\n##  9 audi         a4 quattro  1999    NA\n## 10 audi         a4 quattro  2008     0\n## # … with 224 more rows\nmpg %>% \n  mutate_all(toupper)## # A tibble: 234 × 11\n##    manufacturer model      displ year  cyl   trans drv   cty   hwy   fl    class\n##    <chr>        <chr>      <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n##  1 AUDI         A4         1.8   1999  4     AUTO… F     18    29    P     COMP…\n##  2 AUDI         A4         1.8   1999  4     MANU… F     21    29    P     COMP…\n##  3 AUDI         A4         2     2008  4     MANU… F     20    31    P     COMP…\n##  4 AUDI         A4         2     2008  4     AUTO… F     21    30    P     COMP…\n##  5 AUDI         A4         2.8   1999  6     AUTO… F     16    26    P     COMP…\n##  6 AUDI         A4         2.8   1999  6     MANU… F     18    26    P     COMP…\n##  7 AUDI         A4         3.1   2008  6     AUTO… F     18    27    P     COMP…\n##  8 AUDI         A4 QUATTRO 1.8   1999  4     MANU… 4     18    26    P     COMP…\n##  9 AUDI         A4 QUATTRO 1.8   1999  4     AUTO… 4     16    25    P     COMP…\n## 10 AUDI         A4 QUATTRO 2     2008  4     MANU… 4     20    28    P     COMP…\n## # … with 224 more rows\nmpg %>% \n  mutate_if(is.numeric,round)## # A tibble: 234 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> <chr>\n##  1 audi         a4             2  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4             2  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4             2  2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4             2  2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4             3  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4             3  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4             3  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro     2  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro     2  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro     2  2008     4 manu… 4        20    28 p     comp…\n## # … with 224 more rows\nmpg %>% \n  select(manufacturer,model,displ,cyl) %>% \n  mutate_at(vars(displ,cyl),~(.*2))## # A tibble: 234 × 4\n##    manufacturer model      displ   cyl\n##    <chr>        <chr>      <dbl> <dbl>\n##  1 audi         a4           3.6     8\n##  2 audi         a4           3.6     8\n##  3 audi         a4           4       8\n##  4 audi         a4           4       8\n##  5 audi         a4           5.6    12\n##  6 audi         a4           5.6    12\n##  7 audi         a4           6.2    12\n##  8 audi         a4 quattro   3.6     8\n##  9 audi         a4 quattro   3.6     8\n## 10 audi         a4 quattro   4       8\n## # … with 224 more rows\nmpg %>% \n  select(manufacturer,model,year) %>% \n  mutate(year2=case_when(\n    year<2000~\"very old\",\n    year<2005~\"old\",\n    TRUE~\"new\"\n  ))## # A tibble: 234 × 4\n##    manufacturer model       year year2   \n##    <chr>        <chr>      <int> <chr>   \n##  1 audi         a4          1999 very old\n##  2 audi         a4          1999 very old\n##  3 audi         a4          2008 new     \n##  4 audi         a4          2008 new     \n##  5 audi         a4          1999 very old\n##  6 audi         a4          1999 very old\n##  7 audi         a4          2008 new     \n##  8 audi         a4 quattro  1999 very old\n##  9 audi         a4 quattro  1999 very old\n## 10 audi         a4 quattro  2008 new     \n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"rowwise","chapter":"29 Tidyverse Cheatsheet","heading":"29.5 rowwise()","text":"rowwise() can process data row., can another way.can use rowwise() get max/min/sum/sd .","code":"\nmpg %>% \n  select_if(is.numeric) %>% \n  rowwise() %>% \n  mutate(mean = mean(c_across(displ:hwy)))## # A tibble: 234 × 6\n## # Rowwise: \n##    displ  year   cyl   cty   hwy  mean\n##    <dbl> <int> <int> <int> <int> <dbl>\n##  1   1.8  1999     4    18    29  410.\n##  2   1.8  1999     4    21    29  411.\n##  3   2    2008     4    20    31  413 \n##  4   2    2008     4    21    30  413 \n##  5   2.8  1999     6    16    26  410.\n##  6   2.8  1999     6    18    26  410.\n##  7   3.1  2008     6    18    27  412.\n##  8   1.8  1999     4    18    26  410.\n##  9   1.8  1999     4    16    25  409.\n## 10   2    2008     4    20    28  412.\n## # … with 224 more rows\nmpg %>% \n  rowwise() %>% \n  mutate(mean=rowMeans(across(where(is.numeric))))## # A tibble: 234 × 12\n## # Rowwise: \n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # … with 224 more rows, and 1 more variable: mean <dbl>\nmpg %>% \n  rowwise() %>% \n  mutate(sum=sum(across(where(is.numeric))))## # A tibble: 234 × 12\n## # Rowwise: \n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # … with 224 more rows, and 1 more variable: sum <dbl>"},{"path":"tidyverse-cheatsheet.html","id":"arrange","chapter":"29 Tidyverse Cheatsheet","heading":"29.6 arrange()","text":"arrange() can sort data.desc() can help sort data descending order.","code":"\nmpg %>% \n  arrange(displ)## # A tibble: 234 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 honda        civic        1.6  1999     4 manu… f        28    33 r     subc…\n##  2 honda        civic        1.6  1999     4 auto… f        24    32 r     subc…\n##  3 honda        civic        1.6  1999     4 manu… f        25    32 r     subc…\n##  4 honda        civic        1.6  1999     4 manu… f        23    29 p     subc…\n##  5 honda        civic        1.6  1999     4 auto… f        24    32 r     subc…\n##  6 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  7 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 honda        civic        1.8  2008     4 manu… f        26    34 r     subc…\n## # … with 224 more rows\nmpg %>% \n  arrange(desc(displ))## # A tibble: 234 × 11\n##    manufacturer model     displ  year   cyl trans  drv     cty   hwy fl    class\n##    <chr>        <chr>     <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n##  1 chevrolet    corvette    7    2008     8 manua… r        15    24 p     2sea…\n##  2 chevrolet    k1500 ta…   6.5  1999     8 auto(… 4        14    17 d     suv  \n##  3 chevrolet    corvette    6.2  2008     8 manua… r        16    26 p     2sea…\n##  4 chevrolet    corvette    6.2  2008     8 auto(… r        15    25 p     2sea…\n##  5 jeep         grand ch…   6.1  2008     8 auto(… 4        11    14 p     suv  \n##  6 chevrolet    c1500 su…   6    2008     8 auto(… r        12    17 r     suv  \n##  7 dodge        durango …   5.9  1999     8 auto(… 4        11    15 r     suv  \n##  8 dodge        ram 1500…   5.9  1999     8 auto(… 4        11    15 r     pick…\n##  9 chevrolet    c1500 su…   5.7  1999     8 auto(… r        13    17 r     suv  \n## 10 chevrolet    corvette    5.7  1999     8 manua… r        16    26 p     2sea…\n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"distinct","chapter":"29 Tidyverse Cheatsheet","heading":"29.7 distinct()","text":"distinct() can help delete duplicate rows.","code":"\ndf <- tibble(\n  x=sample(5,20,rep=TRUE),\n  y=sample(5,20,rep=TRUE)\n)\ndf %>% \n  distinct()## # A tibble: 11 × 2\n##        x     y\n##    <int> <int>\n##  1     5     1\n##  2     1     4\n##  3     3     3\n##  4     2     3\n##  5     4     2\n##  6     5     4\n##  7     4     3\n##  8     1     3\n##  9     1     5\n## 10     5     2\n## 11     3     5"},{"path":"tidyverse-cheatsheet.html","id":"rename","chapter":"29 Tidyverse Cheatsheet","heading":"29.8 rename()","text":"rename() can change column name.","code":"\nmpg %>% \n  rename(YEAR=year)## # A tibble: 234 × 11\n##    manufacturer model      displ  YEAR   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"relocate","chapter":"29 Tidyverse Cheatsheet","heading":"29.9 relocate()","text":"relocate() can help change order columns.","code":"\nmpg %>% \n  relocate(year)## # A tibble: 234 × 11\n##     year manufacturer model      displ   cyl trans drv     cty   hwy fl    class\n##    <int> <chr>        <chr>      <dbl> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1  1999 audi         a4           1.8     4 auto… f        18    29 p     comp…\n##  2  1999 audi         a4           1.8     4 manu… f        21    29 p     comp…\n##  3  2008 audi         a4           2       4 manu… f        20    31 p     comp…\n##  4  2008 audi         a4           2       4 auto… f        21    30 p     comp…\n##  5  1999 audi         a4           2.8     6 auto… f        16    26 p     comp…\n##  6  1999 audi         a4           2.8     6 manu… f        18    26 p     comp…\n##  7  2008 audi         a4           3.1     6 auto… f        18    27 p     comp…\n##  8  1999 audi         a4 quattro   1.8     4 manu… 4        18    26 p     comp…\n##  9  1999 audi         a4 quattro   1.8     4 auto… 4        16    25 p     comp…\n## 10  2008 audi         a4 quattro   2       4 manu… 4        20    28 p     comp…\n## # … with 224 more rows\nmpg %>% \n  relocate(year,.after = manufacturer)## # A tibble: 234 × 11\n##    manufacturer  year model      displ   cyl trans drv     cty   hwy fl    class\n##    <chr>        <int> <chr>      <dbl> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi          1999 a4           1.8     4 auto… f        18    29 p     comp…\n##  2 audi          1999 a4           1.8     4 manu… f        21    29 p     comp…\n##  3 audi          2008 a4           2       4 manu… f        20    31 p     comp…\n##  4 audi          2008 a4           2       4 auto… f        21    30 p     comp…\n##  5 audi          1999 a4           2.8     6 auto… f        16    26 p     comp…\n##  6 audi          1999 a4           2.8     6 manu… f        18    26 p     comp…\n##  7 audi          2008 a4           3.1     6 auto… f        18    27 p     comp…\n##  8 audi          1999 a4 quattro   1.8     4 manu… 4        18    26 p     comp…\n##  9 audi          1999 a4 quattro   1.8     4 auto… 4        16    25 p     comp…\n## 10 audi          2008 a4 quattro   2       4 manu… 4        20    28 p     comp…\n## # … with 224 more rows\nmpg %>% \n  relocate(year,.before = displ)## # A tibble: 234 × 11\n##    manufacturer model       year displ   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <int> <dbl> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4          1999   1.8     4 auto… f        18    29 p     comp…\n##  2 audi         a4          1999   1.8     4 manu… f        21    29 p     comp…\n##  3 audi         a4          2008   2       4 manu… f        20    31 p     comp…\n##  4 audi         a4          2008   2       4 auto… f        21    30 p     comp…\n##  5 audi         a4          1999   2.8     6 auto… f        16    26 p     comp…\n##  6 audi         a4          1999   2.8     6 manu… f        18    26 p     comp…\n##  7 audi         a4          2008   3.1     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro  1999   1.8     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro  1999   1.8     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro  2008   2       4 manu… 4        20    28 p     comp…\n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"drop_na","chapter":"29 Tidyverse Cheatsheet","heading":"29.10 drop_na()","text":"drop_na can help drop missing value.","code":"\ndf <- tibble(\n  x=c(1,NA,2,3),\n  y=c(4,5,NA,6)\n)\ndf %>% \n  drop_na()## # A tibble: 2 × 2\n##       x     y\n##   <dbl> <dbl>\n## 1     1     4\n## 2     3     6"},{"path":"tidyverse-cheatsheet.html","id":"pull","chapter":"29 Tidyverse Cheatsheet","heading":"29.11 pull()","text":"pull() can help get single column.","code":"\nmpg %>% \n  pull(year)##   [1] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 2008\n##  [16] 1999 2008 2008 2008 2008 2008 1999 2008 1999 1999 2008 2008 2008 2008 2008\n##  [31] 1999 1999 1999 2008 1999 2008 2008 1999 1999 1999 1999 2008 2008 2008 1999\n##  [46] 1999 2008 2008 2008 2008 1999 1999 2008 2008 2008 1999 1999 1999 2008 2008\n##  [61] 2008 1999 2008 1999 2008 2008 2008 2008 2008 2008 1999 1999 2008 1999 1999\n##  [76] 1999 2008 1999 1999 1999 2008 2008 1999 1999 1999 1999 1999 2008 1999 2008\n##  [91] 1999 1999 2008 2008 1999 1999 2008 2008 2008 1999 1999 1999 1999 1999 2008\n## [106] 2008 2008 2008 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 2008\n## [121] 2008 2008 2008 2008 1999 1999 2008 2008 2008 2008 1999 2008 2008 1999 1999\n## [136] 1999 2008 1999 2008 2008 1999 1999 1999 2008 2008 2008 2008 1999 1999 2008\n## [151] 1999 1999 2008 2008 1999 1999 1999 2008 2008 1999 1999 2008 2008 2008 2008\n## [166] 1999 1999 1999 1999 2008 2008 2008 2008 1999 1999 1999 1999 2008 2008 1999\n## [181] 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 1999 1999\n## [196] 1999 2008 2008 1999 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008\n## [211] 2008 1999 1999 1999 1999 2008 2008 2008 2008 1999 1999 1999 1999 1999 1999\n## [226] 2008 2008 1999 1999 2008 2008 1999 1999 2008"},{"path":"tidyverse-cheatsheet.html","id":"unite","chapter":"29 Tidyverse Cheatsheet","heading":"29.12 unite()","text":"unite() can help unite columns.","code":"\nmpg %>% \n  unite(col=\"newcolumn\",manufacturer:model,sep=\",\")->newdf\n\nnewdf## # A tibble: 234 × 10\n##    newcolumn       displ  year   cyl trans      drv     cty   hwy fl    class  \n##    <chr>           <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr>  \n##  1 audi,a4           1.8  1999     4 auto(l5)   f        18    29 p     compact\n##  2 audi,a4           1.8  1999     4 manual(m5) f        21    29 p     compact\n##  3 audi,a4           2    2008     4 manual(m6) f        20    31 p     compact\n##  4 audi,a4           2    2008     4 auto(av)   f        21    30 p     compact\n##  5 audi,a4           2.8  1999     6 auto(l5)   f        16    26 p     compact\n##  6 audi,a4           2.8  1999     6 manual(m5) f        18    26 p     compact\n##  7 audi,a4           3.1  2008     6 auto(av)   f        18    27 p     compact\n##  8 audi,a4 quattro   1.8  1999     4 manual(m5) 4        18    26 p     compact\n##  9 audi,a4 quattro   1.8  1999     4 auto(l5)   4        16    25 p     compact\n## 10 audi,a4 quattro   2    2008     4 manual(m6) 4        20    28 p     compact\n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"separate","chapter":"29 Tidyverse Cheatsheet","heading":"29.13 separate()","text":"separate() can help separate one column several columns.","code":"\nnewdf %>% \n  separate(\"newcolumn\",into=c(\"manufacturer\",\"model\"),sep=\",\")## # A tibble: 234 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # … with 224 more rows"},{"path":"tidyverse-cheatsheet.html","id":"pivot_longer","chapter":"29 Tidyverse Cheatsheet","heading":"29.14 pivot_longer()","text":"pivot_longer() “lengthens” data, increasing number rows decreasing number columns.","code":"\ndf <- tibble(index=c(1,2),x=c('a','b'),y=c('c','d'),z=c('e','f'))\ndf## # A tibble: 2 × 4\n##   index x     y     z    \n##   <dbl> <chr> <chr> <chr>\n## 1     1 a     c     e    \n## 2     2 b     d     f\ndf2 <- df %>% \n  pivot_longer(cols=!index,names_to=\"key\",values_to=\"val\")\ndf2## # A tibble: 6 × 3\n##   index key   val  \n##   <dbl> <chr> <chr>\n## 1     1 x     a    \n## 2     1 y     c    \n## 3     1 z     e    \n## 4     2 x     b    \n## 5     2 y     d    \n## 6     2 z     f"},{"path":"tidyverse-cheatsheet.html","id":"pivot_wider","chapter":"29 Tidyverse Cheatsheet","heading":"29.15 pivot_wider()","text":"pivot_wider() “widens” data, increasing number columns decreasing number rows.","code":"\ndf2 %>% \n  pivot_wider(names_from=key,values_from=val)## # A tibble: 2 × 4\n##   index x     y     z    \n##   <dbl> <chr> <chr> <chr>\n## 1     1 a     c     e    \n## 2     2 b     d     f"},{"path":"tidyverse-cheatsheet.html","id":"count","chapter":"29 Tidyverse Cheatsheet","heading":"29.16 count()","text":"count() can count number occurences.","code":"\nmpg %>% \n  count(model,sort=TRUE)## # A tibble: 38 × 2\n##    model                   n\n##    <chr>               <int>\n##  1 caravan 2wd            11\n##  2 ram 1500 pickup 4wd    10\n##  3 civic                   9\n##  4 dakota pickup 4wd       9\n##  5 jetta                   9\n##  6 mustang                 9\n##  7 a4 quattro              8\n##  8 grand cherokee 4wd      8\n##  9 impreza awd             8\n## 10 a4                      7\n## # … with 28 more rows"},{"path":"tidyverse-cheatsheet.html","id":"summarize","chapter":"29 Tidyverse Cheatsheet","heading":"29.17 summarize()","text":"summarise() typically used grouped data created group_by(). output one row group.summarise_all() applies functions (non-grouping) columns.summarise_if() operate columns predicate returns TRUE.summarise_at() allow select columns.","code":"\nmpg %>% \n  group_by(model) %>% \n  summarise(count=n(),averagedispl=mean(displ),maxdispl=max(displ))## # A tibble: 38 × 4\n##    model              count averagedispl maxdispl\n##    <chr>              <int>        <dbl>    <dbl>\n##  1 4runner 4wd            6         3.48      4.7\n##  2 a4                     7         2.33      3.1\n##  3 a4 quattro             8         2.42      3.1\n##  4 a6 quattro             3         3.37      4.2\n##  5 altima                 6         2.8       3.5\n##  6 c1500 suburban 2wd     5         5.52      6  \n##  7 camry                  7         2.67      3.5\n##  8 camry solara           7         2.64      3.3\n##  9 caravan 2wd           11         3.39      4  \n## 10 civic                  9         1.71      2  \n## # … with 28 more rows\nmpg %>% \n  group_by(model) %>% \n  select_if(is.numeric) %>% \n  summarise_all(mean)## # A tibble: 38 × 6\n##    model              displ  year   cyl   cty   hwy\n##    <chr>              <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 4runner 4wd         3.48 2002   5.67  15.2  18.8\n##  2 a4                  2.33 2003.  4.86  18.9  28.3\n##  3 a4 quattro          2.42 2004.  5     17.1  25.8\n##  4 a6 quattro          3.37 2005   6.67  16    24  \n##  5 altima              2.8  2005   4.67  20.7  28.7\n##  6 c1500 suburban 2wd  5.52 2006.  8     12.8  17.8\n##  7 camry               2.67 2003.  4.86  19.9  28.3\n##  8 camry solara        2.64 2003.  4.86  19.9  28.1\n##  9 caravan 2wd         3.39 2003.  5.82  15.8  22.4\n## 10 civic               1.71 2003   4     24.4  32.6\n## # … with 28 more rows\nmpg %>% \n  group_by(model) %>% \n  summarise_if(is.numeric,mean)## # A tibble: 38 × 6\n##    model              displ  year   cyl   cty   hwy\n##    <chr>              <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 4runner 4wd         3.48 2002   5.67  15.2  18.8\n##  2 a4                  2.33 2003.  4.86  18.9  28.3\n##  3 a4 quattro          2.42 2004.  5     17.1  25.8\n##  4 a6 quattro          3.37 2005   6.67  16    24  \n##  5 altima              2.8  2005   4.67  20.7  28.7\n##  6 c1500 suburban 2wd  5.52 2006.  8     12.8  17.8\n##  7 camry               2.67 2003.  4.86  19.9  28.3\n##  8 camry solara        2.64 2003.  4.86  19.9  28.1\n##  9 caravan 2wd         3.39 2003.  5.82  15.8  22.4\n## 10 civic               1.71 2003   4     24.4  32.6\n## # … with 28 more rows\nmpg %>% \n  group_by(model) %>% \n  summarise_at(vars(displ,cyl),mean)## # A tibble: 38 × 3\n##    model              displ   cyl\n##    <chr>              <dbl> <dbl>\n##  1 4runner 4wd         3.48  5.67\n##  2 a4                  2.33  4.86\n##  3 a4 quattro          2.42  5   \n##  4 a6 quattro          3.37  6.67\n##  5 altima              2.8   4.67\n##  6 c1500 suburban 2wd  5.52  8   \n##  7 camry               2.67  4.86\n##  8 camry solara        2.64  4.86\n##  9 caravan 2wd         3.39  5.82\n## 10 civic               1.71  4   \n## # … with 28 more rows"},{"path":"learning-sql-with-its-r-dplyr-translation.html","id":"learning-sql-with-its-r-dplyr-translation","chapter":"30 Learning SQL with Its R dplyr Translation","heading":"30 Learning SQL with Its R dplyr Translation","text":"Hang Xu (hx2321)Students usually familiar R Python use often Statistic classes. However, Data Science Data Analyst interviews test SQL skills SQL mostly used industry pulling data companies’ database. Thus, like provide one page Cheat Sheet introduces SQL R translation better understanding. purpose students can know corresponding syntax SQL trying overcome interview questions.GitHub Repository LinkThere pdf file containing cheat sheet.pdf file containing cheat sheet.also readme file better explains motivation.also readme file better explains motivation.","code":""},{"path":"animate-time-series-the-goyav-package.html","id":"animate-time-series-the-goyav-package","chapter":"31 Animate Time Series : the goyav package","heading":"31 Animate Time Series : the goyav package","text":"Antonin Vidon","code":"\n# install.packages(\"devtools\")\n# library(devtools)\n# install_github(\"AntoninVidon/goyav\")\nlibrary(goyav) # must be installed from source\n\n# install.packages(\"gapminder\")\nlibrary(gapminder)"},{"path":"animate-time-series-the-goyav-package.html","id":"motivation","chapter":"31 Animate Time Series : the goyav package","heading":"31.1 Motivation","text":"One main drawbacks IDE RStudio lack real interaction user plots generated raw data. Even simple plots, user often go lot documentation /look heavy web located tutorials.build shiny-app based interface user tunes dynamically plot wants build offering choices among true candidates parameter (error possible). choose focus “animated” plots. dynamic plots generated temporal data. type plot especially tedious build “hard-code” way gganimate package. add interesting features filter factor, choose x,y temporal range, change /scales log-scales, modify duration animation, … common operations meticulous user might want perform order draw impactful plots.","code":""},{"path":"animate-time-series-the-goyav-package.html","id":"features","chapter":"31 Animate Time Series : the goyav package","heading":"31.2 Features","text":"goyav shiny App meant generate GIF animations tabular temporal data. customization animated plots done interactive window.dashboard application looks like :user able navigate two tabs : “Animate” “Advanced animate”, latter offering wider range customization.Animate tab, may choose following :X variable (numeric);Y variable (numeric);size variable (numeric);color variable (optional, factor);temporal variable (numeric);choose want apply log-scale either X, Y X Y (optional).Advanced animated tab, may choose following :X variable (numeric);Y variable (numeric);size variable (numeric);color variable (optional, factor);temporal variable (numeric);choose want apply log-scale either X, Y X Y (optional);X range;Y range;temporal range;Animation duration (s);Factor levels include (optional, among color variable levels).","code":""},{"path":"animate-time-series-the-goyav-package.html","id":"demonstration","chapter":"31 Animate Time Series : the goyav package","heading":"31.3 Demonstration","text":"order better show use package, use gapminder dataset gapminder package.order able create GIF, dataframe candidate columns following variables: X, Y, size temporal. Therefore, calling goyav function dataframe less 4 numeric variables return error.Let’s now call goyav whole dataset :","code":"\n# display first rows of dataset\nknitr::kable(head(gapminder))\n# try to call the `goyav` function on the first 3 columns of the `gapminder` dataset\ngoyav(gapminder[,1:3])## Error in goyav(gapminder[, 1:3]): Your dataframe should have at least 4 numeric variables.\n# call `goyav` on `gapminder`\ngoyav(gapminder)"},{"path":"animate-time-series-the-goyav-package.html","id":"view-from-the-animate-tab","chapter":"31 Animate Time Series : the goyav package","heading":"31.3.1 View from the Animate tab","text":"","code":""},{"path":"animate-time-series-the-goyav-package.html","id":"view-from-the-advanced-animate-tab","chapter":"31 Animate Time Series : the goyav package","heading":"31.3.2 View from the Advanced animate tab","text":"","code":""},{"path":"string-manipulation-in-r.html","id":"string-manipulation-in-r","chapter":"32 String Manipulation in R","heading":"32 String Manipulation in R","text":"Xiaoyi ZhangMaking informative charts graphs common way get actionable insights datasets. However, often times, data, especially form strings, cleaned format want. Therefore, create cheat sheet basic string manipulations creating string, escaping special characters, joining multiple strings, matching strings pattern R.Access cheat sheet:\nresources/stringr_cheatsheet/stringr_cheatsheet.pdf","code":""},{"path":"ggalluvial-cheatsheet.html","id":"ggalluvial-cheatsheet","chapter":"33 ggalluvial Cheatsheet","heading":"33 ggalluvial Cheatsheet","text":"Meggie Wen##Intro ggalluvial:ggalluvial package \nggplot2 extension producing alluvial plots. Alluvial plots use variable-width ribbons stacked bar plots represent multi-dimensional repeated-measures data categorical ordinal variables.","code":""},{"path":"ggalluvial-cheatsheet.html","id":"description","chapter":"33 ggalluvial Cheatsheet","heading":"33.1 Description:","text":"Alluvial plots consist multiple horizontally-distributed columns (axes) representing factor variables, vertical divisions (strata) axes representing variables’ values; splines (alluvial flows) connecting vertical subdivisions (lodes) within strata adjacent axes representing subsets amounts observations take corresponding values corresponding variables.dataframe originally created illustration:Basic Alluvial Plot:ggplot(df, aes(axis1 = x1, axis2 = x2, y = Freq)) +geom_alluvium() +geom_stratum() +geom_text(stat = “stratum”, aes(label = paste(after_stat(stratum))))Example 1:Add color:• Set lines one color:• Set color categories:• Set stratum color:Add label:scale_x_discrete(limits = c(“Step 1”, “Step 2”))Change curve type:geom_alluvium(color = “blue”, fill = “blue”), curve_type = “linear”)Example 2:Multi-variable Alluvial Plot:ggplot(data = df, aes(axis1 = x1, axis2 = x2, axis3 = x3, …, y = frequency)Example 3:","code":"\nlibrary(tidyverse)\nlibrary(ggalluvial)\ndf <- data.frame(x1 = c(\"A\",\"B\",\"C\",\"A\",\"A\",\"A\",\"B\"),\n                 x2 = c(\"X\",\"X\",\"X\",\"Y\",\"Y\",\"Y\",\"Y\"),\n                 x3 = c(\"1\",\"2\",\"3\",\"3\",\"3\",\"3\",\"3\"),\n                 Freq = c(1,2,3,4,5,6,7))\nggplot(df, aes(axis1 = x1, axis2 = x2, y = Freq)) +\n  geom_alluvium() +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum))))geom_alluvium(color = “blue”, fill = “blue”)geom_alluvium(aes(color = x1, fill = x1))geom_stratum(aes(fill = x1))\nggplot(df, aes(axis1 = x1, axis2 = x2, axis3 = x3, y = Freq)) +\n  geom_alluvium(color = \"blue\", fill = \"blue\",curve_type = \"linear\") +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) +\n  scale_x_discrete(limits = c(\"Step 1\", \"Step 2\")) \nggplot(df, aes(axis1 = x1, axis2 = x2, axis3 = x3, y = Freq)) +\n  geom_alluvium(aes(color = x1, fill = x1)) +\n  geom_stratum(aes(color = x1)) +\n  geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) +\n  scale_x_discrete(limits = c(\"Step 1\", \"Step 2\")) "},{"path":"a-cheatsheet-from-pandas-to-base-r-and-tidyverse.html","id":"a-cheatsheet-from-pandas-to-base-r-and-tidyverse","chapter":"34 A cheatsheet from pandas to base r and tidyverse","heading":"34 A cheatsheet from pandas to base r and tidyverse","text":"Megan ZhouThis link cheatsheet created:\nhttps://github.com/meganzhou62/stat5702cc/blob/main/cheatsheet.pdf","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r-ggplot2-python-graphing-performances","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35 Base R, ggplot2, & Python Graphing Performances","text":"Yan Gong, Ziyan Liu","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"preface","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.1 Preface","text":"One powerful reason data scientists consider R efficient tool ability produce wide range plots quickly easily data visualizations. However, Python introduced data science projects, combination several data processing, ploting, exploration packages, including numpy, pandas, matplotlib, seaborn, become another option data visualizations , data explorations. meantime, base R well-known R plotting package, ggplot2, also received multiple updates improved data visualization performances. goal cheatsheet introduce several functions, one base R, ggplot2, Python data science packages, compare results five general type plots data analysis: basic histogram, line graph regression, scatterplot legend, boxplot reordered/formatted axes, boxplot error bars.order represent use packages Python libraries compare regular approaches R (base R ggplot2 package), Python session embedded within basic R session achieve functionality R-based platform, RStudio. Thanks openness diversity R libraries, R package called reticulate, enabling seamless, high-performance interoperability R Python. According documentation reticulate, package functionality :Calling Python R variety ways including R Markdown, sourcing Python scripts, importing Python modules, using Python interactively within R session;Calling Python R variety ways including R Markdown, sourcing Python scripts, importing Python modules, using Python interactively within R session;Translation R Python objects (example, R Pandas data frames, R matrices NumPy arrays);Translation R Python objects (example, R Pandas data frames, R matrices NumPy arrays);Flexible binding different versions Python including virtual environments Conda environments.Flexible binding different versions Python including virtual environments Conda environments.Similar base R ggplot2, functions numpy, pandas, matplotlib, seaborn called generate five main kinds plots.","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"preparation-1","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.2 Preparation","text":"","code":"\n# Clean up workspace environment\nrm(list = ls())\n\n# Call packages used for the assignment\nlibrary(reticulate) # Enable Python usage in R\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gplots)\nset.seed(100)"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"python-environment-setup","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.3 Python Environment Setup","text":"running Python code chunks R markdown, RStudio requires installation miniconda, small, bootstrap version Anaconda conda, Python, packages depend , small number useful packages. default settings, installation Miniconda making default environment forced, unless user explicitly requested particular version Python one use_*() helpers.install miniconda RStudio, check meta path (working directory miniconda) reticulate:::miniconda_meta_path():install miniconda reticulate::install_miniconda():Import Python packages neeeded:Basically, magic funtion (%) applied globally Python, plots figures generated matplotlib shown automatically notebooks:However, seems like magic functions work Python environments platforms. matplotlib.pyplot.show() used figure alternatives.","code":"> library(reticulate)\n> reticulate:::miniconda_meta_path()\n[1] \"C:\\\\Users\\\\Shiro\\\\AppData\\\\Local/r-reticulate/r-reticulate/miniconda.json\"> reticulate::install_miniconda()\n* Downloading \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\" ...\ntrying URL 'https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe'\nContent type 'application/octet-stream' length 60924672 bytes (58.1 MB)\ndownloaded 58.1 MB\n\n* Installing Miniconda -- please wait a moment ...\nCollecting package metadata (current_repodata.json): ...working... done\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: C:\\Users\\Shiro\\AppData\\Local\\R-MINI~1\n\n  added / updated specs:\n    - conda\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2021.10.26 |       haa95532_2         115 KB\n    certifi-2021.10.8          |   py39haa95532_0         152 KB\n    charset-normalizer-2.0.4   |     pyhd3eb1b0_0          35 KB\n    cryptography-35.0.0        |   py39h71e12ea_0         991 KB\n    idna-3.2                   |     pyhd3eb1b0_0          48 KB\n    openssl-1.1.1l             |       h2bbff1b_0         4.8 MB\n    pip-21.2.4                 |   py39haa95532_0         1.8 MB\n    pyopenssl-21.0.0           |     pyhd3eb1b0_1          49 KB\n    pywin32-228                |   py39hbaba5e8_1         5.6 MB\n    requests-2.26.0            |     pyhd3eb1b0_0          59 KB\n    setuptools-58.0.4          |   py39haa95532_0         778 KB\n    tqdm-4.62.3                |     pyhd3eb1b0_1          83 KB\n    tzdata-2021e               |       hda174b7_0         112 KB\n    urllib3-1.26.7             |     pyhd3eb1b0_0         111 KB\n    wheel-0.37.0               |     pyhd3eb1b0_1          33 KB\n    wincertstore-0.2           |   py39haa95532_2          15 KB\n    ------------------------------------------------------------\n                                           Total:        14.8 MB\n\nThe following NEW packages will be INSTALLED:\n\n  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n\nThe following packages will be REMOVED:\n\n  chardet-4.0.0-py39haa95532_1003\n\nThe following packages will be UPDATED:\n\n  ca-certificates                       2021.7.5-haa95532_1 --> 2021.10.26-haa95532_2\n  certifi                          2021.5.30-py39haa95532_0 --> 2021.10.8-py39haa95532_0\n  cryptography                         3.4.7-py39h71e12ea_0 --> 35.0.0-py39h71e12ea_0\n  idna                                    2.10-pyhd3eb1b0_0 --> 3.2-pyhd3eb1b0_0\n  openssl                                 1.1.1k-h2bbff1b_0 --> 1.1.1l-h2bbff1b_0\n  pip                                 21.1.3-py39haa95532_0 --> 21.2.4-py39haa95532_0\n  pyopenssl                             20.0.1-pyhd3eb1b0_1 --> 21.0.0-pyhd3eb1b0_1\n  pywin32                                228-py39he774522_0 --> 228-py39hbaba5e8_1\n  requests                              2.25.1-pyhd3eb1b0_0 --> 2.26.0-pyhd3eb1b0_0\n  setuptools                          52.0.0-py39haa95532_0 --> 58.0.4-py39haa95532_0\n  tqdm                                  4.61.2-pyhd3eb1b0_1 --> 4.62.3-pyhd3eb1b0_1\n  tzdata                                   2021a-h52ac0ba_0 --> 2021e-hda174b7_0\n  urllib3                               1.26.6-pyhd3eb1b0_1 --> 1.26.7-pyhd3eb1b0_0\n  wheel                                 0.36.2-pyhd3eb1b0_0 --> 0.37.0-pyhd3eb1b0_1\n  wincertstore                           0.2-py39h2bbff1b_0 --> 0.2-py39haa95532_2\n\n\n\nDownloading and Extracting Packages\ncertifi-2021.10.8    | 152 KB    | ########## | 100% \npip-21.2.4           | 1.8 MB    | ########## | 100% \npywin32-228          | 5.6 MB    | ########## | 100% \nurllib3-1.26.7       | 111 KB    | ########## | 100% \nopenssl-1.1.1l       | 4.8 MB    | ########## | 100% \nrequests-2.26.0      | 59 KB     | ########## | 100% \ntqdm-4.62.3          | 83 KB     | ########## | 100% \nwincertstore-0.2     | 15 KB     | ########## | 100% \nidna-3.2             | 48 KB     | ########## | 100% \ntzdata-2021e         | 112 KB    | ########## | 100% \ncryptography-35.0.0  | 991 KB    | ########## | 100% \nsetuptools-58.0.4    | 778 KB    | ########## | 100% \npyopenssl-21.0.0     | 49 KB     | ########## | 100% \ncharset-normalizer-2 | 35 KB     | ########## | 100% \nwheel-0.37.0         | 33 KB     | ########## | 100% \nca-certificates-2021 | 115 KB    | ########## | 100% \nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\nCollecting package metadata (current_repodata.json): ...working... done\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: C:\\Users\\Shiro\\AppData\\Local\\R-MINI~1\\envs\\r-reticulate\n\n  added / updated specs:\n    - numpy\n    - python=3.6\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    certifi-2020.12.5          |   py36haa95532_0         140 KB\n    intel-openmp-2021.4.0      |    h57928b3_3556         3.2 MB  conda-forge\n    libblas-3.9.0              |     12_win64_mkl         4.5 MB  conda-forge\n    libcblas-3.9.0             |     12_win64_mkl         4.5 MB  conda-forge\n    liblapack-3.9.0            |     12_win64_mkl         4.5 MB  conda-forge\n    mkl-2021.4.0               |     h0e2418a_729       181.7 MB  conda-forge\n    numpy-1.19.5               |   py36h4b40d73_2         4.9 MB  conda-forge\n    pip-21.3.1                 |     pyhd8ed1ab_0         1.2 MB  conda-forge\n    python-3.6.13              |h39d44d4_2_cpython        18.9 MB  conda-forge\n    python_abi-3.6             |          2_cp36m           4 KB  conda-forge\n    setuptools-49.6.0          |   py36ha15d459_3         921 KB  conda-forge\n    tbb-2021.4.0               |       h2d74725_0         149 KB  conda-forge\n    ucrt-10.0.20348.0          |       h57928b3_0         1.2 MB  conda-forge\n    vc-14.2                    |       hb210afc_5          13 KB  conda-forge\n    vs2015_runtime-14.29.30037 |       h902a5da_5         1.3 MB  conda-forge\n    wheel-0.37.0               |     pyhd8ed1ab_1          31 KB  conda-forge\n    wincertstore-0.2           |py36ha15d459_1006          15 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:       227.0 MB\n\nThe following NEW packages will be INSTALLED:\n\n  certifi            pkgs/main/win-64::certifi-2020.12.5-py36haa95532_0\n  intel-openmp       conda-forge/win-64::intel-openmp-2021.4.0-h57928b3_3556\n  libblas            conda-forge/win-64::libblas-3.9.0-12_win64_mkl\n  libcblas           conda-forge/win-64::libcblas-3.9.0-12_win64_mkl\n  liblapack          conda-forge/win-64::liblapack-3.9.0-12_win64_mkl\n  mkl                conda-forge/win-64::mkl-2021.4.0-h0e2418a_729\n  numpy              conda-forge/win-64::numpy-1.19.5-py36h4b40d73_2\n  pip                conda-forge/noarch::pip-21.3.1-pyhd8ed1ab_0\n  python             conda-forge/win-64::python-3.6.13-h39d44d4_2_cpython\n  python_abi         conda-forge/win-64::python_abi-3.6-2_cp36m\n  setuptools         conda-forge/win-64::setuptools-49.6.0-py36ha15d459_3\n  tbb                conda-forge/win-64::tbb-2021.4.0-h2d74725_0\n  ucrt               conda-forge/win-64::ucrt-10.0.20348.0-h57928b3_0\n  vc                 conda-forge/win-64::vc-14.2-hb210afc_5\n  vs2015_runtime     conda-forge/win-64::vs2015_runtime-14.29.30037-h902a5da_5\n  wheel              conda-forge/noarch::wheel-0.37.0-pyhd8ed1ab_1\n  wincertstore       conda-forge/win-64::wincertstore-0.2-py36ha15d459_1006\n\n\n\nDownloading and Extracting Packages\npython-3.6.13        | 18.9 MB   | ########## | 100% \nlibcblas-3.9.0       | 4.5 MB    | ########## | 100% \nvs2015_runtime-14.29 | 1.3 MB    | ########## | 100% \nmkl-2021.4.0         | 181.7 MB  | ########## | 100% \nwincertstore-0.2     | 15 KB     | ########## | 100% \ntbb-2021.4.0         | 149 KB    | ########## | 100% \nnumpy-1.19.5         | 4.9 MB    | ########## | 100% \nsetuptools-49.6.0    | 921 KB    | ########## | 100% \nwheel-0.37.0         | 31 KB     | ########## | 100% \nucrt-10.0.20348.0    | 1.2 MB    | ########## | 100% \npython_abi-3.6       | 4 KB      | ########## | 100% \ncertifi-2020.12.5    | 140 KB    | ########## | 100% \npip-21.3.1           | 1.2 MB    | ########## | 100% \nvc-14.2              | 13 KB     | ########## | 100% \nintel-openmp-2021.4. | 3.2 MB    | ########## | 100% \nlibblas-3.9.0        | 4.5 MB    | ########## | 100% \nliblapack-3.9.0      | 4.5 MB    | ########## | 100% \nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n#\n# To activate this environment, use\n#\n#     $ conda activate r-reticulate\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n* Miniconda has been successfully installed at \"C:/Users/Shiro/AppData/Local/r-miniconda\".\n[1] \"C:/Users/Shiro/AppData/Local/r-miniconda\"# numpy was automatically installed with miniconda\nimport numpy as np\n\n# install pandas by py_install('pandas')\nimport pandas as pd\n\n# install matplotlib by py_install('matplotlib')\nimport matplotlib.pyplot as plt  # interactive plot: use `plotly` package\n\n# install seaborn by py_install('seaborn')\nimport seaborn as sns\n\n# Set style globally\nsns.set_style('darkgrid')%matplotlib inline"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r-plot-function","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.4 Base R Plot Function","text":"many parameters can change within plot function adjust appearance plot. example, different pch values give different shapes points shown figure.\n","code":"\nX <- 1:20\nY <- X*X\n\npar(mfrow=c(3,3), mar=c(2,3,1,0)+.5, mgp=c(1.5,.6,0)) \n# a window that can show 3 * 3 plots at the same time\n\nplot(X, Y, type = \"p\") # \"p\" for points\nplot(X, Y, type = \"l\") # \"l\" for lines\nplot(X, Y, type = \"b\") # \"b\" for both points and lines\nplot(X, Y, type = \"o\") # \"o\" for both overplotted\nplot(X, Y, type = \"c\") # \"c\" for empty points joined by lines\nplot(X, Y, type = \"s\") # \"s\" for stair steps\nplot(X, Y, type = \"h\") # \"h\" for histogram-like vertical lines\nplot(X, Y, type = \"n\") # \"n\" for no plotting\nplot(X, Y, type = \"p\",\n     las = 1,                  # orient y-axis label\n     pch = 15,                 # shape of point\n     cex=1.5,                  # size of point\n     col = 2,                  # color of point\n     xlab = \"x\",               # x axis label\n     ylab = \"y\",               # y axis label\n     main = \"f(x) = x^2\",      # title of plot\n     cex.lab = 1.5,            # size of axis labels\n     cex.axis = 1.2,           # size of tick mark labels\n     cex.main = 2)             # size of main title"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"ggplot2","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.4.1 ggplot2","text":"","code":"\nggplot(data.frame(X,Y), aes(x=X,y=Y)) + geom_point()\nggplot(data.frame(X,Y), aes(x=X,y=Y)) + geom_line()\nggplot(data.frame(X,Y), aes(x=X,y=Y)) + geom_point() + geom_line()\nggplot(data.frame(X,Y), aes(x=X,y=Y)) + geom_step()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"basic-histogram","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.5 Basic Histogram","text":"","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.5.1 base R","text":"Base R provides function hist, can easily create histograms. Notice parameter break bins ggplot2. give integer break, hist takes suggestion number breaks use histogram.","code":"\nset.seed(123)\nx <- rpois(n=50, lambda = 15)\npar(mfrow=c(1,2)) \nhist(x,      \n     breaks = 12,                          # number of cells, suggestion only\n     xlab = \"x\", ylab = \"frequency\")       # axis labels\n\n\nhist(x,      \n     breaks = c(7:21),                     # vector of breakpoints\n     xlab = \"x\", ylab = \"frequency\",       # axis labels\n     las = 1)                              #rotate the y-axis label direction"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"ggplot2-1","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.5.2 ggplot2","text":"","code":"\nggplot(data.frame(x), aes(x=x)) + \n  geom_histogram(bins = 12, color = \"black\", fill = \"gray\") +\n  xlab(\"x\") + ylab(\"frequency\") + \n  ggtitle(\"Histogram of 50 Randomly Generated Numbers\")"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"python","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.5.3 Python","text":"Loading example dataset pandas:pandas seaborn packages used plot histograms. using matplotlib package construct subplots, figures, axes, multiple histograms plotted within figure:","code":"df = pd.read_csv('https://raw.githubusercontent.com/bryanrgibson/eods-f21/main/data/yellowcab_demo_withdaycategories.csv', \n                 sep = ',', \n                 header = 1, \n                 parse_dates = ['pickup_datetime', 'dropoff_datetime'])# Plotting with `pandas`\n# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 2, figsize = (16,4))\n# Subset dataset for trips before the noon, plot histogram by pandas.DataFrame.plot.hist(), and place the plot in the first subplot\ndf[df.pickup_datetime.dt.hour < 12].fare_amount.plot.hist(bins = 100, ax = ax[0]); \n# Set labels and title\nax[0].set_xlabel(\"fare_amount (dollars)\");\nax[0].set_title(\"Trips before Noon\");\n\n# Subset dataset for trips after the noon, plot histogram by pandas.DataFrame.plot.hist(), and place the plot in the second subplot\ndf[df.pickup_datetime.dt.hour >=  12].fare_amount.plot.hist(bins = 100, ax = ax[1]);\n# Set labels and title\nax[1].set_xlabel(\"fare_amount (dollars)\");\nax[1].set_title(\"Trips after Noon\");\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()# Plotting with `seaborn`\n# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 2, figsize = (16,4))\n# Subset dataset for trips before the noon by pandas.DataFrame.query(), plot histogram by seaborn.histplot(), and place the plot in the first subplot\nsns.histplot(x = 'fare_amount', \n             data = df.query(\"pickup_datetime.dt.hour < 12\"), \n             ax = ax[0]);\n# Subset dataset for trips after the noon, plot histogram by seaborn.histplot(), and place the plot in the first subplot\nsns.histplot(x = df.fare_amount[df.pickup_datetime.dt.hour >= 12], \n             ax = ax[1]);\n             \n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"basic-line-graph-with-regression","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.6 Basic Line Graph with Regression","text":"","code":"\nset.seed(123)\nX1 = c(1:30)\nY1 = rnorm(30, mean=20, sd=5)"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r-1","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.6.1 base R","text":"base R, can plot line graphs plot, change type, width color line parameters efficiently. can add linear regression line fitting model draw line abline base R.","code":"\npar(mfrow=c(2,3)) \nplot(X1, Y1, type=\"l\", xlab=\"x\", ylab=\"y\")\nplot(X1, Y1, type=\"l\", xlab=\"x\", ylab=\"y\", lty=\"dashed\")      #\"lty\" for line type\nplot(X1, Y1, type=\"l\", xlab=\"x\", ylab=\"y\", lty=\"dotted\")\nplot(X1, Y1, type=\"l\", xlab=\"x\", ylab=\"y\", col=\"red\")         #\"col\" for color of line\nplot(X1, Y1, type=\"l\", xlab=\"x\", ylab=\"y\", col=\"red\", lwd=3)  #\"lwd\" for \"line width\"\nplot(X1, Y1, type=\"l\", xlab=\"x\", ylab=\"y\", col=\"red\", lwd=2)\nfit1 = lm(Y1~X1)\nabline(fit1, lty = \"dashed\")                                  #add an abline in the last plot"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"ggplot2-2","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.6.2 ggplot2","text":"add linear model fit line ggplot2, use geom_smooth(method = 'lm').","code":"\nggplot(data.frame(X1, Y1), aes(x=X1, y=Y1)) + \n  geom_line(col = \"red\", lwd=0.8) + \n  geom_smooth(method = \"lm\", se = FALSE, col=\"black\", lty=\"dashed\")"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"python-1","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.6.3 Python","text":"Loading example dataset pandas:Although lm() function generate linear model within R session reticulate allows users access datasets stored environment R Python chunks, still easy use model generated R functions plot Python. Instead, whole process done Python, using sklearn package:, plotting part pandas package involved (pandas.DataFrame.plot()).Without constructing linear regression model, instead, seaborn function called seaborn.regplot(), directly add regression line:","code":"df_wine = pd.read_csv('https://raw.githubusercontent.com/bryanrgibson/eods-f21/main/data/wine_dataset.csv', \n                      usecols = ['alcohol','ash','proline','hue','class'])# install scikit-learn (sklearn) by py_install('scikit-learn')\nfrom sklearn.linear_model import LinearRegression\n\n# Instantiate the model and set hyperparameters\nlr = LinearRegression(fit_intercept = True, \n                      normalize = False)     # by default\n\n# Fit the model\n# .reshape(-1, 1): scikit-learn models expect the input features to be 2 dimensional\nlr.fit(X = df_wine.proline.values.reshape(-1, 1), y = df_wine.alcohol);\n\n# Predict values and plot the model\nx_predict = [df_wine.proline.min(), df_wine.proline.max()]\ny_hat = lr.predict(np.array(x_predict).reshape(-1, 1))\nfig,ax = plt.subplots(1, 1, figsize = (12,8))\nax = sns.scatterplot(x = df_wine.proline, y = df_wine.alcohol);\nax.plot(x_predict, y_hat);\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 1, figsize = (12,8))\n# Add regression line\nsns.regplot(x = 'proline', y = 'alcohol', data = df_wine, ax = ax);\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"scatterplot-with-legend","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.7 Scatterplot with Legend","text":"","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r-2","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.7.1 base R","text":"Without specifying type plot, function default creates scatterplots.","code":"\ndata(iris)\nplot(iris$Sepal.Length, iris$Petal.Length,        # x variable, y variable\n     col = iris$Species,                          # color by species\n     pch = 18,                                    # type of point to use\n     cex = 1.5,                                   # size of point to use\n     xlab = \"Sepal Length\",                       # x axis label\n     ylab = \"Petal Length\",                       # y axis label\n     main = \"Flower Characteristics in Iris\")     # plot title\n\nlegend (\"topleft\", legend = levels(iris$Species), col = c(1:3), pch = 18)"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"ggplot2-3","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.7.2 ggplot2","text":"","code":"\nggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color = Species)) +\n  geom_point() + \n  xlab(\"Sepal Length\") +\n  ylab(\"Petal length\") +\n  ggtitle(\"Flower Characteristics in Iris\")"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"python-2","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.7.3 Python","text":"strightforward way make scatterplots use matplotlib.pyplot.scatter() function matplotlib package:add legend, just simply add matplotlib.pyplot.legend():","code":"# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 2, figsize = (12,4))\n# Plot scatterplot by matplotlib.pyplot.scatter(), and place the plot in the first subplot\nax[0].scatter(df.trip_distance, \n              df.fare_amount, \n              marker = 'x', \n              color = 'blue'); \n# Plot scatterplot by matplotlib.pyplot.scatter(), and place the plot in the second subplot\nax[1].scatter(df.trip_distance, \n              df.tip_amount, \n              color = 'red');\n# Set labels and titles\nax[0].set_xlabel('trip_distance');\nax[1].set_xlabel('trip_distance');\nax[0].set_ylabel('fare_amount'), ax[1].set_ylabel('tip_amount'); \nax[0].set_title('trip_distance vs fare_amount'); \nax[1].set_title('trip_distance vs tip_amount'); \nfig.suptitle('Yellowcab Taxi Features'); \n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()# Create figure with `matplotlib`\nfig,ax = plt.subplots(figsize = (16,4))\n# Subset dataset for trips before the noon, plot scatterplot by matplotlib.pyplot.scatter(), and place the plot in the figure\nax.scatter(df[df.pickup_datetime.dt.hour < 12].trip_distance, \n           df[df.pickup_datetime.dt.hour < 12].fare_amount, \n           marker = 'x', \n           color = 'blue', \n           label = 'Before Noon'); \n# Subset dataset for trips after the noon, plot scatterplot by matplotlib.pyplot.scatter(), and place the plot in the figure\nax.scatter(df[df.pickup_datetime.dt.hour >= 12].trip_distance, \n           df[df.pickup_datetime.dt.hour >= 12].fare_amount, \n           marker = 'o', \n           color = 'red', \n           label = 'After Noon'); \n# Set labels and title\nax.set_xlabel(\"trip_distance\");\nax.set_ylabel(\"fare_amount (dollars)\");\nax.set_title(\"Yellowcab Taxi Features\");\n# Set legends\n# Set the location of legends at the best place\nax.legend(loc = 'best')\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"boxplot-with-reordered-and-formatted-axes","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.8 Boxplot with Reordered and Formatted Axes","text":"","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r-3","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.8.1 base R","text":"function boxplot used base R create boxplots, adding notches plot makes easier see difference among data.","code":"\nboxplot(iris$Sepal.Length ~ iris$Species)\nboxplot(iris$Sepal.Length ~ iris$Species, notch=T)\n\nboxplot(iris$Sepal.Length ~ iris$Species, notch=T,\n        xlab = \"Species\",    \n        ylab = \"Sepal Length\",\n        las = 1,\n        main = \"Sepal Length by Species in Iris\") "},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"ggplot2-4","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.8.2 ggplot2","text":"","code":"\nggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"python-3","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.8.3 Python","text":"convenient method draw boxplot using seaborn. result plots show first quantile, second quantile (median), third quantile, whiskers (usually 1.5*IQR), possible outliers.single horizontal boxplot:Draw vertical/horizontal boxplot grouped categorical variable:Adding hue boxplot:","code":"# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 1, figsize = (16,4))\n# Draw horizontal boxplot\nsns.boxplot(x = df.fare_amount, ax = ax);\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 2, figsize = (16,4))\n# Draw vertical boxplot\nsns.boxplot(x = df.payment_type, y = df.fare_amount, ax = ax[0]);\n# Draw horizontal boxplot for each numeric variable in the DataFrame\nsns.boxplot(data = df, orient = \"h\", ax = ax[1]);\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 1, figsize = (16,4))\n# Draw vertical boxplot\nsns.boxplot(x = df.day_of_week, \n            y = df.fare_amount, \n            hue = df.is_weekend, \n            ax = ax);\n            \n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"barplot-with-error-bars","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.9 Barplot with Error Bars","text":"","code":"\ndragons <- data.frame(\n  TalonLength = c(20.9, 58.3, 35.5),\n  SE = c(4.5, 6.3, 5.5),\n  Population = c(\"England\", \"Scotland\", \"Wales\"))"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"base-r-4","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.9.1 base R","text":"Note plotCI requires use gplots package.","code":"\nbarplot(dragons$TalonLength, names = dragons$Population)\ndragons$Population <- factor(dragons$Population, \n                             levels=c(\"Scotland\",\"Wales\",\"England\"))\n\nbarplot(dragons$TalonLength, names = dragons$Population, \n        ylim=c(0,70),xlim=c(0,4),yaxs='i', xaxs='i',\n        main=\"Dragon Talon Length in the UK\",\n        ylab=\"Mean Talon Length\",\n        xlab=\"Country\")\npar(new=T)\nplotCI (dragons$TalonLength, \n        uiw = dragons$SE, liw = dragons$SE,\n        gap=0,sfrac=0.01,pch=\"\",\n        ylim=c(0,70),\n        xlim=c(0.4,3.7),\n        yaxs='i', xaxs='i',axes=F,ylab=\"\",xlab=\"\")"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"ggplot2-5","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.9.2 ggplot2","text":"ggplot2 recommended use drawing bar plots.","code":"\nggplot(dragons,aes(x=Population, y=TalonLength)) +\n  geom_bar(stat=\"identity\", fill=\"lightgray\", col=\"black\") + \n  geom_errorbar(ymin=dragons$TalonLength-dragons$SE,\n                ymax=dragons$TalonLength+dragons$SE) +\n  ylim(0, 70) + xlab(\"Country\") + ylab(\"Mean Talon Length\")"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"python-4","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.9.3 Python","text":"seaborn package also gives optimal approach generate barplots. function name seaborn.boxplot(). Unlike R, barplots generated seaborn error bars default.simply plotting numeric categorical variables:Add hue barplot:","code":"# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 1, figsize = (16,4))\n# Draw barplot for mean of df.fare_amount\nsns.barplot(x = df.payment_type, \n            y = df.fare_amount, \n            estimator = np.mean, \n            ci = 95); \n# Set label\nax.set_label(\"Mean of fare_amount\"); \n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()# Create figure with `matplotlib`\nfig,ax = plt.subplots(1, 1, figsize = (12,6))\n# Add df.payment_type as the hue\nsns.barplot(x = 'day_of_week', \n            y = 'fare_amount', \n            hue = 'payment_type', \n            data = df);\n# Set legend location\nax.legend(loc = 'best')\n\n# Extra step needed in R markdown; not required in Jupyter Notebook\nplt.show()"},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"conclusion","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.9.4 Conclusion","text":"success achieving data visualization Python R sessions, comparing generated plots base R ggplot2 package, hard find , given processed dataset, R Python well-developed, professional, powerful packages visualize explore dataset.","code":""},{"path":"base-r-ggplot2-python-graphing-performances.html","id":"references-2","chapter":"35 Base R, ggplot2, & Python Graphing Performances","heading":"35.9.5 References","text":"R Base Graphics: Idiot’s Guide https://rstudio-pubs-static.s3.amazonaws.com/7953_4e3efd5b9415444ca065b1167862c349.htmlR Base Graphics: Idiot’s Guide https://rstudio-pubs-static.s3.amazonaws.com/7953_4e3efd5b9415444ca065b1167862c349.htmlEODS-F21: Elements Data Science, Fall 2021 https://github.com/bryanrgibson/eods-f21EODS-F21: Elements Data Science, Fall 2021 https://github.com/bryanrgibson/eods-f21R Interface Python https://rstudio.github.io/reticulate/R Interface Python https://rstudio.github.io/reticulate/Numpy Scipy Documentation https://docs.scipy.org/doc/Numpy Scipy Documentation https://docs.scipy.org/doc/pandas documentation https://pandas.pydata.org/docs/pandas documentation https://pandas.pydata.org/docs/Matplotlib: Visualization Python - matplotlib.pyplot https://matplotlib.org/stable/api/pyplot_summary.htmlMatplotlib: Visualization Python - matplotlib.pyplot https://matplotlib.org/stable/api/pyplot_summary.htmlseaborn: statistical data visualization - User guide tutorial https://seaborn.pydata.org/tutorial.htmlseaborn: statistical data visualization - User guide tutorial https://seaborn.pydata.org/tutorial.html","code":""},{"path":"cheat-sheet-to-score-100-points.html","id":"cheat-sheet-to-score-100-points","chapter":"36 Cheat Sheet to Score 100 Points","heading":"36 Cheat Sheet to Score 100 Points","text":"Sharmi Mathur Ayush Baral##Motivation:motivation behind small cheat sheet future well current\nstudents reference guide points get taken \nsilly reasons.###Things Learned:project helped us look mistakes assignments. gave us\nbrilliant feedback errors made past provided us \nopportunity learn mistakes. steps take going\nforward mistakes occur .####Link Projecthttps://ayushbaral.github.io/edav_cc.pdf","code":""},{"path":"base-r-string-operations-cheatsheet.html","id":"base-r-string-operations-cheatsheet","chapter":"37 Base R String Operations Cheatsheet","heading":"37 Base R String Operations Cheatsheet","text":"Zixian YinThis project collects series String Operations supported Base R pdf version cheatsheet. uses Python reference R beginners can quickly find correct functions want use.Access cheatsheet:https://github.com/gbZachYin/STAT5702/blob/main/Base_R_String_Operations_Cheatsheet.pdf","code":""},{"path":"data-manipulation-in-r.html","id":"data-manipulation-in-r","chapter":"38 Data manipulation in R","heading":"38 Data manipulation in R","text":"Junhao Zhang","code":""},{"path":"data-manipulation-in-r.html","id":"overview","chapter":"38 Data manipulation in R","heading":"38.0.1 Overview","text":"cheat sheet particularly made people don’t experience R briefly talks necessary tools packages used programming assignments can help manipulate dataset visualization. Two packages mainly focused : “dplyr” “tidyr”.","code":""},{"path":"data-manipulation-in-r.html","id":"environment-set-up","chapter":"38 Data manipulation in R","heading":"38.0.2 Environment Set Up","text":"using packages, need install computer. , can simply install tidyverse package import use library() function, includes dplyr, tidyr useful packages.","code":""},{"path":"data-manipulation-in-r.html","id":"part-one-data-import","chapter":"38 Data manipulation in R","heading":"38.0.3 Part One: Data Import","text":"part, three commonly used ways importing data showed.\n1.Direct Import R Package\n2.Import Using URL \n3.Import local computer","code":""},{"path":"data-manipulation-in-r.html","id":"import-from-package","chapter":"38 Data manipulation in R","heading":"38.0.3.1 Import from Package","text":"One greatest thing R R dataset package contains variety datasets can use.\nuse datasets, just need import package R.\neg want use fastfood dataset openintro packaage, just need import openintroIf dataset called fastfood, avoid confusion, can specify clearly package name","code":"\nlibrary(openintro)\nfastfood## # A tibble: 515 × 17\n##    restaurant item      calories cal_fat total_fat sat_fat trans_fat cholesterol\n##    <chr>      <chr>        <dbl>   <dbl>     <dbl>   <dbl>     <dbl>       <dbl>\n##  1 Mcdonalds  Artisan …      380      60         7       2       0            95\n##  2 Mcdonalds  Single B…      840     410        45      17       1.5         130\n##  3 Mcdonalds  Double B…     1130     600        67      27       3           220\n##  4 Mcdonalds  Grilled …      750     280        31      10       0.5         155\n##  5 Mcdonalds  Crispy B…      920     410        45      12       0.5         120\n##  6 Mcdonalds  Big Mac        540     250        28      10       1            80\n##  7 Mcdonalds  Cheesebu…      300     100        12       5       0.5          40\n##  8 Mcdonalds  Classic …      510     210        24       4       0            65\n##  9 Mcdonalds  Double C…      430     190        21      11       1            85\n## 10 Mcdonalds  Double Q…      770     400        45      21       2.5         175\n## # … with 505 more rows, and 9 more variables: sodium <dbl>, total_carb <dbl>,\n## #   fiber <dbl>, sugar <dbl>, protein <dbl>, vit_a <dbl>, vit_c <dbl>,\n## #   calcium <dbl>, salad <chr>\nopenintro::fastfood## # A tibble: 515 × 17\n##    restaurant item      calories cal_fat total_fat sat_fat trans_fat cholesterol\n##    <chr>      <chr>        <dbl>   <dbl>     <dbl>   <dbl>     <dbl>       <dbl>\n##  1 Mcdonalds  Artisan …      380      60         7       2       0            95\n##  2 Mcdonalds  Single B…      840     410        45      17       1.5         130\n##  3 Mcdonalds  Double B…     1130     600        67      27       3           220\n##  4 Mcdonalds  Grilled …      750     280        31      10       0.5         155\n##  5 Mcdonalds  Crispy B…      920     410        45      12       0.5         120\n##  6 Mcdonalds  Big Mac        540     250        28      10       1            80\n##  7 Mcdonalds  Cheesebu…      300     100        12       5       0.5          40\n##  8 Mcdonalds  Classic …      510     210        24       4       0            65\n##  9 Mcdonalds  Double C…      430     190        21      11       1            85\n## 10 Mcdonalds  Double Q…      770     400        45      21       2.5         175\n## # … with 505 more rows, and 9 more variables: sodium <dbl>, total_carb <dbl>,\n## #   fiber <dbl>, sugar <dbl>, protein <dbl>, vit_a <dbl>, vit_c <dbl>,\n## #   calcium <dbl>, salad <chr>"},{"path":"data-manipulation-in-r.html","id":"import-using-url","chapter":"38 Data manipulation in R","heading":"38.0.3.2 Import Using URL","text":"can also import dataset direcly online sources. achieve , using read_csv function.\neg import crime dataset government official website.can alsp import dataset direcly github.\n1.Go github repository link CSV file\n2.Click raw option present top right data\n3.Copy URL put read_csv function\neg read dataset github","code":"\nread_csv(\"https://data.ny.gov/api/views/ca8h-8gjq/rows.csv\")## # A tibble: 21,228 × 15\n##    County Agency      Year `Months Reporte… `Index Total` `Violent Total` Murder\n##    <chr>  <chr>      <dbl>            <dbl>         <dbl>           <dbl>  <dbl>\n##  1 Albany Albany Ci…  1990               NA          6635            1052      9\n##  2 Albany Albany Ci…  1991               NA          7569            1201     11\n##  3 Albany Albany Ci…  1992               NA          7791            1150      8\n##  4 Albany Albany Ci…  1993               NA          7802            1238      6\n##  5 Albany Albany Ci…  1994               NA          8648            1380     13\n##  6 Albany Albany Ci…  1995               NA          8329            1227      7\n##  7 Albany Albany Ci…  1996               NA          8130            1132     11\n##  8 Albany Albany Ci…  1997               NA          7354            1035      7\n##  9 Albany Albany Ci…  1998               NA          7320             995      2\n## 10 Albany Albany Ci…  1999               NA          7475             897     12\n## # … with 21,218 more rows, and 8 more variables: Rape <dbl>, Robbery <dbl>,\n## #   Aggravated Assault <dbl>, Property Total <dbl>, Burglary <dbl>,\n## #   Larceny <dbl>, Motor Vehicle Theft <dbl>, Region <chr>\nread_csv(\"https://raw.githubusercontent.com/curran/data/gh-pages/dataSoup/datasets.csv\")## # A tibble: 57 × 12\n##    `Dataset Name`  `Person Adding` `Date Added` `Dataset Link`  `Most Recent Ye…\n##    <chr>           <chr>           <chr>        <chr>           <chr>           \n##  1 2008 Election … EJ              11/7/2012    https://docs.g… 2008            \n##  2 Occupy Oakland… EJ              11/7/2012    https://docs.g… 2012            \n##  3 NYPD Stop-and-… EJ              11/7/2012    http://api.occ… 2011            \n##  4 Presidential S… Kai             11/7/2012    http://millerc… 2012            \n##  5 USDA National … Kai             11/7/2012    http://www.ars… 2012            \n##  6 US Foreign Aid  Kai             11/7/2012    https://explor… 2010            \n##  7 US Potato Stat… Kai             11/7/2012    https://explor… 2007            \n##  8 Livestock and … Kai             11/7/2012    https://explor… 2011            \n##  9 Mine Accidents… Kai             11/7/2012    http://www.msh… 2012            \n## 10 Sloan Digital … Kai             11/7/2012    http://www.sds… 2011            \n## # … with 47 more rows, and 7 more variables: Earliest Year In Data <dbl>,\n## #   Status <chr>, Dataset Type <chr>, Documentation <chr>, Existing Work <chr>,\n## #   Tags <chr>, Active <chr>"},{"path":"data-manipulation-in-r.html","id":"import-from-the-local-computer","chapter":"38 Data manipulation in R","heading":"38.0.3.3 Import from the local computer","text":"file already computer, can simple import finding path read .\neg read state_wl.csv file computer.\n1. Find location state_wl.csv computer.\n2. Use read_csv read : read_csv(file path).","code":""},{"path":"data-manipulation-in-r.html","id":"missing-values","chapter":"38 Data manipulation in R","heading":"38.0.4 Missing Values","text":"talk data transformation packages, want first address missing values problem. reality, data set tidy, especially data set online sources.common issue given data set missing value. Sometimes missing values just meaningless errors, sometimes important. ’s decide whether include missing values . part show detect, recode exclude missing values.","code":""},{"path":"data-manipulation-in-r.html","id":"detecting-missing-values","chapter":"38 Data manipulation in R","heading":"38.0.4.1 Detecting missing values","text":".na() function help test whether missing values exist returning logical vector either TRUE FALSETo identify location number NAs vector, can use () sum()","code":"\nx <- c(1:4, NA, 6:7, NA)\nx## [1]  1  2  3  4 NA  6  7 NA\nis.na(x)## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n# identify the location of NA\nwhich(is.na(x))## [1] 5 8\n# coun the number of NA\nsum(is.na(x))## [1] 2"},{"path":"data-manipulation-in-r.html","id":"recode-missing-values","chapter":"38 Data manipulation in R","heading":"38.0.4.2 Recode missing values","text":"missing values meaningless. missing values caused human error thus can correct recoding missing values.","code":"\n# original x\nx## [1]  1  2  3  4 NA  6  7 NA\n# replace NA in x by other numbers\nx[is.na(x)] <- mean(x, na.rm = TRUE) \nround(x,2)## [1] 1.00 2.00 3.00 4.00 3.83 6.00 7.00 3.83"},{"path":"data-manipulation-in-r.html","id":"exclude-missing-values","chapter":"38 Data manipulation in R","heading":"38.0.4.3 Exclude missing values","text":"sure missing values meaningless, can remove dataset arithmetic calculation involving NA results NA.Another way remove NA dataframe using na.omit().","code":"\ny <- c(1,2,NA,3)\nmean(y) ## [1] NA\nmean(y, na.rm=TRUE) ## [1] 2\ndf <- data.frame(col1 = c(1:3, NA),\n                 col2 = c(\"this\", NA,\"is\", \"text\"), \n                 col3 = c(TRUE, FALSE, TRUE, TRUE), \n                 col4 = c(2.5, 4.2, 3.2, NA),\n                 stringsAsFactors = FALSE)\ndf##   col1 col2  col3 col4\n## 1    1 this  TRUE  2.5\n## 2    2 <NA> FALSE  4.2\n## 3    3   is  TRUE  3.2\n## 4   NA text  TRUE   NA\nna.omit(df)##   col1 col2 col3 col4\n## 1    1 this TRUE  2.5\n## 3    3   is TRUE  3.2"},{"path":"data-manipulation-in-r.html","id":"part-two-dylyr","chapter":"38 Data manipulation in R","heading":"38.0.5 Part Two: dylyr","text":"7 key functions help transform dataset.\n1.filter: subset data frame columns subset satisfy certain conditions.\n2.select: select columns names.\n3.mutate: adds news columns end dataset preserves existing ones.\n4.arrange: change order rows data frame.\n5.summarise/summarize: reduces multiple values single value usually used grouped data.\n6.group_by: group whole dataset one variables\n7.rename: rename column names.using mtcars dataset r illustrate key functions.","code":""},{"path":"data-manipulation-in-r.html","id":"filter-1","chapter":"38 Data manipulation in R","heading":"38.0.5.1 filter()","text":"can use function get columns based values.\neg find cars 6 cylindersWe can also use c() filter columns\neg find information cars number foward gears 3 5We can also put complex conditions\neg find cars 6 cylinders fuel consumption less 20 mpg.","code":"\nfilter(mtcars,cyl==6)##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nfilter(mtcars,gear %in% c(3,5))##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nfilter(mtcars,cyl>6 & mpg<20)##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8"},{"path":"data-manipulation-in-r.html","id":"select-1","chapter":"38 Data manipulation in R","heading":"38.0.5.2 select()","text":"can use select pick certain columns names, especially really long dataframe, select allows select columns need.eg select car information containing information miles per gallon number cylindersWe can select several columns using index “:”\neg select car information displacement weightWe add minus sign front column name select .\neg select information car except Rear axle ration number carburetorseg Don’t select information displacement weight","code":"\nselect(mtcars,mpg,cyl)##                      mpg cyl\n## Mazda RX4           21.0   6\n## Mazda RX4 Wag       21.0   6\n## Datsun 710          22.8   4\n## Hornet 4 Drive      21.4   6\n## Hornet Sportabout   18.7   8\n## Valiant             18.1   6\n## Duster 360          14.3   8\n## Merc 240D           24.4   4\n## Merc 230            22.8   4\n## Merc 280            19.2   6\n## Merc 280C           17.8   6\n## Merc 450SE          16.4   8\n## Merc 450SL          17.3   8\n## Merc 450SLC         15.2   8\n## Cadillac Fleetwood  10.4   8\n## Lincoln Continental 10.4   8\n## Chrysler Imperial   14.7   8\n## Fiat 128            32.4   4\n## Honda Civic         30.4   4\n## Toyota Corolla      33.9   4\n## Toyota Corona       21.5   4\n## Dodge Challenger    15.5   8\n## AMC Javelin         15.2   8\n## Camaro Z28          13.3   8\n## Pontiac Firebird    19.2   8\n## Fiat X1-9           27.3   4\n## Porsche 914-2       26.0   4\n## Lotus Europa        30.4   4\n## Ford Pantera L      15.8   8\n## Ferrari Dino        19.7   6\n## Maserati Bora       15.0   8\n## Volvo 142E          21.4   4\nselect(mtcars,disp:wt)##                      disp  hp drat    wt\n## Mazda RX4           160.0 110 3.90 2.620\n## Mazda RX4 Wag       160.0 110 3.90 2.875\n## Datsun 710          108.0  93 3.85 2.320\n## Hornet 4 Drive      258.0 110 3.08 3.215\n## Hornet Sportabout   360.0 175 3.15 3.440\n## Valiant             225.0 105 2.76 3.460\n## Duster 360          360.0 245 3.21 3.570\n## Merc 240D           146.7  62 3.69 3.190\n## Merc 230            140.8  95 3.92 3.150\n## Merc 280            167.6 123 3.92 3.440\n## Merc 280C           167.6 123 3.92 3.440\n## Merc 450SE          275.8 180 3.07 4.070\n## Merc 450SL          275.8 180 3.07 3.730\n## Merc 450SLC         275.8 180 3.07 3.780\n## Cadillac Fleetwood  472.0 205 2.93 5.250\n## Lincoln Continental 460.0 215 3.00 5.424\n## Chrysler Imperial   440.0 230 3.23 5.345\n## Fiat 128             78.7  66 4.08 2.200\n## Honda Civic          75.7  52 4.93 1.615\n## Toyota Corolla       71.1  65 4.22 1.835\n## Toyota Corona       120.1  97 3.70 2.465\n## Dodge Challenger    318.0 150 2.76 3.520\n## AMC Javelin         304.0 150 3.15 3.435\n## Camaro Z28          350.0 245 3.73 3.840\n## Pontiac Firebird    400.0 175 3.08 3.845\n## Fiat X1-9            79.0  66 4.08 1.935\n## Porsche 914-2       120.3  91 4.43 2.140\n## Lotus Europa         95.1 113 3.77 1.513\n## Ford Pantera L      351.0 264 4.22 3.170\n## Ferrari Dino        145.0 175 3.62 2.770\n## Maserati Bora       301.0 335 3.54 3.570\n## Volvo 142E          121.0 109 4.11 2.780\nselect(mtcars,-drat,-carb)##                      mpg cyl  disp  hp    wt  qsec vs am gear\n## Mazda RX4           21.0   6 160.0 110 2.620 16.46  0  1    4\n## Mazda RX4 Wag       21.0   6 160.0 110 2.875 17.02  0  1    4\n## Datsun 710          22.8   4 108.0  93 2.320 18.61  1  1    4\n## Hornet 4 Drive      21.4   6 258.0 110 3.215 19.44  1  0    3\n## Hornet Sportabout   18.7   8 360.0 175 3.440 17.02  0  0    3\n## Valiant             18.1   6 225.0 105 3.460 20.22  1  0    3\n## Duster 360          14.3   8 360.0 245 3.570 15.84  0  0    3\n## Merc 240D           24.4   4 146.7  62 3.190 20.00  1  0    4\n## Merc 230            22.8   4 140.8  95 3.150 22.90  1  0    4\n## Merc 280            19.2   6 167.6 123 3.440 18.30  1  0    4\n## Merc 280C           17.8   6 167.6 123 3.440 18.90  1  0    4\n## Merc 450SE          16.4   8 275.8 180 4.070 17.40  0  0    3\n## Merc 450SL          17.3   8 275.8 180 3.730 17.60  0  0    3\n## Merc 450SLC         15.2   8 275.8 180 3.780 18.00  0  0    3\n## Cadillac Fleetwood  10.4   8 472.0 205 5.250 17.98  0  0    3\n## Lincoln Continental 10.4   8 460.0 215 5.424 17.82  0  0    3\n## Chrysler Imperial   14.7   8 440.0 230 5.345 17.42  0  0    3\n## Fiat 128            32.4   4  78.7  66 2.200 19.47  1  1    4\n## Honda Civic         30.4   4  75.7  52 1.615 18.52  1  1    4\n## Toyota Corolla      33.9   4  71.1  65 1.835 19.90  1  1    4\n## Toyota Corona       21.5   4 120.1  97 2.465 20.01  1  0    3\n## Dodge Challenger    15.5   8 318.0 150 3.520 16.87  0  0    3\n## AMC Javelin         15.2   8 304.0 150 3.435 17.30  0  0    3\n## Camaro Z28          13.3   8 350.0 245 3.840 15.41  0  0    3\n## Pontiac Firebird    19.2   8 400.0 175 3.845 17.05  0  0    3\n## Fiat X1-9           27.3   4  79.0  66 1.935 18.90  1  1    4\n## Porsche 914-2       26.0   4 120.3  91 2.140 16.70  0  1    5\n## Lotus Europa        30.4   4  95.1 113 1.513 16.90  1  1    5\n## Ford Pantera L      15.8   8 351.0 264 3.170 14.50  0  1    5\n## Ferrari Dino        19.7   6 145.0 175 2.770 15.50  0  1    5\n## Maserati Bora       15.0   8 301.0 335 3.570 14.60  0  1    5\n## Volvo 142E          21.4   4 121.0 109 2.780 18.60  1  1    4\nselect(mtcars,-(disp:wt))##                      mpg cyl  qsec vs am gear carb\n## Mazda RX4           21.0   6 16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 17.02  0  1    4    4\n## Datsun 710          22.8   4 18.61  1  1    4    1\n## Hornet 4 Drive      21.4   6 19.44  1  0    3    1\n## Hornet Sportabout   18.7   8 17.02  0  0    3    2\n## Valiant             18.1   6 20.22  1  0    3    1\n## Duster 360          14.3   8 15.84  0  0    3    4\n## Merc 240D           24.4   4 20.00  1  0    4    2\n## Merc 230            22.8   4 22.90  1  0    4    2\n## Merc 280            19.2   6 18.30  1  0    4    4\n## Merc 280C           17.8   6 18.90  1  0    4    4\n## Merc 450SE          16.4   8 17.40  0  0    3    3\n## Merc 450SL          17.3   8 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 17.42  0  0    3    4\n## Fiat 128            32.4   4 19.47  1  1    4    1\n## Honda Civic         30.4   4 18.52  1  1    4    2\n## Toyota Corolla      33.9   4 19.90  1  1    4    1\n## Toyota Corona       21.5   4 20.01  1  0    3    1\n## Dodge Challenger    15.5   8 16.87  0  0    3    2\n## AMC Javelin         15.2   8 17.30  0  0    3    2\n## Camaro Z28          13.3   8 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 17.05  0  0    3    2\n## Fiat X1-9           27.3   4 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 16.70  0  1    5    2\n## Lotus Europa        30.4   4 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 14.50  0  1    5    4\n## Ferrari Dino        19.7   6 15.50  0  1    5    6\n## Maserati Bora       15.0   8 14.60  0  1    5    8\n## Volvo 142E          21.4   4 18.60  1  1    4    2"},{"path":"data-manipulation-in-r.html","id":"mutate-1","chapter":"38 Data manipulation in R","heading":"38.0.5.3 mutate()","text":"can use function add new variables adding end dataset preserves existing columns.eg can calculate displacement per cylinder car mpg greater 20 using mutate","code":"\nmtcars %>%\n  filter(mpg>20) %>%\n  mutate(capacity=disp/cyl)##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb capacity\n## Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 26.66667\n## Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 26.66667\n## Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 27.00000\n## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 43.00000\n## Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 36.67500\n## Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2 35.20000\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 19.67500\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 18.92500\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 17.77500\n## Toyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1 30.02500\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 19.75000\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 30.07500\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 23.77500\n## Volvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2 30.25000"},{"path":"data-manipulation-in-r.html","id":"arrange-1","chapter":"38 Data manipulation in R","heading":"38.0.5.4 arrange()","text":"can use function sort rows value columns. default order ascending. Use desc() sort descending order.eg sort cars fuel consumption (mpg) acending decending orderWe can also sort dataframe using one column values\neg sort cars cylinder number descending order cars culinder number sort weight ascending order","code":"\narrange(mtcars,mpg)##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\narrange(mtcars,desc(mpg))##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\narrange(mtcars,desc(cyl),wt)##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2"},{"path":"data-manipulation-in-r.html","id":"summarisesummarize","chapter":"38 Data manipulation in R","heading":"38.0.5.5 summarise()/summarize()","text":"","code":""},{"path":"data-manipulation-in-r.html","id":"group_by","chapter":"38 Data manipulation in R","heading":"38.0.5.6 group_by()","text":"cases, summarise() group_by() functions used together transforming data. Summarise() creates new dataframe contains one column grouping variable one column summary statistics.eg summarize mean count statistics fuel consumption cars without groupingeg first group number cylinders summarize mean count statistics fuel consumption carswe can also group one variables\neg gourp number cylinders weight summarize mean count statistics fuel consumption cars.want perform operations ungrouped data, use ungroup()","code":"\nsummarise(mtcars,mean=mean(mpg),count=n())##       mean count\n## 1 20.09062    32\nmtcars %>%\n  group_by(cyl) %>%\n  summarise(mean=mean(mpg),count=n())## # A tibble: 3 × 3\n##     cyl  mean count\n##   <dbl> <dbl> <int>\n## 1     4  26.7    11\n## 2     6  19.7     7\n## 3     8  15.1    14\nmtcars %>%\n  group_by(cyl,wt) %>%\n  summarise(mean=mean(mpg),count=n())## # A tibble: 30 × 4\n## # Groups:   cyl [3]\n##      cyl    wt  mean count\n##    <dbl> <dbl> <dbl> <int>\n##  1     4  1.51  30.4     1\n##  2     4  1.62  30.4     1\n##  3     4  1.84  33.9     1\n##  4     4  1.94  27.3     1\n##  5     4  2.14  26       1\n##  6     4  2.2   32.4     1\n##  7     4  2.32  22.8     1\n##  8     4  2.46  21.5     1\n##  9     4  2.78  21.4     1\n## 10     4  3.15  22.8     1\n## # … with 20 more rows"},{"path":"data-manipulation-in-r.html","id":"rename-1","chapter":"38 Data manipulation in R","heading":"38.0.5.7 rename()","text":"can use function change column names.\neg change qsec acceleration","code":"\nrename(mtcars,acceleration=qsec)##                      mpg cyl  disp  hp drat    wt acceleration vs am gear carb\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620        16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875        17.02  0  1    4    4\n## Datsun 710          22.8   4 108.0  93 3.85 2.320        18.61  1  1    4    1\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215        19.44  1  0    3    1\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440        17.02  0  0    3    2\n## Valiant             18.1   6 225.0 105 2.76 3.460        20.22  1  0    3    1\n## Duster 360          14.3   8 360.0 245 3.21 3.570        15.84  0  0    3    4\n## Merc 240D           24.4   4 146.7  62 3.69 3.190        20.00  1  0    4    2\n## Merc 230            22.8   4 140.8  95 3.92 3.150        22.90  1  0    4    2\n## Merc 280            19.2   6 167.6 123 3.92 3.440        18.30  1  0    4    4\n## Merc 280C           17.8   6 167.6 123 3.92 3.440        18.90  1  0    4    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070        17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730        17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780        18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250        17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424        17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345        17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200        19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615        18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835        19.90  1  1    4    1\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465        20.01  1  0    3    1\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520        16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435        17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840        15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845        17.05  0  0    3    2\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935        18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140        16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513        16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170        14.50  0  1    5    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770        15.50  0  1    5    6\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570        14.60  0  1    5    8\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780        18.60  1  1    4    2"},{"path":"data-manipulation-in-r.html","id":"part-three-tidyr","chapter":"38 Data manipulation in R","heading":"38.0.6 Part Three: tidyr","text":"2 key functions help transform dataset.\n1.pivot_longer: Increase number rows decrease number columns\n2.pivot_wider: opposite pivot_longer.","code":""},{"path":"data-manipulation-in-r.html","id":"pivot_longer-1","chapter":"38 Data manipulation in R","heading":"38.0.6.1 pivot_longer()","text":"One commom problem dataset column names variable names, values variable. deal case, use pivot_longer transform dataset.\neg table4a, column “1999” “2000” variables values variable \"year, use pivot_longer change .","code":"\ntable4a## # A tibble: 3 × 3\n##   country     `1999` `2000`\n## * <chr>        <int>  <int>\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\npivot_longer(table4a,c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")## # A tibble: 6 × 3\n##   country     year   cases\n##   <chr>       <chr>  <int>\n## 1 Afghanistan 1999     745\n## 2 Afghanistan 2000    2666\n## 3 Brazil      1999   37737\n## 4 Brazil      2000   80488\n## 5 China       1999  212258\n## 6 China       2000  213766"},{"path":"data-manipulation-in-r.html","id":"pivot_wider-1","chapter":"38 Data manipulation in R","heading":"38.0.6.2 Pivot_wider()","text":"function opposite pivot_longer. Instead increasing number rows, decreases number rows increases number columns. normally use observation scattered across multiple rows.\neg table2, observation county year, observation across two row,want transform county year one row.","code":"\ntable2## # A tibble: 12 × 4\n##    country      year type            count\n##    <chr>       <int> <chr>           <int>\n##  1 Afghanistan  1999 cases             745\n##  2 Afghanistan  1999 population   19987071\n##  3 Afghanistan  2000 cases            2666\n##  4 Afghanistan  2000 population   20595360\n##  5 Brazil       1999 cases           37737\n##  6 Brazil       1999 population  172006362\n##  7 Brazil       2000 cases           80488\n##  8 Brazil       2000 population  174504898\n##  9 China        1999 cases          212258\n## 10 China        1999 population 1272915272\n## 11 China        2000 cases          213766\n## 12 China        2000 population 1280428583\npivot_wider(table2,names_from = type,values_from = count)## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <int>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583"},{"path":"data-manipulation-in-r.html","id":"note","chapter":"38 Data manipulation in R","heading":"38.0.6.3 Note:","text":"pivot_longer makes dataframe narrower longer, pivot_wider makes dataframe wider shorter, actually complements.","code":""},{"path":"data-manipulation-in-r.html","id":"part-four-example","chapter":"38 Data manipulation in R","heading":"38.0.7 Part Four: Example","text":"part, use example show functions really work transforming dataset.\ndataset using seattlepets openintro package.\ngoal find dog names cat names decide names popular dogs cats.","code":"\ndogcat <- seattlepets %>%\n  filter(species %in% c(\"Dog\",\"Cat\")) %>%\n  count(animal_name,species,name='total') %>%\n  pivot_wider(id_cols=animal_name,names_from = species,values_from = total) %>%\n  rowwise() %>%\n  mutate(total=sum(Cat,Dog,na.rm = TRUE),proportion=Dog/total,ratio=Dog/Cat) %>%\n  ungroup() %>%\n  arrange(desc(total))\ndogcat## # A tibble: 13,920 × 6\n##    animal_name   Cat   Dog total proportion ratio\n##    <chr>       <int> <int> <int>      <dbl> <dbl>\n##  1 <NA>          406    76   482      0.158 0.187\n##  2 Lucy          102   337   439      0.768 3.30 \n##  3 Charlie        81   306   387      0.791 3.78 \n##  4 Luna          111   244   355      0.687 2.20 \n##  5 Bella          82   249   331      0.752 3.04 \n##  6 Max            83   186   269      0.691 2.24 \n##  7 Daisy          40   221   261      0.847 5.52 \n##  8 Molly          54   186   240      0.775 3.44 \n##  9 Jack           65   167   232      0.720 2.57 \n## 10 Lily           86   146   232      0.629 1.70 \n## # … with 13,910 more rows"},{"path":"data-manipulation-in-r.html","id":"explanation","chapter":"38 Data manipulation in R","heading":"38.0.7.1 Explanation:","text":"use filter function find animal names whose species dog cat.find total number animal name using count function.use pivot_wider expand dataframe get count name dog cat.add new columns total,proportion raio using mutate().reorder datafram total descending order using arrange().notice NAs dog cat columns. can use na.rm avoid .","code":""},{"path":"data-manipulation-in-r.html","id":"summary","chapter":"38 Data manipulation in R","heading":"38.0.8 Summary","text":"motivation project create guide future 5702 students don’t much experience R. personally don’t much experience R taking class, problem sets, kind struggled little bit. want create cheat sheet can help future students get familiar R. project, mainly talk three parts. first part importing data. discussed three common ways people use import data: package, online website, github. second part dplyr package. one useful packages R. contains variety functions help transform data. illustrate 7 key functions package giving simple examples. ’m giving complicated examples cheat sheet students don’t much experience R. last part another package “tidyr”, also one important packages R. allows create tidy data. Two functions mainly talked part. Finally, give example step step using previous functions show use analyzing real problem. Apparently, cheat sheet R beginners stuff covers really simple. talk packages functions used . course, useful functions packages covered cheat sheet. people want know information, please refer following links preferences. next time, add details advanced uses previous stuff.","code":""},{"path":"data-manipulation-in-r.html","id":"references-3","chapter":"38 Data manipulation in R","heading":"38.0.9 References","text":"https://r4ds..co.nz/transform.htmlhttps://dplyr.tidyverse.org/https://r4ds..co.nz/tidy-data.html#tidy-data-1https://tidyr.tidyverse.org/","code":""},{"path":"ggplot_matplotlib_cheatsheet.html","id":"ggplot_matplotlib_cheatsheet","chapter":"39 ggplot_matplotlib_cheatsheet","heading":"39 ggplot_matplotlib_cheatsheet","text":"Binghong YuFor starters, struggle learn new language first. However, happen experience programming languages python R, guide comparing two languages similar functions side side, much easier become familiar new language. Thus, create cheat sheet visualization functions transformation used visualization packages R (.e. ggplot) python (.e. Matplotlib).spent time searching documentation similar plot functions languages. process, become familiar plot functions functions adjusting scales. meantime, learning examples, also learned differences results plotted different languages different functions, difference adjusting parameters.Due space restriction, add much details use functions. Next time, might add details differences parameters adjust inside parentheses.Click following link check cheatsheet","code":"\n\n<!--chapter:end:ggplot_matplotlib_cheatsheet.Rmd-->\n\n# Scatterplots cheatsheet\n\nShangzhi Liu\n\nCheck the link to see the scatterplots cheatsheet pdf file:\n\nhttps://github.com/ShangzhiLiu2021/cc21fall2/blob/scatterplots_cheatsheet/resources/scatterplots_cheatsheet/scatterplots.pdf\n\n\n\n\n\n```r\n# package used\nlibrary(ggplot2)\nlibrary(scatterplot3d)\nlibrary(car) #use data mtcars in package car\n\n# Sample scatterplot 1:\nset.seed(1234)\nx <- rnorm(100,mean = 2, sd = 3)\ny <- -1.5 + 2*x + rnorm(100)\ndf <- data.frame(x = x, y = y)\nggplot(data = df, aes(x = x, y = y)) + geom_point()\n# Sample scatterplot 2:\nggplot(data = df, aes(x = x, y = y)) + geom_point(size = 3, alpha = 0.3, shape = 15, color = \"blue\", stroke = 0.3)\n# 3D scatterplot:\nscatterplot3d(mtcars$wt, mtcars$disp, mtcars$mpg, main=\"3D Scatterplot\")\n# 3D scatterplot example:\nscatterplot3d(mtcars$wt, mtcars$disp, mtcars$mpg, highlight.3d=TRUE, type=\"h\", main=\"3D Scatterplot\")\n# Density contour lines, Hexagonal and Square heatmap \nggplot(data = df, aes(x = x, y = y)) + geom_point() + geom_density2d(color = \"red\")\nggplot(data = df, aes(x = x, y = y)) + geom_point() + geom_hex(color = \"red\", bins = 30)\nggplot(data = df, aes(x = x, y = y)) + geom_point() + geom_bin_2d(color = \"red\", bins = 30)\n# text\nggplot(mtcars, aes(wt, mpg)) + geom_point(color = 'red') + geom_text(aes(wt, mpg, label = rownames(mtcars)))\nggplot(mtcars, aes(wt, mpg)) + geom_point(color = 'red') + annotate(\"text\", x = 3.840, y = 13.3, label = \"Camaro Z28\") + annotate(\"text\", x = 1.935, y = 27.3, label = \"Fiat X1-9\")\n#matrix\npairs(~mpg+disp+hp+drat+wt,data=mtcars)"},{"path":"introduction-to-log-transformation.html","id":"introduction-to-log-transformation","chapter":"40 Introduction to Log-transformation","heading":"40 Introduction to Log-transformation","text":"Shanzhao Qiao, Yijia JinThe community contribution team create cheatsheet topic log transformation.\nwork create comprehensive guidance regarding log-transformation, theoretical analysis, examples manipulated dataset plots visualizations, real-world examples, thoughts situations can applied .PDF version guidance explanation found :\n‘resources/intro_to_log_transformation/intro_log_transf_final.pdf’following github repo:\nhttps://github.com/jyjuni/intro_to_log_transf/blob/master/intro_log_transf_final.pdf","code":""},{"path":"how-to-integrate-r-with-postgresql.html","id":"how-to-integrate-r-with-postgresql","chapter":"41 How to Integrate R with PostgreSQL","heading":"41 How to Integrate R with PostgreSQL","text":"Julia Wang","code":""},{"path":"how-to-integrate-r-with-postgresql.html","id":"motivation-1","chapter":"41 How to Integrate R with PostgreSQL","heading":"41.1 Motivation","text":"guide teach connect RStudio local instance PostgreSQL, popular open source object-relational database server management system, free download. become familiar connecting PostgreSQL instance, perform quick Linear Regression sample data, store data PostgreSQL database, reuse LR model make prediction.Note: instance database used interchangeably guide, mean thing.","code":""},{"path":"how-to-integrate-r-with-postgresql.html","id":"how-to-integrate-r-with-postgresql-1","chapter":"41 How to Integrate R with PostgreSQL","heading":"41.2 How to Integrate R with PostgreSQL","text":"haven’t already, [download PostgreSQL]((https://www.postgresql.org/download/){target=\"_blank\"} setup pgAdmin 4. note user password create process, well make note port host used set PostgreSQL (defaults localhost 5432). need information later establish connection RStudio PostgreSQL instance.set PostgreSQL able connect pgAdmin 4, now RStudio, want install use relevant RPostgreSQL packages. can running following commands.","code":"\ninstall.packages(\"RPostgres\")\ninstall.packages(\"devtools\")\ninstall.packages(\"DBI\")\ninstall.packages(\"remotes\")"},{"path":"how-to-integrate-r-with-postgresql.html","id":"create-your-connection","chapter":"41 How to Integrate R with PostgreSQL","heading":"41.2.1 Create your connection","text":"case want use separate dummy/admin user, can create user going Servers > PostgreSQL > Databases >  > Login > right click Create > Login/Group Role. name user name set password Definition whatever want. priviledge login bare minimum. following R block, going setup connection PostgreSQL instance. One question might reading following block drv (driver) . driver context tool allows applications access data database systems, similar printer drivers allow talk printer print things. pass driver relevant information database database user create connection, able use run R commands, dbListTables, list tables database.","code":"\ndb <- '<your db name here>'  # provide the name of your db\n\nhost_db <- '<your db server here>' # i.e. 'ec2-54-83-201-96.compute-1.amazonaws.com or localhost if you are connecting to an instance on your own laptop'  \ndb_port <- '5432'  # or any other port specified by the DBA\n\ndb_user <- '<your db user here>' # user that has access to your SQL instance\n\ndb_password <- '<your db user pwd here>' # password of the user who has access to your SQL instance.\n\ndrv <- RPostgres::Postgres()\n\ncon <- dbConnect(drv, dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)  \n\ndbListTables(con) # returns a list of tables in your database\ndbExistsTable(con, \"<table name>\") # checks if the table exists in your database"},{"path":"how-to-integrate-r-with-postgresql.html","id":"basic-sql-commands","chapter":"41 How to Integrate R with PostgreSQL","heading":"41.2.2 Basic SQL commands","text":"addition two commands showed , basic commands built RPostgres library, allow read write database. following block show write dataframe table instance, well query afterwards.don’t notice table immediately, make sure right click tables dropdown sidebar hit refresh.Read table:case prefer writing raw sql, can use dbGetQuery method. also dbSendQuery method bit involved, purposes, dbGetQuery sufficient.dbSendQuery method submits synchronously executes SQL query database engine. extract records, need use dbFetch method, must call dbClearResult finish fetching records need. going run SQL queries RStudio, likely using dbGetQuery.Now able connect talk PostgreSQL instance, can now run analyses using data stored well store results . can also pull saved model make predictions well, can save us time retraining models.Make sure clean connections finished working database.","code":"\n# Create a table\nlibrary(tidyverse)\n\ncars <- mtcars %>% rownames_to_column(\"carname\") \n  \ndbWriteTable(con, 'cars', cars) # add the mtcars data set to your database as a table called \"cars\"\n\ncars <- cars %>% mutate(id = row_number()) # if you want to change your data, such as adding an id column\n\ndbWriteTable(con, 'cars', cars, overwrite=TRUE, append=FALSE) # you will need to pass in additional parameters. Overwrite will drop and recreate the table with your new data. Append just appends your data, your df needs to be in the same shape with the same variable names in order to work.\ndbReadTable(con, \"cars\") # read your newly created table\n\nresult <- dbReadTable(con, \"cars\") # can also be stored as variable\nlibrary(ggplot2)\n\ndbGetQuery(con, 'ALTER TABLE cars ADD CONSTRAINT cars_pk PRIMARY KEY (id)') # add primary key to the id column\n\ndb_cars <- dbGetQuery(con, 'SELECT * FROM cars WHERE id <= 20') # filter down our data set \n\nggplot(db_cars, aes(x=disp, y=mpg)) + \n  geom_point() +\n  ggtitle(\"Miles Per Gallon vs. Displacement (cu.in.)\") +\n  ylab(\"Miles Per Gallon\") +\n  xlab(\"Displacement (cu.in.)\")\nmodel <- lm(mpg ~ disp, db_cars)\n\nserialized_model <- rawToChar(serialize(model, NULL, ascii=TRUE)) # serialize (convert string) model so it can be stored in database\n\n# create table for model to be stored in\ndbGetQuery(con, 'CREATE TABLE models (\n    id SERIAL PRIMARY KEY,\n    model TEXT NOT NULL\n);') \n\n# insert model into models table\ninsert_query <-'INSERT INTO models (model) VALUES ($1)'\nrs <- dbSendQuery(con, insert_query, list(serialized_model))\ndbClearResult(rs)\n\n# read the model from postgreSQL\nresult <- dbGetQuery(con, \"SELECT model FROM models WHERE id = 1\")\n\n# revert serialization, and we have a working model again!\ndb_model <- unserialize(charToRaw(as.character(result[,c('model')])))\nsummary(db_model)\n\n# use the model to make a prediction\ndisp <- 128\nX_test <- data.frame(disp)\npredict(db_model, X_test)\n# disconnect once you are finished\ndbDisconnect(con)\ndbUnloadDriver(drv)"},{"path":"how-to-integrate-r-with-postgresql.html","id":"sources","chapter":"41 How to Integrate R with PostgreSQL","heading":"41.3 Sources","text":"https://www.datacareer.de/blog/connect--postgresql--r--step--step-example/https://stackoverflow.com/questions/1395115/storing-r-objects---relational-database","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"tutorial-on-cluster-analysis","chapter":"42 Tutorial on Cluster Analysis","heading":"42 Tutorial on Cluster Analysis","text":"Jannik Wiedenhaupt","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"what-is-clustering-analysis","chapter":"42 Tutorial on Cluster Analysis","heading":"42.1 What is Clustering Analysis?","text":"Clustering Analysis data exploration method one popular classification techniques. Clustering works segregating data points different groups based similarity dissimiliarity attributes. means data clustered homogeneity inside clusters maximized heterogeneity clusters maximized. concept novel human understanding, clustering grouping elements based likeness important.Likewise data science machine learning, clustering algorithms carry task labeling unlabelled data inputs helps data interpretation establishing patterns predictive purposes.understand idea clustering, let’s look following picures points customer rated personal importance price quality.\nCan identify groups data points graph?\n\ncluster data like ?\n\nlike ?\nvisual representation (also two-dimensional), can already clearly decide cluster data points. cluster data points properly, need clustering algorithms.","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"what-types-of-clustering-analysis-exist","chapter":"42 Tutorial on Cluster Analysis","heading":"42.2 What Types of Clustering Analysis Exist?","text":"many different types clustering algorithms particularly useful different situations.four common types :","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"centroid-based-algorithms","chapter":"42 Tutorial on Cluster Analysis","heading":"42.2.1 Centroid-based Algorithms","text":"Centroid-based algorithm separate data points based multiple -called centroids data. data point assigned cluster based squared distance centroid. commonly used type clustering.","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"hierarchical-algorithms","chapter":"42 Tutorial on Cluster Analysis","heading":"42.2.2 Hierarchical Algorithms","text":"Hierarchical algorithms differ centroid-based algorithms constract hierarchy among data points. hierarchy, one can choose different sized clusters based granularity required task hand. normally used hierarchical data structures like company databases taxonomy animal species.\ntwo main types hierarchical algorithms:Agglomerative clustering - observations considered invdividually merged everbigger clustersDivisive cluster - observations considered together split int eversmaller clusters","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"distribution-based-algorithms","chapter":"42 Tutorial on Cluster Analysis","heading":"42.2.3 Distribution-based Algorithms","text":"Distribution-based clustering assumes data composed distributions. Therefore, data points considered parts cluster based probability belong given cluster. distance center cluster increases, probability data point belongs cluster decreases. algorithm recommended know distribution data.","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"density-based-algorithms","chapter":"42 Tutorial on Cluster Analysis","heading":"42.2.4 Density-based Algorithms","text":"Density-based clustering works detecting regions factors focused ’re separated via means regions might empty sparse. Points part cluster categorized noise. Outliers assigned clusters therefore ignored algorithms.","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"how-does-cluster-analysis-work-on-paper","chapter":"42 Tutorial on Cluster Analysis","heading":"42.3 How Does Cluster Analysis Work on Paper?","text":"following process followed approaching cluster analysis.Variable selection: Select variables, called bases, used cluster observations. want make decisions based classification, example targeting different groups customers, likely also want additional variables, called descriptors, help understand found clusters.Similarity/Dissimilarity calculation: Choose suitable measures proximity different observations. Based type bases, need choose distance function similarity function. variables compared individually first. , summed calculate total similarity/distance two observations. Comparing observations yields proximity distance matrix.Cluster creation: Choose suitable clustering method ones mentioned needed also objective functions decide clusters merged split .Additional steps (always required):\n1. Determine number clusters. can either done based thorough understanding problem’s domain, planned interpretation, statistical procedure. example required centroid-based algorithms.\n2. Interpretation clusters.\n3. Test strength clustering results. Test internal homogeneity external homogeneity clusters.","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"how-does-cluster-analysis-work-in-r","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4 How Does Cluster Analysis Work in R?","text":"","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"data-preparation","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.1 Data Preparation","text":"First, load dataset. tutorial, use states dataset cluster US states.Second, also use factoextra package particularly eclust function simplify analysis visualization.Third, check data following form:Rows observations columns variablesMissing values removed estimatedData must standardizedAvoid double-weighting underlying constructs avoiding multicollinearity","code":"\ndf <- datasets::state.x77%>%data.frame()\nhead(df, 3)##         Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska         365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n# Delete NA values\ndf <- na.omit(df)\n\n# Save non-scaled version for later\ndf_original <- df\n\n# Standardize variables\ndf <- df %>% mutate_all(~(scale(.) %>% as.vector))\n\ncor_matrix <- cor(df)\ncorrplot(cor_matrix, method = \"number\", type = \"lower\", tl.pos = 'd')\n# Because murder and life expectancy are strongly correlated, we remove murder\ndf <- subset(df, select = -c(Murder))"},{"path":"tutorial-on-cluster-analysis.html","id":"centroid-based-algorithms-1","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.2 Centroid-based Algorithms","text":"classic centroid-based algorithm called “k-means” used . K-means takes data points input groups k clusters following process.Select inputsSelect k cluster centersAssign cases closest centerUpdate cluster centersReassign casesRepeat steps 4 5 convergenceGoing process R simple requires one function.\nparameters followingFUNcluster: Clustering function. , k-means.hc_metric: Metric used calculating dissimilarities observations. , euclidean distance.k: Number clusters. 5 guessed lack exploration dataset.","code":"\nres.km <- eclust(df, FUNcluster = \"kmeans\", k = 5, hc_metric = \"euclidean\")"},{"path":"tutorial-on-cluster-analysis.html","id":"choosing-the-number-of-clusters","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.2.1 Choosing the Number of Clusters","text":"Alternatively setting number clusters k , can also resort different statistics:1. Gap Statistic2. Silhouette Plot3. Elbow Method\nelbow method visual method, determine cluster based spotting elbow graph.weak (pronounced) elbows 2 6.4. Indices\nUse package NbClust experiment different clustering methods, distances, indices.","code":"\nres.km <- eclust(df, FUNcluster = \"kmeans\", hc_metric = \"euclidean\", graph = FALSE)\nfviz_gap_stat(res.km$gap_stat)\nfviz_silhouette(res.km)##   cluster size ave.sil.width\n## 1       1   15          0.08\n## 2       2   23          0.36\n## 3       3   11          0.52\n## 4       4    1          0.00\nfviz_nbclust(df, FUNcluster = kmeans, method = \"wss\") + labs(subtitle = \"Elbow method\") \ncat(\"C-Index:\\n\", NbClust(data=df, method = \"kmeans\", distance = \"euclidean\", index=\"cindex\")$Best.nc)## C-Index:\n##  3 0.2594\ncat(\"Dunn-Index:\\n\", NbClust(data=df, method = \"kmeans\", distance = \"euclidean\", index=\"dunn\")$Best.nc)## Dunn-Index:\n##  15 0.3403\ncat(\"McClain-Index:\\n\", NbClust(data=df, method = \"kmeans\", distance = \"euclidean\", index=\"mcclain\")$Best.nc)## McClain-Index:\n##  2 0.3932"},{"path":"tutorial-on-cluster-analysis.html","id":"hierarchial-algorithms","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.3 Hierarchial Algorithms","text":"two fundamental methods hierarchical clustering - agglomerative divisive clustering. explain .\nhierarchical clustering need define calculate number clusters running algorithm. Moreover, hierarchical clustering results comprehensible tree-like structure called Dendrogram allows us find number clusters interpretable.","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"divisive-hierarchical-clustering","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.3.1 Divisive Hierarchical Clustering","text":"objects points dataset belong one single clusterPartition single cluster two least similar clustersRepeat step 2 observation single clusterThe parameters followingFUNcluster: “hclust” divisive clustering.hc_metric: “euclidean” euclidean distance., see discrepancy k-means clustering. gap-statistic yielded 4 optimal clusters, hierarchical clustering identifies 2 major cluster.","code":"\nres.hclust <- eclust(df, FUNcluster = \"hclust\", hc_metric = \"euclidean\")\nfviz_dend(res.hclust, rect = TRUE)\nfviz_cluster(res.hclust, labelsize = 10)"},{"path":"tutorial-on-cluster-analysis.html","id":"agglomerative-hierarchical-clustering","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.3.2 Agglomerative Hierarchical Clustering","text":"observation single clusterEvery two observations closest according distance measure, clusteredRepeat step 2 observations one clusterIt important notice agglomerative clustering requires agglomeration method specified. different agglomeration methods can read : https://en.wikipedia.org/wiki/Hierarchical_clustering#Linkage_criteria.\nchoose commonly used ward.D2 measure minimized total within-cluster variance.parameters followingFUNcluster: “agnes” agglomerative nesting.hc_method: Agglomeration method. , ward.D2.hc_metric: “euclidean” euclidean distance.possible see differences agglomerative diviseve clustering, two methods come result example.","code":"\nres.aclust <- eclust(df, FUNcluster = \"hclust\", hc_metric = \"euclidean\", hc_method = \"ward.D2\")\nfviz_dend(res.aclust, rect = TRUE)\nfviz_cluster(res.aclust, labelsize = 10)"},{"path":"tutorial-on-cluster-analysis.html","id":"distribution-based-algorithms-1","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.4 Distribution-based Algorithms","text":"explanation good R-tutorial distribution-based algorithms, please visit (Note: Distribution-based algorithms called model-based algorithms ): https://www.datanovia.com/en/lessons/model-based-clustering-essentials/","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"density-based-algorithms-1","chapter":"42 Tutorial on Cluster Analysis","heading":"42.4.5 Density-based Algorithms","text":"explanation good R-tutorial density-based algorithms, please visit: https://www.datanovia.com/en/lessons/dbscan-density-based-clustering-essentials/","code":""},{"path":"tutorial-on-cluster-analysis.html","id":"using-clustering-for-further-analysis","chapter":"42 Tutorial on Cluster Analysis","heading":"42.5 Using Clustering for Further Analysis","text":"clustering observations, want understand clusters mean. , visualize average strenght variable cluster.First, assign clusters dataframe.(Output res.km following)Second, visualize strength variables using heatmap describe different clusters.clustering variables shows cluster 4 largest area average income. However, comprises one observation thus less interpretable. Cluster 3 average income, life expectancy highschool graduation, average illiteracy. cluster can seen one worse performing states developmental areas. Cluster 2 1 relatively similar mostly average characteristics. meaningful difference population. Therefore, call cluster 2 “Low-populated average states” cluster 2 “High-populated average states”.see final classification results :hope tutorial helpful ! Good luck next clustering analysis!","code":"\ndf_clusters <- res.km$centers\nres.km## K-means clustering with 4 clusters of sizes 15, 23, 11, 1\n## \n## Cluster means:\n##   Population      Income  Illiteracy    Life.Exp    HS.Grad      Frost\n## 1  1.0136832  0.61841919  0.09296733  0.07901309  0.1822459 -0.5975278\n## 2 -0.5147347  0.08615414 -0.74968285  0.56233752  0.4850920  0.6945869\n## 3 -0.2269956 -1.30146170  1.39152706 -1.17731360 -1.4157826 -0.7206500\n## 4 -0.8693980  3.05824562  0.54139799 -1.16850978  1.6828035  0.9145676\n##          Area\n## 1 -0.07085360\n## 2 -0.09444464\n## 3 -0.23402899\n## 4  5.80934967\n## \n## Clustering vector:\n##        Alabama         Alaska        Arizona       Arkansas     California \n##              3              4              1              3              1 \n##       Colorado    Connecticut       Delaware        Florida        Georgia \n##              2              2              2              1              3 \n##         Hawaii          Idaho       Illinois        Indiana           Iowa \n##              1              2              1              2              2 \n##         Kansas       Kentucky      Louisiana          Maine       Maryland \n##              2              3              3              2              1 \n##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n##              1              1              2              3              2 \n##        Montana       Nebraska         Nevada  New Hampshire     New Jersey \n##              2              2              2              2              1 \n##     New Mexico       New York North Carolina   North Dakota           Ohio \n##              3              1              3              2              1 \n##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n##              2              2              1              2              3 \n##   South Dakota      Tennessee          Texas           Utah        Vermont \n##              2              3              1              2              2 \n##       Virginia     Washington  West Virginia      Wisconsin        Wyoming \n##              1              1              3              2              2 \n## \n## Within cluster sum of squares by cluster:\n## [1] 66.96776 52.45135 19.80725  0.00000\n##  (between_SS / total_SS =  59.4 %)\n## \n## Available components:\n## \n##  [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n##  [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"silinfo\"     \n## [11] \"nbclust\"      \"data\"         \"gap_stat\"\nmelt_df <- melt(df_clusters)\n\nheatmap <- ggplot(melt_df, aes(Var2, Var1)) +\n  scale_fill_continuous(type = \"viridis\", direction = -1) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = round(value, 1))) +\n  theme_bw() +\n  ggtitle(\"Strength of Each of the Variables in the Clusters\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(x=\"Variable\", y=\"Cluster\")\nheatmap\ndf_original[\"Cluster\"] <- res.km$cluster\ndf_out <- df_original[order(-df_original$Cluster), ]\nknitr::kable(df_out)"},{"path":"tutorial-on-cluster-analysis.html","id":"sources-1","chapter":"42 Tutorial on Cluster Analysis","heading":"42.6 Sources","text":"Giordani, P., Ferraro, M. B., & Martella, F. (2020). Introduction Clustering. https://doi.org/10.1007/978-981-13-0553-5_1Sultana, S. (2020, December 21). Hierarchical Clustering Algorithm Works. Retrieved October 24, 2021, https://dataaspirant.com/hierarchical-clustering-algorithm/#t-1608531820434Rawat, S. (2021, June 25). 6 Types Clustering Algorithms Machine Learning | Analytics Steps. Retrieved October 23, 2021, https://www.analyticssteps.com/blogs/6-types-clustering-algorithms-machine-learningDatanovia. (n.d.). Agglomerative Hierarchical Clustering - Datanovia. Retrieved October 24, 2021, https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/TechVidvan. (n.d.). Cluster Analysis R - Complete Guide Clustering R - TechVidvan. Retrieved October 24, 2021, https://techvidvan.com/tutorials/cluster-analysis--r/R Bloggers. (2019, July). Customer Segmentation using RFM Analysis - Rsquared Academy Blog - Explore Discover Learn. Retrieved October 24, 2021, https://blog.rsquaredacademy.com/customer-segmentation-using-rfm-analysis/=======","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"tutorial-on-cluster-analysis-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43 Tutorial on Cluster Analysis","text":"Jannik Wiedenhaupt","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"what-is-clustering-analysis-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.1 What is Clustering Analysis?","text":"Clustering Analysis data exploration method one popular classification techniques. Clustering works segregating data points different groups based similarity dissimiliarity attributes. means data clustered homogeneity inside clusters maximized heterogeneity clusters maximized. concept novel human understanding, clustering grouping elements based likeness important.Likewise data science machine learning, clustering algorithms carry task labeling unlabelled data inputs helps data interpretation establishing patterns predictive purposes.understand idea clustering, let’s look following picures points customer rated personal importance price quality.\nCan identify groups data points graph?\n\ncluster data like ?\n\nlike ?\nvisual representation (also two-dimensional), can already clearly decide cluster data points. cluster data points properly, need clustering algorithms.","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"what-types-of-clustering-analysis-exist-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.2 What Types of Clustering Analysis Exist?","text":"many different types clustering algorithms particularly useful different situations.four common types :","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"centroid-based-algorithms-2","chapter":"43 Tutorial on Cluster Analysis","heading":"43.2.1 Centroid-based Algorithms","text":"Centroid-based algorithm separate data points based multiple -called centroids data. data point assigned cluster based squared distance centroid. commonly used type clustering.","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"hierarchical-algorithms-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.2.2 Hierarchical Algorithms","text":"Hierarchical algorithms differ centroid-based algorithms constract hierarchy among data points. hierarchy, one can choose different sized clusters based granularity required task hand. normally used hierarchical data structures like company databases taxonomy animal species.\ntwo main types hierarchical algorithms:Agglomerative clustering - observations considered invdividually merged everbigger clustersDivisive cluster - observations considered together split int eversmaller clusters","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"distribution-based-algorithms-2","chapter":"43 Tutorial on Cluster Analysis","heading":"43.2.3 Distribution-based Algorithms","text":"Distribution-based clustering assumes data composed distributions. Therefore, data points considered parts cluster based probability belong given cluster. distance center cluster increases, probability data point belongs cluster decreases. algorithm recommended know distribution data.","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"density-based-algorithms-2","chapter":"43 Tutorial on Cluster Analysis","heading":"43.2.4 Density-based Algorithms","text":"Density-based clustering works detecting regions factors focused ’re separated via means regions might empty sparse. Points part cluster categorized noise. Outliers assigned clusters therefore ignored algorithms.","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"how-does-cluster-analysis-work-on-paper-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.3 How Does Cluster Analysis Work on Paper?","text":"following process followed approaching cluster analysis.Variable selection: Select variables, called bases, used cluster observations. want make decisions based classification, example targeting different groups customers, likely also want additional variables, called descriptors, help understand found clusters.Similarity/Dissimilarity calculation: Choose suitable measures proximity different observations. Based type bases, need choose distance function similarity function. variables compared individually first. , summed calculate total similarity/distance two observations. Comparing observations yields proximity distance matrix.Cluster creation: Choose suitable clustering method ones mentioned needed also objective functions decide clusters merged split .Additional steps (always required):\n1. Determine number clusters. can either done based thorough understanding problem’s domain, planned interpretation, statistical procedure. example required centroid-based algorithms.\n2. Interpretation clusters.\n3. Test strength clustering results. Test internal homogeneity external homogeneity clusters.","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"how-does-cluster-analysis-work-in-r-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4 How Does Cluster Analysis Work in R?","text":"","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"data-preparation-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.1 Data Preparation","text":"First, load dataset. tutorial, use states dataset cluster US states.Second, also use factoextra package particularly eclust function simplify analysis visualization.Third, check data following form:Rows observations columns variablesMissing values removed estimatedData must standardizedAvoid double-weighting underlying constructs avoiding multicollinearity","code":"\ndf <- datasets::state.x77%>%data.frame()\nhead(df, 3)##         Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska         365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n# Delete NA values\ndf <- na.omit(df)\n\n# Save non-scaled version for later\ndf_original <- df\n\n# Standardize variables\ndf <- df %>% mutate_all(~(scale(.) %>% as.vector))\n\ncor_matrix <- cor(df)\ncorrplot(cor_matrix, method = \"number\", type = \"lower\", tl.pos = 'd')\n# Because murder and life expectancy are strongly correlated, we remove murder\ndf <- subset(df, select = -c(Murder))"},{"path":"tutorial-on-cluster-analysis-1.html","id":"centroid-based-algorithms-3","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.2 Centroid-based Algorithms","text":"classic centroid-based algorithm called “k-means” used . K-means takes data points input groups k clusters following process.Select inputsSelect k cluster centersAssign cases closest centerUpdate cluster centersReassign casesRepeat steps 4 5 convergenceGoing process R simple requires one function.\nparameters followingFUNcluster: Clustering function. , k-means.hc_metric: Metric used calculating dissimilarities observations. , euclidean distance.k: Number clusters. 5 guessed lack exploration dataset.","code":"\nres.km <- eclust(df, FUNcluster = \"kmeans\", k = 5, hc_metric = \"euclidean\")"},{"path":"tutorial-on-cluster-analysis-1.html","id":"choosing-the-number-of-clusters-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.2.1 Choosing the Number of Clusters","text":"Alternatively setting number clusters k , can also resort different statistics:1. Gap Statistic2. Silhouette Plot3. Elbow Method\nelbow method visual method, determine cluster based spotting elbow graph.weak (pronounced) elbows 2 6.4. Indices\nUse package NbClust experiment different clustering methods, distances, indices.","code":"\nres.km <- eclust(df, FUNcluster = \"kmeans\", hc_metric = \"euclidean\", graph = FALSE)\nfviz_gap_stat(res.km$gap_stat)\nfviz_silhouette(res.km)##   cluster size ave.sil.width\n## 1       1   15          0.08\n## 2       2   23          0.36\n## 3       3   11          0.52\n## 4       4    1          0.00\nfviz_nbclust(df, FUNcluster = kmeans, method = \"wss\") + labs(subtitle = \"Elbow method\") \ncat(\"C-Index:\\n\", NbClust(data=df, method = \"kmeans\", distance = \"euclidean\", index=\"cindex\")$Best.nc)## C-Index:\n##  3 0.2594\ncat(\"Dunn-Index:\\n\", NbClust(data=df, method = \"kmeans\", distance = \"euclidean\", index=\"dunn\")$Best.nc)## Dunn-Index:\n##  15 0.3403\ncat(\"McClain-Index:\\n\", NbClust(data=df, method = \"kmeans\", distance = \"euclidean\", index=\"mcclain\")$Best.nc)## McClain-Index:\n##  2 0.3932"},{"path":"tutorial-on-cluster-analysis-1.html","id":"hierarchial-algorithms-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.3 Hierarchial Algorithms","text":"two fundamental methods hierarchical clustering - agglomerative divisive clustering. explain .\nhierarchical clustering need define calculate number clusters running algorithm. Moreover, hierarchical clustering results comprehensible tree-like structure called Dendrogram allows us find number clusters interpretable.","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"divisive-hierarchical-clustering-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.3.1 Divisive Hierarchical Clustering","text":"objects points dataset belong one single clusterPartition single cluster two least similar clustersRepeat step 2 observation single clusterThe parameters followingFUNcluster: “hclust” divisive clustering.hc_metric: “euclidean” euclidean distance., see discrepancy k-means clustering. gap-statistic yielded 4 optimal clusters, hierarchical clustering identifies 2 major cluster.","code":"\nres.hclust <- eclust(df, FUNcluster = \"hclust\", hc_metric = \"euclidean\")\nfviz_dend(res.hclust, rect = TRUE)\nfviz_cluster(res.hclust, labelsize = 10)"},{"path":"tutorial-on-cluster-analysis-1.html","id":"agglomerative-hierarchical-clustering-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.3.2 Agglomerative Hierarchical Clustering","text":"observation single clusterEvery two observations closest according distance measure, clusteredRepeat step 2 observations one clusterIt important notice agglomerative clustering requires agglomeration method specified. different agglomeration methods can read : https://en.wikipedia.org/wiki/Hierarchical_clustering#Linkage_criteria.\nchoose commonly used ward.D2 measure minimized total within-cluster variance.parameters followingFUNcluster: “agnes” agglomerative nesting.hc_method: Agglomeration method. , ward.D2.hc_metric: “euclidean” euclidean distance.possible see differences agglomerative diviseve clustering, two methods come result example.","code":"\nres.aclust <- eclust(df, FUNcluster = \"hclust\", hc_metric = \"euclidean\", hc_method = \"ward.D2\")\nfviz_dend(res.aclust, rect = TRUE)\nfviz_cluster(res.aclust, labelsize = 10)"},{"path":"tutorial-on-cluster-analysis-1.html","id":"distribution-based-algorithms-3","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.4 Distribution-based Algorithms","text":"explanation good R-tutorial distribution-based algorithms, please visit (Note: Distribution-based algorithms called model-based algorithms ): https://www.datanovia.com/en/lessons/model-based-clustering-essentials/","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"density-based-algorithms-3","chapter":"43 Tutorial on Cluster Analysis","heading":"43.4.5 Density-based Algorithms","text":"explanation good R-tutorial density-based algorithms, please visit: https://www.datanovia.com/en/lessons/dbscan-density-based-clustering-essentials/","code":""},{"path":"tutorial-on-cluster-analysis-1.html","id":"using-clustering-for-further-analysis-1","chapter":"43 Tutorial on Cluster Analysis","heading":"43.5 Using Clustering for Further Analysis","text":"clustering observations, want understand clusters mean. , visualize average strenght variable cluster.First, assign clusters dataframe.(Output res.km following)Second, visualize strength variables using heatmap describe different clusters.clustering variables shows cluster 4 largest area average income. However, comprises one observation thus less interpretable. Cluster 3 average income, life expectancy highschool graduation, average illiteracy. cluster can seen one worse performing states developmental areas. Cluster 2 1 relatively similar mostly average characteristics. meaningful difference population. Therefore, call cluster 2 “Low-populated average states” cluster 2 “High-populated average states”.see final classification results :hope tutorial helpful ! Good luck next clustering analysis!","code":"\ndf_clusters <- res.km$centers\nres.km## K-means clustering with 4 clusters of sizes 15, 23, 11, 1\n## \n## Cluster means:\n##   Population      Income  Illiteracy    Life.Exp    HS.Grad      Frost\n## 1  1.0136832  0.61841919  0.09296733  0.07901309  0.1822459 -0.5975278\n## 2 -0.5147347  0.08615414 -0.74968285  0.56233752  0.4850920  0.6945869\n## 3 -0.2269956 -1.30146170  1.39152706 -1.17731360 -1.4157826 -0.7206500\n## 4 -0.8693980  3.05824562  0.54139799 -1.16850978  1.6828035  0.9145676\n##          Area\n## 1 -0.07085360\n## 2 -0.09444464\n## 3 -0.23402899\n## 4  5.80934967\n## \n## Clustering vector:\n##        Alabama         Alaska        Arizona       Arkansas     California \n##              3              4              1              3              1 \n##       Colorado    Connecticut       Delaware        Florida        Georgia \n##              2              2              2              1              3 \n##         Hawaii          Idaho       Illinois        Indiana           Iowa \n##              1              2              1              2              2 \n##         Kansas       Kentucky      Louisiana          Maine       Maryland \n##              2              3              3              2              1 \n##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n##              1              1              2              3              2 \n##        Montana       Nebraska         Nevada  New Hampshire     New Jersey \n##              2              2              2              2              1 \n##     New Mexico       New York North Carolina   North Dakota           Ohio \n##              3              1              3              2              1 \n##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n##              2              2              1              2              3 \n##   South Dakota      Tennessee          Texas           Utah        Vermont \n##              2              3              1              2              2 \n##       Virginia     Washington  West Virginia      Wisconsin        Wyoming \n##              1              1              3              2              2 \n## \n## Within cluster sum of squares by cluster:\n## [1] 66.96776 52.45135 19.80725  0.00000\n##  (between_SS / total_SS =  59.4 %)\n## \n## Available components:\n## \n##  [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n##  [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"silinfo\"     \n## [11] \"nbclust\"      \"data\"         \"gap_stat\"\nmelt_df <- melt(df_clusters)\n\nheatmap <- ggplot(melt_df, aes(Var2, Var1)) +\n  scale_fill_continuous(type = \"viridis\", direction = -1) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = round(value, 1))) +\n  theme_bw() +\n  ggtitle(\"Strength of Each of the Variables in the Clusters\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(x=\"Variable\", y=\"Cluster\")\nheatmap\ndf_original[\"Cluster\"] <- res.km$cluster\ndf_out <- df_original[order(-df_original$Cluster), ]\nknitr::kable(df_out)"},{"path":"tutorial-on-cluster-analysis-1.html","id":"sources-2","chapter":"43 Tutorial on Cluster Analysis","heading":"43.6 Sources","text":"Giordani, P., Ferraro, M. B., & Martella, F. (2020). Introduction Clustering. https://doi.org/10.1007/978-981-13-0553-5_1Sultana, S. (2020, December 21). Hierarchical Clustering Algorithm Works. Retrieved October 24, 2021, https://dataaspirant.com/hierarchical-clustering-algorithm/#t-1608531820434Rawat, S. (2021, June 25). 6 Types Clustering Algorithms Machine Learning | Analytics Steps. Retrieved October 23, 2021, https://www.analyticssteps.com/blogs/6-types-clustering-algorithms-machine-learningDatanovia. (n.d.). Agglomerative Hierarchical Clustering - Datanovia. Retrieved October 24, 2021, https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/TechVidvan. (n.d.). Cluster Analysis R - Complete Guide Clustering R - TechVidvan. Retrieved October 24, 2021, https://techvidvan.com/tutorials/cluster-analysis--r/R Bloggers. (2019, July). Customer Segmentation using RFM Analysis - Rsquared Academy Blog - Explore Discover Learn. Retrieved October 24, 2021, https://blog.rsquaredacademy.com/customer-segmentation-using-rfm-analysis/","code":""},{"path":"apply-function-in-r.html","id":"apply-function-in-r","chapter":"44 Apply function in r","heading":"44 Apply function in r","text":"Tianmai JiaoMy youtube link: https://youtu./GVoa_zwYGzs\ntutorial video apply function R. video, firstly introduce components apply function, give examples regarding use mean,max,percentile,plot function apply(). Finally, write function hand, apply apply function.","code":""},{"path":"introduction-to-interactive-graphs-in-r.html","id":"introduction-to-interactive-graphs-in-r","chapter":"45 Introduction to interactive graphs in R","heading":"45 Introduction to interactive graphs in R","text":"Lihui Pan(lp2892)Many different graphs intensely discussed class using ggplot, static. may others due ability show changes whole process, like alluvial diagrams, others can see differences groups.kinds diagrams may disadvantages, example. hard us go deeper specific group. Usually, drawing another diagram way. However, using interactive diagrams likely deal problems. Besides, actual-world application, changing aspects picture time may help better clarify problems deliver ideas.finishing document, understand popular packages used carry interactive plots know draw basic diagrams using . Apart , also learned three important skills:(1) download packages GitHub: Simply use install_github devtools - devtools::install_github()(2) change different R versions: Multiple ways posted online. However, easiest way right now using Rswitch. (Download : https://rud./rswitch/)(3)skip chunks knitting: Add eval=FALSE, message=TRUEThere also problems document need fixing, still find perfect solution:Diagram pictured rPlot can shown viewer rather directly chunksOutput fails returns error pandoc document conversion failed error 1\nThus, section 4&5 graph shown html file work rmd file.possible, may go future give detailed explanation packages.","code":"\nlibrary(networkD3)\nlibrary(visNetwork)\nlibrary(igraph)\nlibrary(igraphdata)\nlibrary(stringr)\nlibrary(rpart)\nlibrary(sparkline)\nlibrary(dygraphs)\nlibrary(plotly)\nlibrary(devtools)\n# library(recharts) remove this package for loading error and not being used in file\nlibrary(rCharts)\nlibrary(highcharter)\nlibrary(tidyverse)\nlibrary(data.table)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"networkd3","chapter":"45 Introduction to interactive graphs in R","heading":"45.1 networkD3","text":"field data visualization, visualization relational network data always topic widespread concern. example, can clearly understand relationship different characters kinds pictures. However, static pictures meet deep-seated needs, : quickly find character (node)? functions achieved static network diagrams, usually require introduction JavaScript implement interactive functions. , networkD3 works.(majority examples http://christophergandrud.github.io/networkD3/ slightly modified adopt documentation)","code":"\nlibrary(networkD3)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"simplenetwork","chapter":"45 Introduction to interactive graphs in R","heading":"45.1.1 simplenetwork","text":"start simple network.","code":"\nlibrary(networkD3)\n\nsrc <- c(\"A\", \"A\", \"A\", \"A\",\n        \"B\", \"B\", \"C\", \"C\", \"D\")\ntarget <- c(\"B\", \"C\", \"D\", \"J\",\n            \"E\", \"F\", \"G\", \"H\", \"I\")\nnetworkData <- data.frame(src, target)\n\nsimpleNetwork(networkData)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"forcenetwork","chapter":"45 Introduction to interactive graphs in R","heading":"45.1.2 forceNetwork","text":"come forceNetwork can parameters define network looks like. used data draw graphs second graph, parameters set like zoom helped zoom well change position pictures details.","code":"\ndata(MisLinks)\ndata(MisNodes)\nforceNetwork(Links = MisLinks, Nodes = MisNodes,\n             Source = \"source\", Target = \"target\",\n             Value = \"value\", NodeID = \"name\",\n             Group = \"group\")\nforceNetwork(Links = MisLinks, Nodes = MisNodes,\n             Source = \"source\", Target = \"target\",\n             Value = \"value\", NodeID = \"name\",\n             Group = \"group\", opacity = 0.8,\n             zoom = TRUE)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"sankeynetwork","chapter":"45 Introduction to interactive graphs in R","heading":"45.1.3 sankeyNetwork","text":"Sankey diagrams, also known Sankey energy flow diagrams, form flow chart, usually used show changes relationships “flow” data. Famous Matthew Henry Phineas Riall Sankey’s 1898 “diagram energy efficiency Steam Engine,” since named Sankey Diagram . Sankey diagrams, lines represent flow one node another, better suited visualizing “energy diversion” features compared conventional bar charts pie charts. time, width line proportional flow, larger width , larger flow . Important flows can identified intuitively. addition, connection level reflects traffic value also shows information structure distribution defined system. characteristics, Sankey diagrams widely used visualization analysis natural social sciences.","code":"\n# Load energy projection data\n# Load energy projection data\nURL <- paste0(\n        \"https://cdn.rawgit.com/christophergandrud/networkD3/\",\n        \"master/JSONdata/energy.json\")\nEnergy <- jsonlite::fromJSON(URL)\n# Plot\nsankeyNetwork(Links = Energy$links, Nodes = Energy$nodes, Source = \"source\",\n             Target = \"target\", Value = \"value\", NodeID = \"name\",\n             units = \"TWh\", fontSize = 12, nodeWidth = 30)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"visnetwork","chapter":"45 Introduction to interactive graphs in R","heading":"45.2 visNetwork","text":"visNetwork every similar usage networkD3. Thus, one example displayed . perspective, advantages. example, comes drawing Sankey diagram, prefer networkD3. However, visualizing decision tree model required, visNetowrk first choice.","code":"\nlibrary(visNetwork)\nlibrary(igraph)\nlibrary(igraphdata)\nlibrary(stringr)\nlibrary(rpart)\nlibrary(sparkline)\n\ndata(\"karate\")\n\nkaratedf <- igraph::as_data_frame(karate,what = \"both\")\nnodedf <- karatedf$vertices\nedagedf <- karatedf$edges\n#define the shape of nodes\nshape <-  c(\"square\", \"triangle\", \"box\", \"circle\", \"dot\", \"star\",\n          \"ellipse\", \"database\", \"text\", \"diamond\")\n#define the color of nodes\ncolor <- c(\"orange\", \"darkblue\", \"purple\",\"darkred\", \"grey\")\n#define the size of nodes\nnodesize <- degree(karate)\nNewnodes <- data.frame(id=nodedf$name, \n                       label = nodedf$label, \n                       group = paste(\"Group\",nodedf$Faction), \n                       title = nodedf$name,\n                       shape = shape[nodedf$Faction], \n                       color = color[nodedf$color], \n                       size = 10+ nodesize /2  \n                       )\nNewedages <- data.frame(from = edagedf$from,\n                        to = edagedf$to,\n                        label = paste(\"weight\",edagedf$weight,sep = \"-\"),\n                        width = edagedf$weight, \n                        color = color[edagedf$weight]\n                        )\nvisNetwork(Newnodes, Newedages, height = \"600px\", width = \"100%\",\n           main = \"visNetwork\",background = \"lightblue\") %>%\n  visGroups(groupname = \"Group 1\",color = color[1], shape = shape[1])%>%\n  visGroups(groupname = \"Group 2\",color = color[2], shape = shape[2])%>%\n  visLegend(useGroups = TRUE,width = 0.1,position = \"right\")%>%\n  visOptions(selectedBy = \"group\", \n             highlightNearest = TRUE, \n             nodesIdSelection = TRUE)%>%\n  visLayout(randomSeed = 4)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"dygraphs","chapter":"45 Introduction to interactive graphs in R","heading":"45.3 dygraphs","text":"Dygraphs open-source JS library. used generate scalable time charts can interacted user. mainly used display dense data sets, users can browse view data well.(Cite http://rstudio.github.io/dygraphs/index.html)Since time series really import finance sales volume forecasting, quite lot projects used R support. great example Kaggle Ross store sales volume forecasting visualization R :https://www.kaggle.com/shearerp/rossmann-store-sales/interactive-sales-visualization/code.detailed information can found .","code":"\nlibrary(dygraphs)\nlungDeaths <- cbind(mdeaths, fdeaths)\ndygraph(lungDeaths) %>%\n  dySeries(\"mdeaths\", label = \"Male\") %>%\n  dySeries(\"fdeaths\", label = \"Female\") %>%\n  dyOptions(stackedGraph = TRUE) %>%\n  dyRangeSelector(height = 20)\nhw <- HoltWinters(ldeaths)\npredicted <- predict(hw, n.ahead = 72, prediction.interval = TRUE)\n\ndygraph(predicted, main = \"Predicted Lung Deaths (UK)\") %>%\n  dyAxis(\"x\", drawGrid = FALSE) %>%\n  dySeries(c(\"lwr\", \"fit\", \"upr\"), label = \"Deaths\") %>%\n  dyOptions(colors = RColorBrewer::brewer.pal(3, \"Set1\"))"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"plotly","chapter":"45 Introduction to interactive graphs in R","heading":"45.4 plotly","text":"Plotly interactive visualization library. official website provides Python, R, Matlab, JavaScript, Excel interfaces, can easily call Plotly software achieve interactive visualization.Plotly supports facet, facet shapes exceed 9, bug appears legend.\nthree main functions:(1)plot_lyplot_ly basic function used generate graph Plotly.Main parameters:Data - source dataData - source datatype diagram - ‘scatter’,‘bar’,‘box’,‘heatmap’,‘histogram’,‘histogram 2d’,‘area’,‘pie’,‘contour’,‘histogram 2d’,‘contour’, ‘scatter3d’,‘surface’,‘mesh3d’,scattergeo’,‘choropleth’type diagram - ‘scatter’,‘bar’,‘box’,‘heatmap’,‘histogram’,‘histogram 2d’,‘area’,‘pie’,‘contour’,‘histogram 2d’,‘contour’, ‘scatter3d’,‘surface’,‘mesh3d’,scattergeo’,‘choropleth’type symbol - ‘dot’, ‘cross’,‘diamond’, ‘square’, ‘triangle-’, ‘triangle-left’, ‘triangle-right’,‘triangle-’type symbol - ‘dot’, ‘cross’,‘diamond’, ‘square’, ‘triangle-’, ‘triangle-left’, ‘triangle-right’,‘triangle-’(2)add_trace()Add_trace() used add layers original diagram.(3)layout()believe Plotly one useful package among since can directly work ggplot quite familar . Examples shown section3.2.","code":"\nplot_ly(data = data.frame(), ..., type = NULL, color, colors = NULL,\n        alpha = 1, symbol, symbols = NULL, size, sizes = c(10, 100), linetype,\n        linetypes = NULL, split, width = NULL, height = NULL, source = \"A\")add_trace(p = last_plot(), …, group, color, colors, symbol, symbols,size, data = NULL, evaluate = FALSE)p <- layout(p,              \n    title = \"unemployment\",  \n    xaxis = list(title = \"time\", showgrid = F),\n    yaxis = list(title = \"uidx\"),\n    annotations = listlist(x = maxdf$date,y = maxdf$uempmed,text = \"Peak\",showarrow = T)\n    )\n)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"direct-usage","chapter":"45 Introduction to interactive graphs in R","heading":"45.4.1 Direct Usage","text":"","code":"\nlibrary(plotly)\nnames(iris) = gsub(\"\\\\.\", \"\", names(iris))\np <- plot_ly(iris, x = ~PetalLength, y = ~PetalWidth, color = ~Species, colors = \"Set1\", mode = \"markers\")\np"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"combining-with-gglot","chapter":"45 Introduction to interactive graphs in R","heading":"45.4.2 Combining with gglot","text":"","code":"\nlibrary(ggplot2)\np <- ggplot(data=lattice::singer,aes(x=height,fill=voice.part))+\n  geom_density()+\n  facet_grid(voice.part~.)+\n  ggtitle('Original plot') +\n  theme(plot.title = element_text(hjust = 0.5))\np\nlibrary(plotly)\np <- ggplot(data=lattice::singer,aes(x=height,fill=voice.part))+\n  geom_density()+\n  facet_grid(voice.part~.)+\n  ggtitle('Original plot using ggplotly') +\n  theme(plot.title = element_text(hjust = 0.5))\n(gg <- ggplotly(p))"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"rcharts","chapter":"45 Introduction to interactive graphs in R","heading":"45.5 rCharts","text":"rCharts may famous popular package interactive diagrams. R package create, customize publish interactive javascript visualizations R using familiar lattice style plotting interface(Cited https://ramnathv.github.io/rCharts/). way use functions quite straightforward - type used define specific diagram type, formulation data used specify data use use.","code":"\n# devtools::install_github('ramnathv/rCharts',force = TRUE)\nlibrary(devtools)\nlibrary(knitr)"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"rplot","chapter":"45 Introduction to interactive graphs in R","heading":"45.5.1 rPlot","text":"","code":"\nlibrary(rCharts)\nnames(iris) = gsub(\"\\\\.\", \"\", names(iris))\np <- rPlot(PetalWidth ~ PetalLength, data = iris, color = 'Species', type = 'point')\n#p$save(\"p1.1.html\", standalone = TRUE)\np"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"hplot","chapter":"45 Introduction to interactive graphs in R","heading":"45.5.2 hPlot","text":"Highcharts pure Javascript library charting, supporting types charts: line charts, graphs, region charts, region charts, bar charts, pie charts, scatter charts, etc. hPlot function provided rCharts package achieve . example plotted bubble plots student height pulse beats per minute, using age variable adjust bubble size.","code":"\nlibrary(rCharts)\na <- hPlot(Pulse ~ Height, data = MASS::survey, type = \"bubble\",title = \"hPlot Example\", subtitle = \"bubble chart\",size = \"Age\", group = \"Exer\")\na$colors('rgba(223, 83, 83, .5)', 'rgba(119, 152, 191, .5)','rgba(60, 179, 113, .5)')\na$chart(zoomType = \"xy\")\na$exporting(enabled = T)\n#a$save(\"p1.2.html\", standalone = TRUE)\na"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"nplot","chapter":"45 Introduction to interactive graphs in R","heading":"45.5.3 nPlot","text":"detailed tutorials examples can found .http://ramnathv.github.io/rCharts//https://github.com/ramnathv/rCharts/tree/master/demo","code":"\nlibrary(rCharts)\nhair_eye_male <- subset(as.data.frame(HairEyeColor), Sex == \"Male\")\nhair_eye_male[,1] <- paste0(\"Hair\",hair_eye_male[,1])\nhair_eye_male[,2] <- paste0(\"Eye\",hair_eye_male[,2])\nn1 <- nPlot(Freq ~ Hair, group = \"Eye\", data = hair_eye_male,type = \"multiBarChart\")\n#n1$save(\"p1.3.html\", standalone = TRUE)\nn1"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"recharts","chapter":"45 Introduction to interactive graphs in R","heading":"45.6 reCharts","text":"reCharts means redefine. reason behind name package gives users different experience design. redefine React design, also redefines composition configuration.\ndevelopers mentioned introduced package, main problems creating diagrams maybe :\n(1) much parameter can used, make process complicated also cause misunderstandings.\n(2) Styles diagrams vary lot hard unify. example, someone may ask - pillar bar chart triangle?reCharts sovles problems use following methods: (1) Declarative tags make writing charts easy writing HTML\n(2) Configuration items close native SVG make configuration items natural\n(3) Interface API, solve variety personalized needsMore detailed tutorials examples can found .https://recharts.org/en-US/","code":"\n# devtools::install_github('madlogos/recharts',force = TRUE)\nlibrary(devtools)\nlibrary(recharts)\nnames(iris) = gsub(\"\\\\.\", \"\", names(iris))\np <- echart(data=iris,x = ~PetalLength, y = ~PetalWidth,series = ~Species,type = 'scatter')\np\nhair_eye_male <- subset(as.data.frame(HairEyeColor), Sex == \"Male\")\nhair_eye_male[,1] <- paste0(\"Hair\",hair_eye_male[,1])\nhair_eye_male[,2] <- paste0(\"Eye\",hair_eye_male[,2])\np <- echart(data = hair_eye_male, x = ~Hair, y = ~Freq,  series = ~Eye,\n       type = 'bar', palette='fivethirtyeight',\n       xlab = 'Hair', ylab = 'Freq')\np"},{"path":"introduction-to-interactive-graphs-in-r.html","id":"highcharter","chapter":"45 Introduction to interactive graphs in R","heading":"45.7 highcharter","text":"final package document highcharter. Highcharter chart library written pure JavaScript makes easy add interactive charts Web sites Web applications.\nsection, complicated diagram introduced try figure procedures detailed way.(cite fromhttps://zhuanlan.zhihu.com/p/42430990)","code":"\nlibrary(highcharter)\nlibrary(tidyverse)\nlibrary(data.table)\nm <- c(1746181,1884428,2089758,2222362,2537431,2507081,2443179,2664537,3556505,3680231, 3143062 ,2721122, 2229181 ,2227768,\n       2176300, 1329968 , 836804,354784,90569,28367,3878)\nf <- c(1656154, 1787564, 1981671, 2108575, 2403438, 2366003, 2301402, 2519874,3360596, 3493473, 3050775, 2759560, 2304444,\n       2426504, 2568938, 1785638,1447162, 1005011, 330870, 130632, 21208)\nclass <- c('0-4', '5-9', '10-14', '15-19','20-24', '25-29', '30-34', '35-39', '40-44','45-49', '50-54','55-59', '60-64',\n           '65-69', '70-74', '75-79', '80-84', '85-89', '90-94','95-99', '100 + ')\n\nhighchart() %>%\n    hc_xAxis(list(categories = class,\n        reversed = FALSE, # whether to flip the coordinate\n        labels = list(step = 1)),\n      list(\n        opposite = TRUE, # ajust the position of the coordinate\n        categories = class,\n        reversed = FALSE,\n        linkedTo = 0,\n        labels = list(step = 1))\n    ) %>%\n  hc_plotOptions(series = list(borderWidth = 0))%>%\n  hc_yAxis(\n    labels = list(formatter = JS(\"function () { return (Math.abs(this.value) / 1000000) + 'M';}\")),\n    min = -4000000,max = 4000000)%>%# give the range of the x-axis\n  hc_tooltip(formatter = JS(\"function () {\n                                   return '<b>' + this.series.name + ', age ' + this.point.category + '<\/b><br/>' +\n                                     'popluation: ' + Highcharts.numberFormat(Math.abs(this.point.y), 0);}\"))%>%\n  hc_title(text = \"Popluation in 2015 Germany\",align=\"center\")%>%\n  hc_plotOptions(series= list(stacking = \"normal\")) %>%\n  # set the value to be negative to make them display in a same coordinate\n  hc_add_series(name = \"Male\",data = -m, type = \"bar\") %>% \n  hc_add_series(name = \"Female\",data = f,type = \"bar\") %>%\n  hc_add_theme(hc_theme_538())\ntext_data <- data.table(text =c(\"Brazil\",    \"Canada\",  \"Mexico\",  \"USA\",   \"Switerzeland\",  \"France\",   \"Spain\",  \"British\",   \"South Africa\",   \"Russia\",  \"Germany\", \"Iceland\",   \"Korean\",  \"China\",   \"India\",   \"Japan\"),\n                        weight =c(2,    3,  1,  1,  4,  5,  4,  3,  1,  1,  2,  3,  2,  6,  1,  1))\n\nhighchart() %>%\n  hc_title(text = \"World Cloud\") %>%\n  hc_add_series(data = text_data,type = \"wordcloud\",name= \"Score\",hcaes(name = text,weight = weight)) %>%\n  hc_add_theme(hc_theme_flat())\nexplorer_rate <- data.table(name = c('Firefox','IE','Chrome','Safari','Opera','other'),\n                            rate = c(45,    26.8,   12.8,   8.5,6.2,0.7))\n\nhighchart() %>%\n  hc_title(\n    #“<br>” can help change a line\n    text = \"Web broswer<br>Percentage\",\n    #position of title in horizontal axis\n    align = \"center\",\n    #position of title in vertical axis\n    verticalAlign = \"middle\",\n    y = 60) %>%\n  hc_plotOptions(pie = list(\n    dataLabels = list(\n      #show the label\n      enabled = TRUE,\n      #change the position\n      distance = -50,\n      style = list(fontWeight = \"bold\",color = \"white\")),\n    #the start angle of the chart\n    startAngle = -90,\n    #the end angle of the chart\n    endAngle = 90,\n    center = c('50%','75%')))%>%\n  hc_tooltip(\n    headerFormat = \"{series.name}<br>\",\n    pointFormat = \"{point.name}: <b>{point.percentage:.1f}%<\/b>\")%>%\n  hc_add_series(explorer_rate,type = \"pie\",hcaes(name = name, y = rate),name = \"percentage\",\n                #control the size\n                innerSize = \"50%\") %>%\n  hc_add_theme(hc_theme_google())\ntype <- c(\"Sales\", \"Marketing\", \"Development\", \"Support\",\"IT\",\"Administration\")\nactual <- c(50000, 39000, 42000, 31000, 26000, 14000)\nplan <- c(43000, 19000, 60000, 35000, 17000, 10000)\n\nhighchart() %>%\n  #set as polar coordinates\n  hc_chart(polar = TRUE,type = \"line\") %>% \n  hc_title(text = \"Budget and Expenditure\",x=-60) %>%\n  hc_pane(size = \"80%\") %>%\n  hc_legend(align = \"right\",verticalAlign = \"top\",y = 70,layout = \"vertical\") %>%\n  hc_xAxis(categories = type,\n           #hide the horizontal line\n           lineWidth = 0,\n           #rotate to match the vertices of the polygon\n           tickmarkPlacement = \"on\") %>%\n  hc_yAxis(#set as polygon\n           gridLineInterpolation = \"polygon\",\n           #hide the vertial line\n           lineWidth = 0,\n           min = 0) %>%\n  hc_tooltip(#show the result for two different type in the same line\n             shared = TRUE,\n             pointFormat = '<span style=\"color:{series.color}\">{series.name}: <b>${point.y:,.0f}<\/b><br/>')%>%\n  hc_add_series(name = \"Expenditure\",actual) %>%\n  hc_add_series(name = \"Budget\",plan) %>%\n  hc_add_theme(hc_theme_google())"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"plot-and-analyze-stock-data-with-r","chapter":"46 Plot and Analyze Stock Data with R","heading":"46 Plot and Analyze Stock Data with R","text":"Yue Zhang Yue Xiong","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"motivation-2","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.1 Motivation","text":"idea tutorial arise research final project involves lot interactions stock prices. need plot stock prices, analyze data convey financial information behind data. Therefore, gathered useful resources found thoughts tutorial.","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"plot-stock-data","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2 Plot stock data","text":"","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"using-ggplot2-to-plot-time-series","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2.1 Using ggplot2 to plot time series","text":"Let’s first look can basic R ggplot2. Stock data essentially time series, bare minimum, can plot stock data time series withe techniques learned class. Let’s use Pfizer data illustration function. downloaded 2-year historical data Pfizer Yahoo Finance saved csv file.plot, can clearly see stock’s trend past two year. However, time series plot encapsulates close price misses many information (open, high, low etc). exists commonly used financial charts describe price movements better, candlestick chart.","code":"\npfe_df <- read_csv(\"resources/stock_analysis/PFE.csv\")\n\npfe_df %>%\n  ggplot(aes(Date, Close)) +\n  geom_line() +\n  ggtitle(\"Pfizer 2-Year Stock price\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"candlestick-chart","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2.2 Candlestick Chart","text":"","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"background-history","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2.2.1 Background History","text":"Candlestick charts thought developed 18th century Munehisa Homma, Japanese rice trader.[4] introduced Western world Steve Nison book, Japanese Candlestick Charting Techniques. often used today stock analysis along analytical tools.","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"description-1","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2.2.2 Description","text":"“candlestick” typically shows one day. similar bar chart candlestick represents four important pieces information day: open close thick body; high low “candle wick”. asset closed higher opened, body hollow green colored, opening price bottom body closing price top. asset closed lower opened, body solid red colored, opening price top closing price bottom. Thus, color candle represents price movement relative prior period’s close “fill” (solid hollow/green red) candle represents price direction period isolation (solid/red higher open lower close; hollow/green lower open higher close).","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"draw-candlestick-chart-in-r","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2.2.3 Draw Candlestick Chart in R","text":"going use package quantmod draw Candlestick Chart. Quantmod stands quantitative financial modelling framework. three main functions: download data, charting, technical indicator.Instead going search stock’s price Yahoo Finance download csv files read R, easier way quantmod, use function getSymbols. function can help download specific stock price directly r. default source ‘finance.yahoo.com’, can switch sources changing src.need know ticker stock can specify interested date range argument ‘’. , want download past 2 years’ data. gives open price, close price, daily highest price, daily lowest price, daily trading volumes adjusted price.first plot time series line style, looks exactly like one plotted ggplot2.try plot data candlestick style.","code":"\nPFE <- getSymbols(\"PFE\", src=\"yahoo\", from = \"2019-10-25\", to = \"2021-10-25\", auto.assign = FALSE)\nchartSeries(PFE,\n            type=\"line\",\n            theme=chartTheme('white'))\nchartSeries(PFE,\n            type=\"candlesticks\",\n            theme=chartTheme('white'))\nchartSeries(PFE,\n            type=\"candlesticks\",\n            subset='2021-09-25::2021-10-25',\n            theme=chartTheme('white'))"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"interactive-plot","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.2.2.4 Interactive Plot","text":"see previous examples, date range deeply impacts information displayed graph. Sometimes ’d like look stock’s trend long period; sometimes need zoom focus shorter period time look one specific day. ’s hard accomplish static charts graphed quantmod. going introduce plotly graph interactive plots.","code":"\ndf <- data.frame(Date=index(PFE),coredata(PFE))\n\ndf %>% \n  plot_ly(x = ~Date, \n          type=\"candlestick\",\n          open = ~PFE.Open, \n          close = ~PFE.Close,\n          high = ~PFE.High, \n          low = ~PFE.Low) %>% \n  layout(title = \"Basic Candlestick Chart\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"analyze-stock-data","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3 Analyze stock data","text":"","code":""},{"path":"plot-and-analyze-stock-data-with-r.html","id":"simple-moving-average","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.1 Simple Moving Average","text":"simple moving average (SMA) calculates average selected range prices, usually closing prices, number periods range. stock market, simple moving average allows us ignore noise daily price moving rather focus relatively long term trajectory stock. choose n = 5, 30, 200 represent average price short, medium long term respectively. example, graph, around March 2021, can see 200-day SMA PFE rose 30-day SMA, bullish indicator, suggests price PFE increase. Indeed, see PFE’s price rises afterwards.","code":"\nchartSeries(PFE, theme=chartTheme('white'))\naddSMA(n = 5, on = 1, col = \"purple\")\naddSMA(n = 30, on = 1, col = \"blue\")\naddSMA(n = 200, on = 1, col = \"red\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"bolinger-band","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.2 Bolinger Band","text":"Bollinger Band technical analysis tool defined set trendlines plotted x standard deviations (positively negatively) away simple moving average (SMA) security’s price. upper lower bands typically 2 standard deviations +/- 20-day simple moving average, can modified.\nprices move closer upper band, market overbought price likely fall, good time sell; prices move closer lower band, market oversold price likely rise, good time buy. PFE 20-day SMA 2 standard deviation Bolinger Band mid May 2021 mid June 2021 perfectly follows trend.","code":"\nchartSeries(PFE,\n            subset='2021-04-25::2021-10-25',\n            theme=chartTheme('white'))\naddBBands(n=20,sd=2)"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"relative-strength-index","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.3 Relative Strength Index","text":"relative strength index (RSI) momentum indicator measures magnitude recent price changes evaluate overbought oversold conditions price stock asset. standard use 14 periods calculate initial RSI value. RSI value ranges 0 100, normally values 70 indicate stock overbought, good time sell; values 30 indicate stock oversold, good chance buy.","code":"\nchartSeries(PFE,\n            subset='2021-04-25::2021-10-25',\n            theme=chartTheme('white'))\naddRSI(n=14, maType = \"SMA\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"sma-and-bolinger-band-in-interactive-graph","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.4 SMA and Bolinger Band in interactive graph","text":"can also add SMA Bolinger Band top interactive graph.","code":"\nbbands <- BBands(PFE[,c(\"PFE.High\",\"PFE.Low\",\"PFE.Close\")])\ndf <- cbind(df, data.frame(bbands[,1:3]))\n\ndf %>% \n  plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~PFE.Open, close = ~PFE.Close,\n          high = ~PFE.High, low = ~PFE.Low, name = \"PFE\") %>% \n  add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = 'grey', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) %>% \n  add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = 'grey', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") %>% \n  add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = 'pink', width = 0.5),\n            hoverinfo = \"none\", inherit = F) %>% \n  layout(yaxis = list(title = \"Price\"))"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"calendar-heatmap","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.5 Calendar Heatmap","text":"can calculate aggregated return percentage using function periodReturn. aggregate level can specified changing argument period. set aggregate level daily, can also set monthly, yearly etc.Now, want know daily return looks like across year, can use Calendar Heatmap. need first construct dataframe, containing information week year day week . Function .POSIXlt used convert date number week year number day week respectively.dataframe, can know using ggplot plot calendar color calender daily return stock. Since stock market, green represents increasing price red represents decreasing price, follow color pattern Calendar Heatmap.However, graph contain monthly information, need another way include month Calendar Heatmap.*Take note, idea keeps previous method, use stock price data since 2015 include data better demonstration purpose.","code":"\nPFE_ret <- na.omit(periodReturn(PFE, period=\"daily\", type = \"arithmetic\"))\nPFE_ret <- transform(PFE_ret,\n                     week = as.POSIXlt(index(PFE_ret))$yday %/% 7 +1,\n                     wday = as.POSIXlt(index(PFE_ret))$wday,\n                     year = as.POSIXlt(index(PFE_ret))$year + 1900\n                    )\nggplot(PFE_ret, aes(week, wday, fill = daily.returns)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradientn('PFE return', colors= c('red', 'white', 'green')) +\n  facet_wrap(~ year, ncol = 1) +\n  ggtitle(\"PFE weekly return heatmap\")\nPFE_2015 <- getSymbols(\"PFE\", src=\"yahoo\", from = \"2015-01-01\", auto.assign = FALSE)\n\nPFE_2015_ret <- na.omit(periodReturn(PFE_2015, period=\"daily\", type = \"arithmetic\"))\n\ndat <- data.frame(date=index(PFE_2015_ret), PFE_2015_ret$daily.returns)\ndat %>%\n  # calculate all date related columns\n  mutate(year = as.numeric(as.POSIXlt(date)$year + 1900),\n         month = as.numeric(as.POSIXlt(date)$mon + 1),\n         monthf = factor(month,\n                         levels = as.character(1:12), \n                         labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"),\n                         order=TRUE),\n         weekday = as.POSIXlt(date)$wday,\n         weekdayf = factor(weekday,\n                           levels = rev(1:7),\n                           labels=rev(c(\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\")),\n                           ordered=TRUE),\n         yearmonth = as.yearmon(date),\n         yearmonthf = factor(yearmonth),\n         week = as.numeric(format(date,\"%W\"))\n         ) %>%\n  ddply(.(yearmonthf), transform, monthweek = 1 + week - min(week)) %>%\n  # Plot the heatmap\n  ggplot(aes(monthweek, weekdayf, fill = daily.returns)) +\n    geom_tile(color = \"White\") + \n    facet_grid(year~monthf) +\n    scale_fill_gradientn('PFE Returns', colors= c('red', 'white', 'green')) +\n    ggtitle(\"PFE weekly return heatmap\") +\n    xlab(\"week of Month\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"correlation-graph","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.6 Correlation Graph","text":"Now, introduce new useful package trying plot correlation graph r, ggcorrplot. going examine whether correlation different stock price. Correlation concept important finance, one application correlation trading strategy observing price correlated stocks one stock market different time zone, can infer price change stock stock markets time zone.firstly choose list stocks like find correlation get stock price data since 2015 Yahoo Finance. store price stock one column dataframe, called tickerPrices. row, shows closing price stock particular trading day. tickerPrice looks like.Now, need construct correlation matrix pair stocks list tickers using function cor convert correlation matrix molten dataframe can used draw correlation graph using ggplot2. shows normal correlation matrix molten format correlation matrix respectively.Now, since know correlation matrix symmetric along diagonal line, information repetition whole correlation matrix, therefore, half correlation matrix required show correlation different pairs. , can choose either maintain upper triangle lower triangle specify ‘upper’ ‘lower’ argument ‘type’.type graph called Correlogram. stronger correlation , darker color circle . correlation , color circle white. strong positive correlation, color circle approaching dark green, strong negative correlation, color circle approaching dark red.Also, size circle also affected correlation coefficient. larger absolute value correlation , larger circlw .actually helps quickly identify pairs strong correlation, FB ^GSPC, FaceBook S&P 500 respectively.","code":"\ntickers <- c(\"PFE\",\"MRNA\",\"JNJ\",\"SVA\",\"^GSPC\",\"AAPL\",\"AMZN\",\"FB\",\"GS\",\"JPM\")\n\ntickerPrices <- NULL\nfor (ticker in tickers)\n  tickerPrices <- cbind(tickerPrices, getSymbols(ticker, src = \"yahoo\", from = \"2015-01-01\", auto.assign=FALSE)[,4])\n\ndrop_na <- apply(tickerPrices,1,function(x) all(!is.na(x)))\ntickerPrices <- tickerPrices[drop_na]\ncolnames(tickerPrices) <- tickers\ncorr <- cor(tickerPrices)\nmelted_cormat <- melt(corr)\nggcorrplot(corr, \n           type = 'upper',\n           lab = TRUE,\n           lab_size = 3,\n           method = 'circle',\n           colors = c(\"red\", \"white\", \"green\"),\n           title=\"Correlogram of Stocks\",\n           legend.title = \"Correlation\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"violin-chart","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.3.7 Violin Chart","text":"One graph useful determine trading strategy Violin Chart. graph shows distribution returns stock past. understanding normal profit range particular stock , can know exists one particular month two stock performs better usual. example, concept call Santa Relay January Effect trading, tells us stock price usually goes Dec January. Violin Chart allows us check whether particular stock follows 2 concepts. , use Pfizer example demonstration.Since want know monthly return distribution, year, every month 1 data point, make distribution reliable, need get historical data plot violin chart. first Initial Public Offering (IPO) Pfizer made 1972, try retrieve stock price data 1972.first need calculate monthly return Pfizer since 1972 stored returns dataframe called cal_rets show . Take note, function table.CalendarReturns can return table returns containing monthly return yearly return stock. can also modify number decimal place want keep changing value argument digits. Besides, values dataframe percentage since argument .perc set true., remove monthly.returns column since care overall performance Pfizer whole year. transpose dataframe make aggregated level month melt transposed dataframe make ggplot understand data plot violin chart.code plotting violin chart r. x axis month, y axis monthly return stock. wider violin chart , monthly return concentrate wider part. , can see Dec Jan, monthly return highest, Jan, widest part violin chart actually located negative area. shows Pfizer follow 2 rules mentioned , simply buy Pfizer Dec Jan.","code":"\nPFE_1972 <- getSymbols(\"PFE\", src=\"yahoo\", from = \"1972-01-01\", auto.assign = FALSE)[,4]\nPFE_1972_ret <- na.omit(periodReturn(PFE_1972, period=\"monthly\", type = \"arithmetic\"))\ncal_rets <- table.CalendarReturns(PFE_1972_ret, digits = 2, as.perc = TRUE)\ncal_rets$monthly.returns = NULL\ncal_rets_t <- cal_rets %>% \n  t() %>%\n  melt(id.vars=NULL)\nggplot(cal_rets_t, aes(x=Var1,y=value)) +\n  geom_violin(aes(fill = Var1)) +\n  geom_boxplot(width=0.1) +\n  ggtitle(\"PFE Monthly Returns\") +\n  xlab(\"Month\") +\n  ylab(\"return(%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\")"},{"path":"plot-and-analyze-stock-data-with-r.html","id":"citations","chapter":"46 Plot and Analyze Stock Data with R","heading":"46.4 Citations","text":"https://www.quantmod.com/examples/intro/https://plotly.com/r/candlestick-charts/https://www.investopedia.com/terms/s/sma.asphttps://www.investopedia.com/terms/b/bollingerbands.asphttps://www.investopedia.com/terms/r/rsi.asphttp://www.sthda.com/english/wiki/ggcorrplot-visualization---correlation-matrix-using-ggplot2","code":""},{"path":"data-visulization-ordering.html","id":"data-visulization-ordering","chapter":"47 Data Visulization Ordering","heading":"47 Data Visulization Ordering","text":"Anshuo Wu Shengdi ChenFor community contribution, decided give presentation Data Ordering Different Plots. now, seen many various kinds data plots. graphs, different kinds data, usually exists various ways order audience can best visualize analyze information. importance analyzing order within data allow audience better catch key information data, also helps data analysts create appropriate figures different kinds data illustrate effective ordering techniques. Therefore, goal presentation let people need designing graphs organize data visualization clear perceivable way.order accomplish goal project, design content follows. First, introduce different kinds data plots, give succinct explanation data plot can used different circumstances. following functionality type data plots, start telling audience order data plots clarity practicability can maximized. included typical ordering examples data types, including order categorical data, continuous data, ordinal data, numerical data either numerical means ordinal means. ordering within plots, include examples single histogram, multiple histograms faceted single variable, heatmap, mosaic plot, scatter plots, box plots, ridgeline plots. introduce various data plots, summarize suggestions techniques data ordering plots learning orders provided.Overall, learned , plotting dataset, essential understand variables values, can correctly determine type variables need draw. Next, need think sort data plots according type data, type plot.Links:\nVideo,\nSlides","code":""},{"path":"tutorial-on-lattice-package-in-r.html","id":"tutorial-on-lattice-package-in-r","chapter":"48 Tutorial on lattice package in R","heading":"48 Tutorial on lattice package in R","text":"Yunxiao WangInstall lattice packageload lattice","code":"\n#install.packages(\"lattice\")\nlibrary(lattice)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"overview-1","chapter":"48 Tutorial on lattice package in R","heading":"48.1 Overview","text":"lattice package powerful high-level graphing package written Deepayan Sarkar.lattice package provides complete system visualizing univariate multivariable data. Many users turn learn lattice package makes easier generate grid graphics. Grid graphics can show distribution variables relationships variables clearlyThe lattice package provides many useful functions, can generate single factor graphs (dot plot, histogram, bar plot, box plot), bivariate graphs (scatter plots, strip graphs parallel boxplot) multivariate graphs (three-dimensional graphs scatter plot matrixes).","code":""},{"path":"tutorial-on-lattice-package-in-r.html","id":"introduction-3","chapter":"48 Tutorial on lattice package in R","heading":"48.2 Introduction","text":"overall format :graph_function(formula, data=, options)formula:specifies variables displayed adjustment variables;\nData: specify data frame\noptions:comma separated parameters used adjust content, arrangement annotation graphics.","code":""},{"path":"tutorial-on-lattice-package-in-r.html","id":"common-graph_function","chapter":"48 Tutorial on lattice package in R","heading":"48.2.1 Common graph_function:","text":"xyplot(): Scatter plotsplom(): Scatter plot matrixcloud(): 3D scatter plotstripplot(): strip plots (1-D scatter plots)bwplot(): Box plotdotplot(): Dot plotbarchart(): bar charthistogram(): Histogramdensityplot(): Kernel density plotqqmath(): Theoretical quantile plotqq(): Two-sample quantile plotcontourplot(): 3D contour plot surfaceslevelplot(): False color level plot surfacesparallel(): Parallel coordinates plot","code":""},{"path":"tutorial-on-lattice-package-in-r.html","id":"common-formula-format","chapter":"48 Tutorial on lattice package in R","heading":"48.2.2 common formula format","text":"Let lowercase numeric variable, capital letter factor variableThe format :y ~ x | * BVariable left side primary variable, variables right side conditioning variable.y ~ x : y y axis x x axisIf one variable use ~x instead y~x.3D plot，use z ~ x*y instead y~xFor Multivariate graph, like scatter matrix parallel corrdinate plot, use data.frame instead y~x.~x|represent plot x level factor .y~x|*B represent given levels B, plot relationship y xFor convenience, use mtcars dataset tutorial make examples.","code":"\nattach(mtcars)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"scatter-plot-matrix","chapter":"48 Tutorial on lattice package in R","heading":"48.2.3 scatter plot matrix","text":"","code":"\n#Plot scatter matrix\n\nsplom(mtcars[c(1, 2, 3, 4, 5)], \n main=\"Scatter Plot Matrix for mtcars Data\") "},{"path":"tutorial-on-lattice-package-in-r.html","id":"density-plot","chapter":"48 Tutorial on lattice package in R","heading":"48.2.4 Density Plot","text":"","code":"\ndensityplot(~wt, \n main=\"Density Plot\", \n xlab=\"wt\")"},{"path":"tutorial-on-lattice-package-in-r.html","id":"density-plot-by-transmission","chapter":"48 Tutorial on lattice package in R","heading":"48.2.5 Density Plot by transmission","text":"","code":"\n#create new variable\nmtcars$transmission <- factor(mtcars$am, levels=c(0, 1), \n labels=c(\"Automatic\", \"Manual\"))\n#Density Plot by transmission\ndensityplot(~mpg |transmission , data=mtcars,\n main=\"MPG Distribution by Transmission Type\", \n xlab=\"Miles per Gallon\") "},{"path":"tutorial-on-lattice-package-in-r.html","id":"xyplot","chapter":"48 Tutorial on lattice package in R","heading":"48.2.6 xyplot","text":"xyplot also type scatterplot.","code":"\nxyplot(mpg ~ wt | transmission, data=mtcars,\n main=\"Scatter Plots by transmission \", \n xlab=\"Car Weight\", ylab=\"Miles per Gallon\")"},{"path":"tutorial-on-lattice-package-in-r.html","id":"boxplot","chapter":"48 Tutorial on lattice package in R","heading":"48.2.7 boxplot","text":"","code":"\n#group by transmission\n\nbwplot(~mpg | transmission, data=mtcars,\n main=\"Box Plots by transmission\", \n xlab=\"Miles per Gallon\")"},{"path":"tutorial-on-lattice-package-in-r.html","id":"kernel-density-estimation-with-grouped-variables-and-custom-legend","chapter":"48 Tutorial on lattice package in R","heading":"48.2.8 Kernel density estimation with grouped variables and custom legend","text":"put many comments describe things line code .","code":"\n#create new variable\nmtcars$transmission <- factor(mtcars$am, levels=c(0, 1), \n labels=c(\"Automatic\", \"Manual\"))\n#set color\ncolors <- c(\"red\", \"green\") \n#set line type\nlines <- c(1,3)\n#set point type\npoints <- c(16,18)\n\n#customize legend\nkey.trans <- list(title=\"Transmission\", \n space=\"bottom\", columns=2, \n text=list(levels(mtcars$transmission)),\n points=list(pch=points, col=colors), \n lines=list(col=colors, lty=lines), \n cex.title=1, cex=.95)\n\n\n#kernel density plot\ndensityplot(~mpg, data=mtcars, \n#set group by variable\n group=transmission, \n #give title\n main=\"MPG Distribution by Transmission Type\",\n \n xlab=\"Miles per Gallon\", \n #set type of point and line and set color\n pch=points, lty=lines, col=colors,\n #size\n lwd=2, jitter=.005, \n #draw legend as we customized above\n key=key.trans)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"adjust-variable","chapter":"48 Tutorial on lattice package in R","heading":"48.2.9 Adjust variable","text":"regulatory variable usually factor variable. continuous variables still interested relationship?One way use cut() function converts continuous variables discrete variables.\nAlternatively, functions provided lattice package can also efficiently.variable transformed data structure named shingle. Specifically, continuous variables divided series (possibly) overlapping ranges.disp original continious variable，divided three group factor variable","code":"\ndisplace <- equal.count(mtcars$disp, number=3, overlap=0)\nxyplot(mpg~wt|displace, data=mtcars, \n main = \"Miles per Gallon vs. Weight by Engine Displace\", \n xlab = \"Weight\", ylab = \"Miles per Gallon\", \n layout=c(3, 1), aspect=1.5)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"panel-function","chapter":"48 Tutorial on lattice package in R","heading":"48.2.10 Panel function","text":"can create panel functions draw plot 7","code":"\ndisplacement <- equal.count(mtcars$disp, number=3, overlap=0) \n#customize panel function, including plottype, layout, line color..etc.\nmypanel <- function(x, y) { \n panel.xyplot(x, y, pch=15) \n panel.rug(x, y) \n panel.grid(h=-1, v=-1) \n panel.lmline(x, y, col=\"blue\", lwd=1, lty=2) \n} \n#draw grouped scatterplot\nxyplot(mpg~wt|displacement, data=mtcars, \n layout=c(3, 1), \n aspect=1.5, \n main = \"Miles per Gallon vs. Weight by Engine Displacement\", \n xlab = \"Weight\", \n ylab = \"Miles per Gallon\", \n panel = mypanel)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"xyplot-for-customizing-panel-functions-and-additional-options","chapter":"48 Tutorial on lattice package in R","heading":"48.2.11 xyplot for customizing panel functions and additional options","text":"","code":"\nmtcars$transmission <- factor(mtcars$am, levels=c(0,1), \n labels=c(\"Automatic\", \"Manual\")) \npanel.smoother <- function(x, y) { \n panel.grid(h=-1, v=-1) \n panel.xyplot(x, y) \n #panel. loess() give the nonparameter fitting line\n panel.loess(x, y) \n panel.abline(h=mean(y), lwd=2, lty=2, col=\"red\") \n } \nxyplot(mpg~disp|transmission,data=mtcars, \n scales=list(cex=.8, col=\"blue\"), \n panel=panel.smoother, \n xlab=\"Displacement\", ylab=\"Miles per Gallon\", \n main=\"MPG vs Displacement by Transmission Type\", \n sub = \"Dotted lines are Group Means\", aspect=1)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"page-layout","chapter":"48 Tutorial on lattice package in R","heading":"48.2.12 Page layout","text":"","code":""},{"path":"tutorial-on-lattice-package-in-r.html","id":"split","chapter":"48 Tutorial on lattice package in R","heading":"48.2.12.1 split","text":"can put 1 plot 1 page want.\nsplit function can split one page specify number rows columns, put plot specified cell.\nformat split :\nsplit=c(x, y, nx, ny)\nPut current plot xth row yth column cell nx*ny 2D array.\norigin top left corner.","code":"\n#draw a histogram\ngraph1 <- histogram(~height | voice.part, data = singer, \n main = \"Heights of Choral Singers by Voice Part\" ) \n#draw a density plot\ngraph2 <- densityplot(~height|voice.part, data = singer) \n#whoe page is 1*2 matrix\n#plot graph 2 at (1,1)\nplot(graph1, split = c(1, 1, 1, 2)) \n#plot graph 2 at (1,2)\nplot(graph2, split = c(1, 2, 1, 2), newpage = FALSE)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"position","chapter":"48 Tutorial on lattice package in R","heading":"48.2.12.2 position","text":"Another way use postion function:\nposition=c(xmin, ymin, xmax, ymax)\nwhole page 2D coordinate. x y range(0,1).\n(0,0) origin point bottom left.\ncan arrage plot size precisely now.","code":"\ngraph1 <- histogram(~height, data = singer ) \n\ngraph2 <- densityplot(~height, data = singer) \nplot(graph1, position=c(0.3, 0, 1, 1))\nplot(graph2, position=c(0, 0, 0.3, 1), newpage=FALSE)"},{"path":"tutorial-on-lattice-package-in-r.html","id":"sources-3","chapter":"48 Tutorial on lattice package in R","heading":"48.3 Sources","text":"http://mng.bz/jXUGhttp://www.sthda.com/english/wiki/lattice-graphsR action(second edition) Chapter 23","code":""},{"path":"network-visualization-in-r.html","id":"network-visualization-in-r","chapter":"49 Network Visualization in R","heading":"49 Network Visualization in R","text":"Yunze Pan","code":""},{"path":"network-visualization-in-r.html","id":"introduction-4","chapter":"49 Network Visualization in R","heading":"49.1 Introduction","text":"visNetwork powerful tool R help us describe networks explore structure visually. extremely useful us obtain valuable information interactive network graph. tutorial, offer quick introduction newcomers learn concepts creating networks R. Hope enjoy!","code":""},{"path":"network-visualization-in-r.html","id":"installation-1","chapter":"49 Network Visualization in R","heading":"49.2 Installation","text":"main packages going use network visualization R visNetwork igraph. can installed install.packages(“visNetwork”) install.packages(“igraph”).","code":"\nlibrary(visNetwork)\nlibrary(igraph)"},{"path":"network-visualization-in-r.html","id":"dataframe","chapter":"49 Network Visualization in R","heading":"49.3 Dataframe","text":"section create small network simulates student interactions campus. objective get familiar using visNetwork quickly possible. order visualize interactive networks, first read two datasets (nodes data.frame edges data.frame). , can explore various layout options adding different variables nodes data.frame edges data.frame.","code":""},{"path":"network-visualization-in-r.html","id":"nodes","chapter":"49 Network Visualization in R","heading":"49.3.1 Nodes","text":"nodes data.frame must include id column. id represents node want display graph. optional columns can also added nodes data.frame. can help us distinguish nodes graph. example, node student unique assigned id, /name, major, major.type.","code":"\nnodes <- data.frame(id=1:7, # id column (must be called id)\n                    name=c(\"Asher\",\"Bella\",\"Chloe\",\"Daniel\",\"Emma\",\"Frank\",\"Gabriel\"), # student names\n                    major=c(\"CS\",\"CS\",\"CS\",\"STAT\",\"DS\",\"DS\",\"DS\"), # CS: computer science major, STAT: statistics major, DS: data science major\n                    major.type=c(1,1,1,2,3,3,3)) # 1: CS, 2: STAT, 3: DS\ndata.frame(nodes)##   id    name major major.type\n## 1  1   Asher    CS          1\n## 2  2   Bella    CS          1\n## 3  3   Chloe    CS          1\n## 4  4  Daniel  STAT          2\n## 5  5    Emma    DS          3\n## 6  6   Frank    DS          3\n## 7  7 Gabriel    DS          3"},{"path":"network-visualization-in-r.html","id":"edges","chapter":"49 Network Visualization in R","heading":"49.3.2 Edges","text":"edges data.frame must include column column denoting starting node ending node edge. use id represent starting node ending node. also add weight column edges data.frame describe frequency interactions two nodes. example, first row, know student 1 reached student 2 .","code":"\nedges <- data.frame(from=c(1,1,2,3,5,5,6,7),\n                    to=c(2,4,3,1,4,6,7,5),\n                    weight=c(1,1,1,1,1,1,1,1))\ndata.frame(edges)##   from to weight\n## 1    1  2      1\n## 2    1  4      1\n## 3    2  3      1\n## 4    3  1      1\n## 5    5  4      1\n## 6    5  6      1\n## 7    6  7      1\n## 8    7  5      1"},{"path":"network-visualization-in-r.html","id":"visualiztion","chapter":"49 Network Visualization in R","heading":"49.4 Visualiztion","text":"Now can visualize student interaction network using visNetwork. Examples showed . start default setting move customize network better interactive visualization.","code":""},{"path":"network-visualization-in-r.html","id":"minimal-example","chapter":"49 Network Visualization in R","heading":"49.4.1 Minimal Example","text":"","code":"\nvisNetwork(nodes, edges)"},{"path":"network-visualization-in-r.html","id":"customize-node","chapter":"49 Network Visualization in R","heading":"49.4.2 Customize Node","text":"","code":"\ncolors <- colorRampPalette(brewer.pal(3, \"RdBu\"))(3) # use three colors to distinguish students by their majors\nnodes <- nodes %>% mutate(shape=\"dot\", # \"shape\" variable: customize shape of nodes (\"dot\", \"square\", \"triangle\")\n                          shadow=TRUE, # \"shadow\" variable: include/exclude shadow of nodes\n                          title=major, # \"title\" variable: tooltip (html or character), when the mouse is above\n                          label=name, # \"label\" variable: add labels on nodes\n                          size=20, # \"size\" variable: set size of nodes\n                          borderWidth=1, # \"borderWidth\" variable: set border width of nodes\n                          color.background=colors[major.type], # \"color.background\" variable: set color of nodes\n                          color.border=\"grey\", # \"color.border\" variable: set frame color\n                          color.highlight.background=\"yellow\", # \"color.highlight.background\" variable: set color of the selected node\n                          color.highlight.border=\"black\") # \"color.highlight.border\" variable: set frame color of the selected node\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>% # \"main\" variable: add a title\n  visLayout(randomSeed=4) # give a random seed manually so that the layout will be the same every time"},{"path":"network-visualization-in-r.html","id":"customize-edge","chapter":"49 Network Visualization in R","heading":"49.4.3 Customize Edge","text":"","code":"\nedges <- edges %>% mutate(width=weight*3, # \"width\" variable: set width of each edge\n                          color=\"lightgrey\", # \"color\" variable: set color of edges\n                          arrows=\"to\", # \"arrows\" variable: set arrow for each edge (\"to\", \"middle\", \"from \")\n                          smooth=TRUE) # \"smooth\" variable: each edge to be curved or not\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>% \n  visLayout(randomSeed=4)"},{"path":"network-visualization-in-r.html","id":"add-legend-based-on-groups","chapter":"49 Network Visualization in R","heading":"49.4.4 Add Legend Based on Groups","text":"","code":"\nnodes <- nodes %>% mutate(group=major) # add a \"group\" column on node data.frame and add groups on nodes\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>%\n  visLayout(randomSeed=4) %>% \n  visGroups(groupname=\"CS\", color=colors[1]) %>% # color \"colors[1]\" for \"CS\" group \n  visGroups(groupname=\"STAT\", color=colors[2]) %>%\n  visGroups(groupname=\"DS\", color=colors[3]) %>%\n  visLegend(width=0.1, position=\"right\", main=\"Academic Major\") # \"position\" variable: set position (\"left\", \"right\") "},{"path":"network-visualization-in-r.html","id":"select-by-node","chapter":"49 Network Visualization in R","heading":"49.4.5 Select by Node","text":"","code":"\nnodes <- nodes %>% select(-group) # remove \"group\" column because we don't want to show legend this time\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>%\n  visLayout(randomSeed=4) %>% \n  visOptions(nodesIdSelection=TRUE, # \"nodesIdSelection\" variable: select a node by id\n             selectedBy=\"major\") %>% # \"selectedBy\" variable: select a node by the values of a column such as \"major\" column\n  visLegend()"},{"path":"network-visualization-in-r.html","id":"highlight-nearest-nodes","chapter":"49 Network Visualization in R","heading":"49.4.6 Highlight Nearest Nodes","text":"","code":"\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>% \n  visLayout(randomSeed=4) %>% \n  visOptions(highlightNearest = list(enabled = TRUE, # \"enabled\" variable: highlight nearest nodes and edges by clicking on a node\n                                     degree = 2)) # \"degree\" variable: set degree of depth"},{"path":"network-visualization-in-r.html","id":"edit-network","chapter":"49 Network Visualization in R","heading":"49.4.7 Edit Network","text":"","code":"\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>%\n  visLayout(randomSeed=4) %>% \n  visOptions(highlightNearest=TRUE, # degree of depth = 1\n             nodesIdSelection=TRUE,\n             selectedBy=\"major\",\n             manipulation=TRUE) %>%  # \"manipulation\" variable: add/delete nodes/edges or change edges\n  visLegend()"},{"path":"network-visualization-in-r.html","id":"add-navigation-buttons-and-control-interactions","chapter":"49 Network Visualization in R","heading":"49.4.8 Add Navigation Buttons and Control Interactions","text":"","code":"\nvisNetwork(nodes, edges, width=\"100%\", main=\"Student Interaction Network\") %>%\n  visLayout(randomSeed=4) %>%\n  visOptions(highlightNearest=TRUE,\n             nodesIdSelection=TRUE,\n             selectedBy=\"major\") %>% \n  visInteraction(hideEdgesOnDrag=TRUE, # \"hideEdgesOnDrag\" variable: hide edges when dragging the view\n                 dragNodes=TRUE, # \"dragNodes\" variable: hide nodes when dragging the view\n                 dragView=TRUE, # \"dragView\" variable: enable or not the movement of the full network\n                 zoomView=TRUE, # \"zoomView\" variable: enable or not the zoom (use mouse scroll)\n                 navigationButtons=TRUE) %>% # \"navigationButtons\" variable: show navigation buttons\n  visLegend()"},{"path":"network-visualization-in-r.html","id":"export","chapter":"49 Network Visualization in R","heading":"49.5 Export","text":"Finally, use visSave() save network html file.","code":"\nour_network <- visNetwork(nodes, edges)\nvisSave(our_network, file = \"Student Interaction Network.html\", background=\"white\")"},{"path":"network-visualization-in-r.html","id":"help","chapter":"49 Network Visualization in R","heading":"49.6 Help?","text":"information visNetwork.","code":"\n?visNodes\n?visEdges\n?visOptions\n?visGroups\n?visLegend\n?visLayout"},{"path":"network-visualization-in-r.html","id":"social-network-analysis","chapter":"49 Network Visualization in R","heading":"49.7 Social Network Analysis","text":"already learned visualize interactive network. help better understand application, use visNetwork igraph perform social network analysis.","code":""},{"path":"network-visualization-in-r.html","id":"dataset","chapter":"49 Network Visualization in R","heading":"49.7.1 Dataset","text":"investigate interactions movie Star Wars Episode IV. First, import two csv files (“nodes.csv” “edges.csv”). node “nodes.csv” character edge “edges.csv” tells whether two characters appeared together scene movie. Thus, edges undirected. Since characters may appear multiple scenes together, edge weight.group characters (“dark side” “light side” “”).Let’s try another network package called igraph explore network.First, use graph_from_data_frame function, needs two arguments: d vertices. igraph object g indicates 22 nodes 66 edges.Next, output portion adjacency matrix network.","code":"\nsw_nodes <- read.csv(\"https://raw.githubusercontent.com/pablobarbera/data-science-workshop/master/sna/data/star-wars-network-nodes.csv\")\nhead(sw_nodes)##          name id\n## 1       R2-D2  0\n## 2   CHEWBACCA  1\n## 3       C-3PO  2\n## 4        LUKE  3\n## 5 DARTH VADER  4\n## 6       CAMIE  5\nsw_edges <- read.csv(\"https://raw.githubusercontent.com/pablobarbera/data-science-workshop/master/sna/data/star-wars-network-edges.csv\")\nhead(sw_edges)##      source target weight\n## 1     C-3PO  R2-D2     17\n## 2      LUKE  R2-D2     13\n## 3   OBI-WAN  R2-D2      6\n## 4      LEIA  R2-D2      5\n## 5       HAN  R2-D2      5\n## 6 CHEWBACCA  R2-D2      3\ndark_side <- c(\"DARTH VADER\", \"MOTTI\", \"TARKIN\")\nlight_side <- c(\"R2-D2\", \"CHEWBACCA\", \"C-3PO\", \"LUKE\", \"CAMIE\", \"BIGGS\", \"LEIA\", \"BERU\", \"OWEN\", \"OBI-WAN\", \"HAN\", \"DODONNA\", \"GOLD LEADER\", \"WEDGE\", \"RED LEADER\", \"RED TEN\", \"GOLD FIVE\")\nother <- c(\"GREEDO\", \"JABBA\")\nsw_nodes$group <- NA\nsw_nodes$group[sw_nodes$name %in% dark_side] <- \"dark side\"\nsw_nodes$group[sw_nodes$name %in% light_side] <- \"light side\"\nsw_nodes$group[sw_nodes$name %in% other] <- \"other\"\ng <- graph_from_data_frame(d=sw_edges, vertices=sw_nodes, directed=FALSE) # an undirected graph\ng## IGRAPH 035e136 UNW- 22 60 -- \n## + attr: name (v/c), id (v/n), group (v/c), weight (e/n)\n## + edges from 035e136 (vertex names):\n##  [1] R2-D2      --C-3PO       R2-D2      --LUKE        R2-D2      --OBI-WAN    \n##  [4] R2-D2      --LEIA        R2-D2      --HAN         R2-D2      --CHEWBACCA  \n##  [7] R2-D2      --DODONNA     CHEWBACCA  --OBI-WAN     CHEWBACCA  --C-3PO      \n## [10] CHEWBACCA  --LUKE        CHEWBACCA  --HAN         CHEWBACCA  --LEIA       \n## [13] CHEWBACCA  --DARTH VADER CHEWBACCA  --DODONNA     LUKE       --CAMIE      \n## [16] CAMIE      --BIGGS       LUKE       --BIGGS       DARTH VADER--LEIA       \n## [19] LUKE       --BERU        BERU       --OWEN        C-3PO      --BERU       \n## [22] LUKE       --OWEN        C-3PO      --LUKE        C-3PO      --OWEN       \n## + ... omitted several edges\ng[1:6, 1:6] # the first six rows and columns## 6 x 6 sparse Matrix of class \"dgCMatrix\"\n##             R2-D2 CHEWBACCA C-3PO LUKE DARTH VADER CAMIE\n## R2-D2           .         3    17   13           .     .\n## CHEWBACCA       3         .     5   16           1     .\n## C-3PO          17         5     .   18           .     .\n## LUKE           13        16    18    .           .     2\n## DARTH VADER     .         1     .    .           .     .\n## CAMIE           .         .     .    2           .     ."},{"path":"network-visualization-in-r.html","id":"visualization","chapter":"49 Network Visualization in R","heading":"49.7.2 Visualization","text":"Alternatively, can show heat map adjacency matrix. number square equals weight one edge. observe LUKE popular character.also compute characters’ importance using strength() function based number scenes appear rank importance descending order. goal strength() function sum edge weights adjacent edges node., use visNetwork visualize.may wonder important character Star Wars network. Therefore, want utilize three proposed measures (degree centrality, betweenness centrality, closeness centrality) quantify node’s importance network visualize importance different others.","code":"\nsw_matrix <- as.matrix(g[])\nsw_matrix <- sw_matrix[order(rownames(sw_matrix)), order(colnames(sw_matrix))]\nmelted_sw_matrix <- melt(sw_matrix)\nggplot(melted_sw_matrix, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() +\n  geom_text(aes(label=value), color=\"red\") +\n  scale_fill_gradient(low=\"white\", high=\"black\") +\n  xlab(\"characters\") + ylab(\"characters\") +\n  theme(axis.text.x=element_text(angle=45)) +\n  labs(fill=\"weight\")\nimportance <- strength(g)\nsw_nodes$importance <- importance\nhead(arrange(sw_nodes, -importance))##        name id      group importance\n## 1      LUKE  3 light side        129\n## 2       HAN 13 light side         80\n## 3     C-3PO  2 light side         64\n## 4 CHEWBACCA  1 light side         63\n## 5      LEIA  7 light side         59\n## 6     R2-D2  0 light side         50\nsw_colors <- colorRampPalette(brewer.pal(3, \"RdBu\"))(3)\nsw_nodes$group.type <- NA\nsw_nodes$group.type[sw_nodes$group==\"dark side\"] <- sw_colors[1]\nsw_nodes$group.type[sw_nodes$group==\"other\"] <- sw_colors[2]\nsw_nodes$group.type[sw_nodes$group==\"light side\"] <- sw_colors[3]\nsw_nodes <- sw_nodes %>% select(-id) %>%\n  mutate(id=name,\n         shape=\"dot\",\n         shadow=TRUE,\n         title=group,\n         label=name,\n         size=log((importance+3)^5), # adjust size with respect to a node's importance\n         borderWidth=1,\n         color.background=group.type,\n         color.border=\"grey\",\n         color.highlight.background=\"yellow\",\n         color.highlight.border=\"black\") %>% \n  arrange(id)\nsw_edges <- sw_edges %>% mutate(from=source,\n                                to=target,\n                                width=log((weight+3)^1.5), # adjust width with respect to an edge's weight\n                                color=\"lightgrey\",\n                                smooth=FALSE)\nvisNetwork(sw_nodes, sw_edges, width=\"100%\", main=\"Star Wars Episode IV Network\") %>%\n  visLayout(randomSeed=21) %>% \n  visGroups(groupname=\"dark side\", color=sw_colors[1]) %>%\n  visGroups(groupname=\"other\", color=sw_colors[2]) %>%\n  visGroups(groupname=\"light side\", color=sw_colors[3]) %>%\n  visLegend(width=0.1, position=\"right\", main=\"Group\") %>%\n  visOptions(highlightNearest=TRUE,\n             nodesIdSelection=TRUE,\n             selectedBy=\"group\") %>% \n  visInteraction(hideEdgesOnDrag=TRUE,\n                 dragNodes=TRUE,\n                 dragView=TRUE,\n                 zoomView=TRUE,\n                 navigationButtons=TRUE)"},{"path":"network-visualization-in-r.html","id":"centrality-measurement","chapter":"49 Network Visualization in R","heading":"49.7.3 Centrality Measurement","text":"Degree centrality deﬁned number adjacent edges node. ranking degree centrality, find LUKE greatest value. implies LUKE interacting great amount unique characters. color node based degree centrality value. node greatest value warmest color.Betweenness centrality deﬁned number shortest paths nodes pass particular node. ranking betweenness centrality, find LEIA greatest value. implies LEIA tends critical communication process. color node based betweenness centrality value. node greatest value warmest color.Closeness centrality deﬁned number steps required access every node given node. ranking closeness centrality, find BIGGS greatest value. implies BIGGS close many characters. color node based closeness centrality value. node greatest value warmest color.Lastly, output network find discrepancies among three measurements.","code":"\ndegree_centrality <- degree(g)\nsw_nodes$degree_centrality <- degree_centrality[as.character(sw_nodes$name)]\nhead(sort(degree_centrality, decreasing=TRUE))##      LUKE      LEIA     C-3PO CHEWBACCA       HAN     R2-D2 \n##        15        12        10         8         8         7\nsw_colors_centrality <- rev(colorRampPalette(brewer.pal(9, \"Oranges\"))(22))\nsw_nodes <- sw_nodes %>% mutate(degree_rank=23-floor(rank(degree_centrality)),\n                                color.background=sw_colors_centrality[degree_rank])\nnetwork_degree <- visNetwork(sw_nodes, sw_edges, height='350px', width=\"100%\", main=\"Degree Centrality\") %>%\n  visLayout(randomSeed=21) %>% \n  visOptions(highlightNearest=TRUE,\n             nodesIdSelection=TRUE,\n             selectedBy=\"degree_rank\") %>% \n  visInteraction(hideEdgesOnDrag=TRUE,\n                 dragNodes=TRUE,\n                 dragView=TRUE,\n                 zoomView=TRUE,\n                 navigationButtons=TRUE)\nbetweenness_centrality <- betweenness(g)\nsw_nodes$betweenness_centrality <- betweenness_centrality[as.character(sw_nodes$name)]\nhead(sort(betweenness_centrality, decreasing=TRUE))##       LEIA    DODONNA        HAN      C-3PO      BIGGS RED LEADER \n##   59.95000   47.53333   37.00000   32.78333   31.91667   31.41667\ncloseness_centrality <- closeness(g, normalized=TRUE)\nsw_nodes$closeness_centrality <- closeness_centrality[as.character(sw_nodes$name)]\nhead(sort(closeness_centrality, decreasing=TRUE))##       BIGGS  RED LEADER     DODONNA GOLD LEADER        LEIA       C-3PO \n##   0.2625000   0.2625000   0.2560976   0.2560976   0.2530120   0.2441860"},{"path":"network-visualization-in-r.html","id":"external-resource","chapter":"49 Network Visualization in R","heading":"49.8 External Resource","text":"visNetwork package;star-wars-network.","code":""},{"path":"dplyr-tutorial-in-r.html","id":"dplyr-tutorial-in-r","chapter":"50 Dplyr tutorial in R","heading":"50 Dplyr tutorial in R","text":"Pinyi Yang","code":"\n#install.packages(\"dplyr\")\n#install.packages(\"ggplot2\")\n#install.packages(\"stringr\")\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)"},{"path":"dplyr-tutorial-in-r.html","id":"environment-setup","chapter":"50 Dplyr tutorial in R","heading":"50.1 Environment Setup","text":"order use dyplr package, need first install library beginning.\nBesides dplyr package, use two packages better demonstration functions dplyr. Related documentations two accessory packages provided :ggplot2: https://ggplot2.tidyverse.org/reference/ggplot2: https://ggplot2.tidyverse.org/reference/stringr: https://www.rdocumentation.org/packages/stringr/versions/1.4.0stringr: https://www.rdocumentation.org/packages/stringr/versions/1.4.0","code":""},{"path":"dplyr-tutorial-in-r.html","id":"introduction-5","chapter":"50 Dplyr tutorial in R","heading":"50.2 Introduction","text":"Dplyr package tidyverse R. key role dplyr dataframe manipulation enables data management. Dplyr contain numerous useful functions allows user-friendly data query, data cleaning, data summary, etc. Dplyr especially important data visualization, , effective informative visualization requires structural integrity dataframe used well removal excessive redundant information keeping information needed.","code":""},{"path":"dplyr-tutorial-in-r.html","id":"data","chapter":"50 Dplyr tutorial in R","heading":"50.3 Data","text":"One widely used data tutorial use purpose R ‘diamond’ dataset. information ‘diamond’ dataset, please visit following link: https://www.rdocumentation.org/packages/ggplot2/versions/3.3.5/topics/diamonds.following section showcases attributes ‘diamond’ dataset assist understanding data manipulation using dplyr.","code":"\nstr(diamonds)## tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n##  $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n##  $ cut    : Ord.factor w/ 5 levels \"Fair\"<\"Good\"<..: 5 4 2 4 2 3 3 3 1 3 ...\n##  $ color  : Ord.factor w/ 7 levels \"D\"<\"E\"<\"F\"<\"G\"<..: 2 2 2 6 7 7 6 5 2 5 ...\n##  $ clarity: Ord.factor w/ 8 levels \"I1\"<\"SI2\"<\"SI1\"<..: 2 3 5 4 2 6 7 3 4 5 ...\n##  $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n##  $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n##  $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n##  $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n##  $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n##  $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\nhead(diamonds, n = 3)## # A tibble: 3 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31\nsummary(diamonds[,!(names(diamonds) %in% c(\"color\", \"clarity\", \"cut\"))])##      carat            depth           table           price      \n##  Min.   :0.2000   Min.   :43.00   Min.   :43.00   Min.   :  326  \n##  1st Qu.:0.4000   1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  950  \n##  Median :0.7000   Median :61.80   Median :57.00   Median : 2401  \n##  Mean   :0.7979   Mean   :61.75   Mean   :57.46   Mean   : 3933  \n##  3rd Qu.:1.0400   3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5324  \n##  Max.   :5.0100   Max.   :79.00   Max.   :95.00   Max.   :18823  \n##        x                y                z         \n##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  \n##  1st Qu.: 4.710   1st Qu.: 4.720   1st Qu.: 2.910  \n##  Median : 5.700   Median : 5.710   Median : 3.530  \n##  Mean   : 5.731   Mean   : 5.735   Mean   : 3.539  \n##  3rd Qu.: 6.540   3rd Qu.: 6.540   3rd Qu.: 4.040  \n##  Max.   :10.740   Max.   :58.900   Max.   :31.800"},{"path":"dplyr-tutorial-in-r.html","id":"column-and-row-manipulation","chapter":"50 Dplyr tutorial in R","heading":"50.4 Column and Row Manipulation","text":"data analysis, sometimes original data set contains much information, actually need analysis good structure us use. order make data looked structured, use Dplyr organize clean Data. Dplyr can effectively help us reorganize data set based criteria commanded. example, data set may contain observations (.e. rows) past 10 years, need analyze trend recent 5 years. case, need ignore data previous years order generate reasonable visualizations, thus filter function Dplyr come handy. Also, might times data contains numerous redundant parameters (.e. columns) analysis, need select variables important goal study. case, can use Dplyr’s select function choose desired columns.\n### Select\n‘select’ dplyr package used select certain columns data frame. can use ‘select’ function following syntax:\n\\[select(data, column\\ names/column\\ position , \\dots)\\]\n#### Column Selection\nfollowing example, selecting three columns observations : ‘carat’, ‘clarity’, ‘price’. Noted can select name position.","code":"\n#only keeping \"carat\", \"clarity\", and \"price\" columns\nselect_result <- select(diamonds, c(carat, clarity, price))\nhead(select_result,n = 3)## # A tibble: 3 × 3\n##   carat clarity price\n##   <dbl> <ord>   <int>\n## 1  0.23 SI2       326\n## 2  0.21 SI1       326\n## 3  0.23 VS1       327\n# same result but use position\nselect_result <- select(diamonds, c(1,4,7))\nhead(select_result,n = 3)## # A tibble: 3 × 3\n##   carat clarity price\n##   <dbl> <ord>   <int>\n## 1  0.23 SI2       326\n## 2  0.21 SI1       326\n## 3  0.23 VS1       327\n# or use both column name and position, not recommended\nselect_result <- select(diamonds, c(1,4, price))\nhead(select_result,n = 3)## # A tibble: 3 × 3\n##   carat clarity price\n##   <dbl> <ord>   <int>\n## 1  0.23 SI2       326\n## 2  0.21 SI1       326\n## 3  0.23 VS1       327"},{"path":"dplyr-tutorial-in-r.html","id":"column-deselection","chapter":"50 Dplyr tutorial in R","heading":"50.4.0.1 Column Deselection","text":"can also deselect certain columns, range columns using negate command \\(!\\). following example, want remove (.e. deselect) columns x z.","code":"\n#deselecting columns from x to z\nselect_result <- select(diamonds, !(x:z))\nhead(select_result,n = 3)## # A tibble: 3 × 7\n##   carat cut     color clarity depth table price\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int>\n## 1  0.23 Ideal   E     SI2      61.5    55   326\n## 2  0.21 Premium E     SI1      59.8    61   326\n## 3  0.23 Good    E     VS1      56.9    65   327"},{"path":"dplyr-tutorial-in-r.html","id":"helper-functions","chapter":"50 Dplyr tutorial in R","heading":"50.4.0.2 Helper Functions","text":"several helper functions selection deal specific cases used select function:\\(contains()\\): Use select column contains specific string\\(contains()\\): Use select column contains specific string\\(starts\\_with()\\): Use select columns start given string\\(starts\\_with()\\): Use select columns start given string\\(ends\\_with()\\): Use select columns end given string\\(ends\\_with()\\): Use select columns end given string\\(matches()\\): Use select columns match string (often use regular expression)\\(matches()\\): Use select columns match string (often use regular expression)\\(everything()\\): Select columns\\(everything()\\): Select columns","code":""},{"path":"dplyr-tutorial-in-r.html","id":"the-matches-function","chapter":"50 Dplyr tutorial in R","heading":"50.4.0.3 The matches() Function","text":"introduced , \\(matches()\\) function can select columns names match regular expression (regex). Regular expression way represent certain string patterns, extensively used natural language processing, linguistic analysis, etc. details regular expression, grammar, application, please visit following link: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions/Cheatsheet.example, want keep columns names start c, can use regular expression “^c” \\(matches()\\) function. (p.s. “^c” regular expression defines strings start letter lower-case c)Notice columns “carat”, “cut”, “color”, “clarity” kept selection, satisfy pattern first letter string letter “c”.","code":"\nhead(select(diamonds, matches(\"^c\")),n = 3)## # A tibble: 3 × 4\n##   carat cut     color clarity\n##   <dbl> <ord>   <ord> <ord>  \n## 1  0.23 Ideal   E     SI2    \n## 2  0.21 Premium E     SI1    \n## 3  0.23 Good    E     VS1"},{"path":"dplyr-tutorial-in-r.html","id":"the-contains-function","chapter":"50 Dplyr tutorial in R","heading":"50.4.0.4 The contains() Function","text":"sake demonstration, create pseudo-dataset names specifically designed include certain strings. case, multiple column names include string “length”Now, can use \\(contains()\\) function select names contain string “length”.","code":"\ncolors <- c(\"red\", \"blue\", \"yellow\", \"green\")\npseudo_car_data <- data.frame(obs = 1:10,\n                              cabin_length = runif(10, 5, 7.5),\n                              door_length = runif(10, 1, 3),\n                              wind_shield_length = runif(10, 4, 6),\n                              color = sample(colors, 10, replace = T))\nhead(pseudo_car_data,,n = 3)##   obs cabin_length door_length wind_shield_length  color\n## 1   1     5.632943    2.321677           5.253965 yellow\n## 2   2     6.418753    2.584403           5.945934 yellow\n## 3   3     6.058065    2.688777           4.891338   blue\nhead(select(pseudo_car_data, contains(\"length\")),n = 3)##   cabin_length door_length wind_shield_length\n## 1     5.632943    2.321677           5.253965\n## 2     6.418753    2.584403           5.945934\n## 3     6.058065    2.688777           4.891338"},{"path":"dplyr-tutorial-in-r.html","id":"filter-2","chapter":"50 Dplyr tutorial in R","heading":"50.4.1 Filter","text":"Similar select function discussed last section, filter function dplyr package also used selection existing data frame. Filter function used select rows satisfies specific conditions/criteria.second argument must logical value. Logical operation useful . example, ‘.na’ function return TRUE value NA FALSE . can also use \\(\\&\\) \\(|\\) operators combine multiple logical statements.syntax filter function follow:\n\\[filter(data, condition, \\dots)\\]","code":""},{"path":"dplyr-tutorial-in-r.html","id":"filter-numerical-values","chapter":"50 Dplyr tutorial in R","heading":"50.4.1.1 Filter Numerical Values","text":"filter function can used filter numerical character type data. First, lets use price parameter demonstrate filter numerical values. filter observations diamond price 500. words, keep rows observations \\(price \\leq 500\\).","code":"\n#the range of prices for diamonds\nrange(diamonds$price)## [1]   326 18823\n#filter observations that have price less than or equal to 500\nfilter_result <- filter(diamonds, price <= 500)\nhead(filter_result,n = 3)## # A tibble: 3 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31"},{"path":"dplyr-tutorial-in-r.html","id":"filter-character-values","chapter":"50 Dplyr tutorial in R","heading":"50.4.1.2 Filter Character Values","text":"Now, can filter character type variables. example, can choose observations “Fair” “Good” cuts:checking counts category, can see filtering dataframe, observations cut “Good” “Premium” “Ideal” filtered , leaving “Fair” “Good” observations. Note table filtration still contains three categories. ‘cut’ variable stored factor dataframe. Thus, filtration eliminate original levels.","code":"\n#check levels of cuts\nlevels(diamonds$cut)## [1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"\ntable(diamonds$cut)## \n##      Fair      Good Very Good   Premium     Ideal \n##      1610      4906     12082     13791     21551\n#filter by cut, keeping Fair and Good\nfilter_result <- filter(diamonds, cut %in% c(\"Fair\", \"Good\"))\nhead(filter_result,n = 3)## # A tibble: 3 × 10\n##   carat cut   color clarity depth table price     x     y     z\n##   <dbl> <ord> <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n## 2  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n## 3  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\ntable(filter_result$cut)## \n##      Fair      Good Very Good   Premium     Ideal \n##      1610      4906         0         0         0"},{"path":"dplyr-tutorial-in-r.html","id":"slice","chapter":"50 Dplyr tutorial in R","heading":"50.4.2 Slice","text":"slice function dplyr also useful selecting rows based position order. use similar head function base R select top tail n rows data set.syntax slice function follow:\n\\[slice(data,  number\\ \\ \\ sliced,... )\\]also wrapper functions ‘slice’ can used get pieces data specified number/range rows also filtering conditions, slice_max,slice_min. syntax slice function follow:\n\\[slice\\_max(data, order\\_by = column \\ name, n =  number\\ \\ \\ sliced )\\]","code":"\n#only get the first five observation of the data frame \nslice_result <- slice(diamonds, 1:5)\nslice_result ## # A tibble: 5 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31\n## 4  0.29 Premium I     VS2      62.4    58   334  4.2   4.23  2.63\n## 5  0.31 Good    J     SI2      63.3    58   335  4.34  4.35  2.75\n# only get the first five observation of the data frame with highest price\nslice_result <- slice_max(diamonds, order_by = price, n = 5)\nslice_result## # A tibble: 5 × 10\n##   carat cut       color clarity depth table price     x     y     z\n##   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n## 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n## 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n## 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n## 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01"},{"path":"dplyr-tutorial-in-r.html","id":"order-manipulation","chapter":"50 Dplyr tutorial in R","heading":"50.5 Order Manipulation","text":"Sometimes, want get direct visualization data ranking observations using one multiple parameters.\n### Ascending Order\ncan use ‘arrange’ function arrange data, based one parameter multiple parameters, ascending order, using following syntax:\n\\[arrange(data, column\\ \\ arrangement, \\dots)\\]","code":"\nhead(arrange(diamonds, price),n = 3)## # A tibble: 3 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31"},{"path":"dplyr-tutorial-in-r.html","id":"descending-order","chapter":"50 Dplyr tutorial in R","heading":"50.5.1 Descending Order","text":"can use ‘desc’ function arrange data, based one multiple parameters, descending order, using following syntax:\n\\[arrange(data, desc(column\\ \\ arrangement))\\]","code":"\nhead(arrange(diamonds, desc(price)),n = 3)## # A tibble: 3 × 10\n##   carat cut       color clarity depth table price     x     y     z\n##   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n## 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n## 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56"},{"path":"dplyr-tutorial-in-r.html","id":"entry-manipulation","chapter":"50 Dplyr tutorial in R","heading":"50.6 Entry Manipulation","text":"real life data processing, sometimes, need gather data order reduce dimension. example, want show mean number products sold across months data shows daily sales. case, need gather observations combining observations within month calculate cumulative mean generate desirable visualizations. grouping function summary function dplyr comes handy.\n### Grouping\nGrouping function dplyr groups data based categories assigned column(s). general syntax :\n\\[group\\_by(data, column\\ used\\ \\ grouping, \\dots)\\]Notice dataframe grouping looks exactly ! ‘group_by’ function change dataframe , rather, prepares dataframe data processing. can see next section, can summarize data grouping.","code":"\nhead(group_by(diamonds, cut),n = 3)## # A tibble: 3 × 10\n## # Groups:   cut [3]\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31\nhead(diamonds,n = 3)## # A tibble: 3 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31"},{"path":"dplyr-tutorial-in-r.html","id":"summary-statistics","chapter":"50 Dplyr tutorial in R","heading":"50.6.1 Summary Statistics","text":"Often times, want reduce complexity data creating summary statistics, mean, median, max, etc. ‘summarise’ function dplyr just job. grouping categories certain column(s), can employ ‘summarise’ function create new column(s) summary statistics, following syntax:\n\\[summarise(grouped\\_data, new\\_column\\_name = function(column\\ \\ \\ summarized), \\dots)\\]example, trying get mean price based cut diamond type.","code":"\nsummarise(group_by(diamonds, cut), mean_price = mean(price))## # A tibble: 5 × 2\n##   cut       mean_price\n##   <ord>          <dbl>\n## 1 Fair           4359.\n## 2 Good           3929.\n## 3 Very Good      3982.\n## 4 Premium        4584.\n## 5 Ideal          3458."},{"path":"dplyr-tutorial-in-r.html","id":"helper-function-for-summarise","chapter":"50 Dplyr tutorial in R","heading":"50.6.1.1 Helper function for summarise()","text":"several helper functions can efficiently find summary statistics within ‘summarise’ function.$mean(), median() $: Center data$mean(), median() $: Center data$sd() $: Spread data$sd() $: Spread data$min(), max() $: Range data$min(), max() $: Range data","code":""},{"path":"dplyr-tutorial-in-r.html","id":"mutation","chapter":"50 Dplyr tutorial in R","heading":"50.6.2 Mutation","text":"‘mutate’ function extremely useful data manipulation allows greater freedom transforming existing dataframe, also application missing value imputation. syntax mutate function :\n\\[mutate(data, new\\_column\\_name = operation, \\dots)\\]","code":""},{"path":"dplyr-tutorial-in-r.html","id":"transforming-dataframe","chapter":"50 Dplyr tutorial in R","heading":"50.6.2.1 Transforming Dataframe","text":"can add new parameter performing operations existing parameters. following example, calculate price per carat diamond diamond listed dataset. can use formula \\(\\frac{price}{carat}\\) row observation.Notice mutated dataframe one column end titled “price_per_carat”. new column calculated dividing price carat within observation.","code":"\nhead(mutate(diamonds, price_per_carat = price/carat),n = 3)## # A tibble: 3 × 11\n##   carat cut    color clarity depth table price     x     y     z price_per_carat\n##   <dbl> <ord>  <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>           <dbl>\n## 1  0.23 Ideal  E     SI2      61.5    55   326  3.95  3.98  2.43           1417.\n## 2  0.21 Premi… E     SI1      59.8    61   326  3.89  3.84  2.31           1552.\n## 3  0.23 Good   E     VS1      56.9    65   327  4.05  4.07  2.31           1422."},{"path":"dplyr-tutorial-in-r.html","id":"imputation-using-mean","chapter":"50 Dplyr tutorial in R","heading":"50.6.2.2 Imputation – Using Mean","text":"sake demonstration, create pseudo-dataset contains missing values showcase imputation power mutate function.example, can see Lexa Garcia’s ages missing. use mutate function impute missing values using mean rest ages (.e. age Tom, Jay, Marry). \\(.na()\\) function used check entry contain NA values.","code":"\n#create new data\npseudo_missing_data <- data.frame(name = c(\"Tom\", \"Jay\", \"Marry\", \"Lexa\", \"Garcia\"),\n                          age = c(sample(40:80, 3, replace = F), NA, NA))\npseudo_missing_data##     name age\n## 1    Tom  47\n## 2    Jay  45\n## 3  Marry  41\n## 4   Lexa  NA\n## 5 Garcia  NA\nimputed_data <- mutate(pseudo_missing_data,\n                       age = ifelse(is.na(age), mean(age, na.rm = T), age))\nimputed_data##     name      age\n## 1    Tom 47.00000\n## 2    Jay 45.00000\n## 3  Marry 41.00000\n## 4   Lexa 44.33333\n## 5 Garcia 44.33333"},{"path":"dplyr-tutorial-in-r.html","id":"imputation-removing-observations","chapter":"50 Dplyr tutorial in R","heading":"50.6.2.3 Imputation – Removing Observations","text":"Also, brutal way cleaning data – removing observations NA values (.e. removing entire row row contain NA value). However, type cleaning technique generally recommended large quantity missing values, severely depricate integrity original dataset lose important information. general way removing observations NA values calling ‘na.omit’ function:can see latter two rows (.e. observations Lexa Garcia) completely removed!","code":"\nna.omit(pseudo_missing_data)##    name age\n## 1   Tom  47\n## 2   Jay  45\n## 3 Marry  41"},{"path":"dplyr-tutorial-in-r.html","id":"useful-helper-functions-for-mutate","chapter":"50 Dplyr tutorial in R","heading":"50.6.2.4 Useful Helper Functions for mutate()","text":"multiple functions can embedded ‘mutate’ function helps perform complex tasks otherwise requires us define functions :\\(pmin(), pmax()\\): Element-wise min max\\(pmin(), pmax()\\): Element-wise min max\\(cummin(), cummax()\\): Cumulative min max\\(cummin(), cummax()\\): Cumulative min max\\(()\\): value range\\(()\\): value range\\(cumsum(), cumprod()\\): Cumulative sum cumulative product\\(cumsum(), cumprod()\\): Cumulative sum cumulative product\\(cummean()\\): Cumulative mean\\(cummean()\\): Cumulative meanexample:","code":"\nhelper_example <- mutate(diamonds,\n                         #use cummin to find the minimum value up to the current observation\n                         cummin_price = cummin(price),\n                         #use between to determine if the price is in range\n                         in_range = between(price, 330, 340))\nhead(helper_example)[,c(\"price\", \"cummin_price\", \"in_range\")]## # A tibble: 6 × 3\n##   price cummin_price in_range\n##   <int>        <int> <lgl>   \n## 1   326          326 FALSE   \n## 2   326          326 FALSE   \n## 3   327          326 FALSE   \n## 4   334          326 TRUE    \n## 5   335          326 TRUE    \n## 6   336          326 TRUE"},{"path":"dplyr-tutorial-in-r.html","id":"distinct-1","chapter":"50 Dplyr tutorial in R","heading":"50.6.3 Distinct","text":"‘distinct’ function used check duplicates data set. wouldn’t want data excessive duplicated observations. distinct function can help us delete replicated rows simplify dataframe.\n\\[distinct(data, \\dots)\\]","code":"\nrepeated_diamonds <- slice(diamonds,1:3,1:2)\nrepeated_diamonds ## # A tibble: 5 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31\n## 4  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 5  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\ndistinct(repeated_diamonds )## # A tibble: 3 × 10\n##   carat cut     color clarity depth table price     x     y     z\n##   <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07  2.31"},{"path":"dplyr-tutorial-in-r.html","id":"piping---tying-everything-together","chapter":"50 Dplyr tutorial in R","heading":"50.7 Piping - Tying Everything Together","text":"Piping effective way construct data processing pipelines, especially process involves multiple manipulation steps. grammar used pipe commands ‘%>%’. direction arrow pointing towards way data processing proceeds.example, want first filter observations ‘Good’ cut; keep parameters numerical (.e. ‘carat’, ‘price’, ‘depth’, ‘table’, ‘x’, ‘y’, ‘z’), well ‘clarity’; , summarize data taking mean price grouping ‘clarity’. careful using piping ggplot2. know ggplot2 different layers graphical components connected \\(+\\) sign. Even though piping operator \\(%>%\\) function plus sign, used different conditions. Don’t mix !","code":"\n#pass down to the next manipulation command using piping\ndiamonds %>%\n  #filter only those observations with Good cuts\n  filter(cut == \"Good\") %>%\n  #select all columns except for cut and color\n  select(-c(cut, color)) %>%\n  #summarize the price when grouping by clarity\n  group_by(clarity) %>%\n  summarise(mean_price = mean(price)) %>%\n  #order the mean prices in descending order of clarity\n  arrange(desc(mean_price))## # A tibble: 8 × 2\n##   clarity mean_price\n##   <ord>        <dbl>\n## 1 SI2          4580.\n## 2 VS2          4262.\n## 3 IF           4098.\n## 4 VS1          3801.\n## 5 SI1          3690.\n## 6 I1           3597.\n## 7 VVS2         3079.\n## 8 VVS1         2255."},{"path":"dplyr-tutorial-in-r.html","id":"using-dplyr-with-ggplot","chapter":"50 Dplyr tutorial in R","heading":"50.8 Using Dplyr with GGplot","text":"essence dplyr clean data used perform model construction data visualization. section, use simple data visualization example combines dplyr ggplot2 shows can tie functions introduced graphic design using ggplot2.","code":"\ndiamonds %>%\n  #data manipulation using dplyr\n  filter(cut %in% c(\"Fair\", \"Good\", \"Very Good\")) %>%\n  select(cut, carat, color, price) %>%\n  group_by(cut, color) %>%\n  summarise(mean_price = mean(price),\n            mean_carat = mean(carat)) %>%\n  #data visualization using ggplot2\n  ggplot(mapping = aes(x = mean_carat, y = mean_price)) +\n  geom_point(mapping = aes(color = color)) +\n  ylab(\"Mean Price\") + xlab(\"Mean Carat\") +\n  labs(title = \"Mean Price vs. Mean Carat of Diamonds\") +\n  theme(plot.title = element_text(hjust = 0.5))"},{"path":"downloading-files.html","id":"downloading-files","chapter":"51 Downloading files","heading":"51 Downloading files","text":"Aaron Aknin Ashkan BozorgzadSo Section use r download files. important use right method importing dataset order optimize use.","code":""},{"path":"downloading-files.html","id":"getset-your-working-directory","chapter":"51 Downloading files","heading":"51.1 Get/set your working directory","text":"basic component working data knowing working directoryThe two main commands getwd() setwd().aware relative versus absolute paths","code":""},{"path":"downloading-files.html","id":"checking-for-and-creating-directories","chapter":"51 Downloading files","heading":"51.2 Checking for and creating directories","text":"file.exists (\"directoryName\") check see directory exists dir.create (\"directoryName\") create directory doesn’t exist. example checking “data” directory creating doesn’t exist.","code":"\nif (!file.exists(\"data\")){\n        dir.create(\"data\")\n        }"},{"path":"downloading-files.html","id":"download-a-file-from-the-web","chapter":"51 Downloading files","heading":"51.3 Download a file from the web","text":"Download file internetEven hand, helps reproducibilityImportant parameters url, destfile, methodUseful downloading tab-delimited, csv, filesThe method ’re going use curl. necessary website https, secure website, ’re Mac, need specify method curl order work. list files data directory, see now one file directory, ’s cameras.csv file.important component downloading files internet files might change. want keep track date downloaded data, can date function. just assign dateDownloaded command date give date data downloaded.","code":"\nlist.files( \"./data\" )## [1] \"output.csv\"\ndateDownloaded <- date()\ndateDownloaded## [1] \"Tue Dec 28 04:24:37 2021\""},{"path":"downloading-files.html","id":"reading-local-files","chapter":"51 Downloading files","heading":"51.4 Reading Local Files","text":"Loading flat files - read.table().main function reading data R.Flexible robust requires parameters.Reads data RAM big data can cause problems.can also use read.csv case csv file, automatically sets sep equal quote comma automatically sets header = TRUE.","code":"\ndf <- read.table( \"resources/downloading_files/output.csv\" , sep = \",\",header= TRUE )\nhead(df)##   year industry_code_ANZSIC              industry_name_ANZSIC rme_size_grp\n## 1 2011                    A Agriculture, Forestry and Fishing          a_0\n## 2 2011                    A Agriculture, Forestry and Fishing          a_0\n## 3 2011                    A Agriculture, Forestry and Fishing          a_0\n## 4 2011                    A Agriculture, Forestry and Fishing          a_0\n## 5 2011                    A Agriculture, Forestry and Fishing          a_0\n## 6 2011                    A Agriculture, Forestry and Fishing          a_0\n##                                          variable value              unit\n## 1                                   Activity unit 46134             COUNT\n## 2                          Rolling mean employees     0             COUNT\n## 3                         Salaries and wages paid   279 DOLLARS(millions)\n## 4 Sales, government funding, grants and subsidies  8187 DOLLARS(millions)\n## 5                                    Total income  8866 DOLLARS(millions)\n## 6                               Total expenditure  7618 DOLLARS(millions)\ndf <- read.csv( \"resources/downloading_files/output.csv\")\nhead(df)##   year industry_code_ANZSIC              industry_name_ANZSIC rme_size_grp\n## 1 2011                    A Agriculture, Forestry and Fishing          a_0\n## 2 2011                    A Agriculture, Forestry and Fishing          a_0\n## 3 2011                    A Agriculture, Forestry and Fishing          a_0\n## 4 2011                    A Agriculture, Forestry and Fishing          a_0\n## 5 2011                    A Agriculture, Forestry and Fishing          a_0\n## 6 2011                    A Agriculture, Forestry and Fishing          a_0\n##                                          variable value              unit\n## 1                                   Activity unit 46134             COUNT\n## 2                          Rolling mean employees     0             COUNT\n## 3                         Salaries and wages paid   279 DOLLARS(millions)\n## 4 Sales, government funding, grants and subsidies  8187 DOLLARS(millions)\n## 5                                    Total income  8866 DOLLARS(millions)\n## 6                               Total expenditure  7618 DOLLARS(millions)"},{"path":"downloading-files.html","id":"some-more-important-parameters","chapter":"51 Downloading files","heading":"51.4.1 Some more important parameters","text":"quote - can tell R whether quoted values quote=\"\" means quotes.na.strings - set character represents missing value.nrows - many rows read file (e.g. nrows=10 reads 10 lines).skip- number lines skip starting read","code":""},{"path":"downloading-files.html","id":"reading-xml","chapter":"51 Downloading files","heading":"51.5 Reading XML","text":"Extensible markup languageFrequently used store structured dataParticularly widely used internet applicationsExtracting XML basis web scrapingComponents\nMarkup - labels give text structure\nContent - actual text document\nMarkup - labels give text structureContent - actual text document","code":"\nfileURL<-\"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml\"\ndoc <- xmlTreeParse(sub(\"s\", \"\", fileURL), useInternal = TRUE)\nrootNode <- xmlRoot(doc)\n\nxmlName(rootNode)## [1] \"response\"\nrootNode[[1]][[1]]## <row _id=\"1\" _uuid=\"93CACF6F-C8C2-4B87-95A8-8177806D5A6F\" _position=\"1\" _address=\"http://data.baltimorecity.gov/resource/k5ry-ef3g/1\">\n##   <name>410<\/name>\n##   <zipcode>21206<\/zipcode>\n##   <neighborhood>Frankford<\/neighborhood>\n##   <councildistrict>2<\/councildistrict>\n##   <policedistrict>NORTHEASTERN<\/policedistrict>\n##   <location_1 human_address=\"{&quot;address&quot;:&quot;4509 BELAIR ROAD&quot;,&quot;city&quot;:&quot;Baltimore&quot;,&quot;state&quot;:&quot;MD&quot;,&quot;zip&quot;:&quot;&quot;}\" needs_recoding=\"true\"/>\n## <\/row>"},{"path":"downloading-files.html","id":"reading-json","chapter":"51 Downloading files","heading":"51.6 Reading JSON","text":"Javascript Object NotationLightweight data storageCommon format data application programming interfaces (APIs)Similar structure XML different syntax/formatData stored \nNumbers (double)\nStrings (double quoted)\nBoolean (true false)\nArray (ordered, comma separated enclosed square brackets [])\nObject (unorderd, comma separated collection key:value pairs curley brackets {})\nNumbers (double)Strings (double quoted)Boolean (true false)Array (ordered, comma separated enclosed square brackets [])Object (unorderd, comma separated collection key:value pairs curley brackets {})httr package allows connect websites connect APIs. allows, GET request made (request access result search), retrieve result form text reworked. general, call API via httr made follows:","code":"\n# extracting data from the website\njsonData <- fromJSON(\"https://data.ny.gov/api/views/9a8c-vfzj/rows.json?accessType=DOWNLOAD\")\n# extract the data node\nfood_market <- jsonData[['data']]\n# assembling the data into data frames\n\nfood_market[1,19]## [1] \"GREENVILLE\"\nurl <- \"https://world.openfoodfacts.org/api/v0/product/3017620425400.json\"\n\nresults <- \n  httr::content(\n    httr::GET(url),             # url to query\n    as=\"text\",                  # type of the output\n    httr::content_type_json(),  # type of the answer\n    encoding= \"UTF-8\"           # encoding of the answer\n  )\n\njsonData <-fromJSON(results)"},{"path":"downloading-files.html","id":"webscraping","chapter":"51 Downloading files","heading":"51.7 Webscraping","text":"can great way get dataMany websites information may want programaticaly readIn cases terms service websiteAttempting read many pages quickly can get IP address blockedIn order retrieve data web page can use rvest package.order begin parsing web page, must first request data computer server contains . rvest, function read_html() function. tutorial use weather website, National Weather Service website.\nHigh: 63 °F\nhard read, One way deal , ’ve seen use XML package. , use URL. Use XML package, parse HTML , using InternalNodes get complete structure .","code":"\npage <- read_html(\"https://forecast.weather.gov/MapClick.php?lat=37.7771&lon=-122.4196#.Xl0j6BNKhTY\")\npage## {html_document}\n## <html class=\"no-js\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body>\\n        <main class=\"container\"><header class=\"row clearfix\" id=\" ...\nresults = page  %>% html_nodes(\".temp\") %>% html_text()\nresults## [1] \"Low: 43 °F\"  \"High: 49 °F\" \"Low: 44 °F\"  \"High: 50 °F\" \"Low: 40 °F\" \n## [6] \"High: 52 °F\" \"Low: 41 °F\"  \"High: 52 °F\" \"Low: 39 °F\""},{"path":"downloading-files.html","id":"dplyr-pachage","chapter":"51 Downloading files","heading":"51.8 dplyr Pachage","text":"section dplyr package R, package specifically designed help work data frames. lecture just cover basics introduce package ’ll talk verbs arrange, filter, select, mutate rename.","code":""},{"path":"downloading-files.html","id":"load-the-dplyr-package","chapter":"51 Downloading files","heading":"51.8.1 Load the dplyr package","text":"dataset going work data set air pollution weather variables city Chicago years 1987 2005, ’s kind daily data.","code":"\nchicago <- readRDS(\"resources/downloading_files/chicago.rds\")\ndim(chicago)## [1] 6940    8\nstr(chicago)## 'data.frame':    6940 obs. of  8 variables:\n##  $ city      : chr  \"chic\" \"chic\" \"chic\" \"chic\" ...\n##  $ tmpd      : num  31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n##  $ dptp      : num  31.5 29.9 27.4 28.6 28.9 ...\n##  $ date      : Date, format: \"1987-01-01\" \"1987-01-02\" ...\n##  $ pm25tmean2: num  NA NA NA NA NA NA NA NA NA NA ...\n##  $ pm10tmean2: num  34 NA 34.2 47 NA ...\n##  $ o3tmean2  : num  4.25 3.3 3.33 4.38 4.75 ...\n##  $ no2tmean2 : num  20 23.2 23.8 30.4 30.3 ..."},{"path":"downloading-files.html","id":"select-2","chapter":"51 Downloading files","heading":"51.8.2 Select","text":"Show dataframe first 5 columnsif want look columns starting city ending D DPTP, dew point column. , want include columns .can use minus sign say, want look columns except . columns indicated range. can use select function just say minus city colon dew point sequence. ’ll get columns, except columns.equivalent code kind regular R, without using deplyr package little bit tricky","code":"\nhead(select(chicago, 1:5))##   city tmpd   dptp       date pm25tmean2\n## 1 chic 31.5 31.500 1987-01-01         NA\n## 2 chic 33.0 29.875 1987-01-02         NA\n## 3 chic 33.0 27.375 1987-01-03         NA\n## 4 chic 29.0 28.625 1987-01-04         NA\n## 5 chic 32.0 28.875 1987-01-05         NA\n## 6 chic 40.0 35.125 1987-01-06         NA\nnames(chicago)[1:3]## [1] \"city\" \"tmpd\" \"dptp\"\nhead(select(chicago, city:dptp))##   city tmpd   dptp\n## 1 chic 31.5 31.500\n## 2 chic 33.0 29.875\n## 3 chic 33.0 27.375\n## 4 chic 29.0 28.625\n## 5 chic 32.0 28.875\n## 6 chic 40.0 35.125\nhead(select(chicago, -(city:dptp)))##         date pm25tmean2 pm10tmean2 o3tmean2 no2tmean2\n## 1 1987-01-01         NA   34.00000 4.250000  19.98810\n## 2 1987-01-02         NA         NA 3.304348  23.19099\n## 3 1987-01-03         NA   34.16667 3.333333  23.81548\n## 4 1987-01-04         NA   47.00000 4.375000  30.43452\n## 5 1987-01-05         NA         NA 4.750000  30.33333\n## 6 1987-01-06         NA   48.00000 5.833333  25.77233\ni <- match(\"city\", names(chicago))\nj <- match(\"dptp\", names(chicago))\nhead(chicago[, -(i:j)])##         date pm25tmean2 pm10tmean2 o3tmean2 no2tmean2\n## 1 1987-01-01         NA   34.00000 4.250000  19.98810\n## 2 1987-01-02         NA         NA 3.304348  23.19099\n## 3 1987-01-03         NA   34.16667 3.333333  23.81548\n## 4 1987-01-04         NA   47.00000 4.375000  30.43452\n## 5 1987-01-05         NA         NA 4.750000  30.33333\n## 6 1987-01-06         NA   48.00000 5.833333  25.77233"},{"path":"downloading-files.html","id":"verbs","chapter":"51 Downloading files","heading":"51.8.3 Verbs","text":"","code":""},{"path":"downloading-files.html","id":"filter-3","chapter":"51 Downloading files","heading":"51.8.3.1 filter","text":"filter function next function deplyr ’ll talk ’s basically used subset rows based conditions. , example, might want take rows Chicago data set pm 2.5 greater 30you don’t just subset rows based values one column. can take multiple columns create complicated logical sequence. looking pm2.5 greater 30. well temperature greater 80.","code":"\nchic.f <- filter(chicago, pm25tmean2 > 30)\nhead(select(chic.f, 1:3, pm25tmean2), 10)##    city tmpd dptp pm25tmean2\n## 1  chic   23 21.9      38.10\n## 2  chic   28 25.8      33.95\n## 3  chic   55 51.3      39.40\n## 4  chic   59 53.7      35.40\n## 5  chic   57 52.0      33.30\n## 6  chic   57 56.0      32.10\n## 7  chic   75 65.8      56.50\n## 8  chic   61 59.0      33.80\n## 9  chic   73 60.3      30.30\n## 10 chic   78 67.1      41.40\nchic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)\nhead(select(chic.f, 1:3, pm25tmean2, tmpd), 10)##    city tmpd dptp pm25tmean2\n## 1  chic   81 71.2    39.6000\n## 2  chic   81 70.4    31.5000\n## 3  chic   82 72.2    32.3000\n## 4  chic   84 72.9    43.7000\n## 5  chic   85 72.6    38.8375\n## 6  chic   84 72.6    38.2000\n## 7  chic   82 67.4    33.0000\n## 8  chic   82 63.5    42.5000\n## 9  chic   81 70.4    33.1000\n## 10 chic   82 66.2    38.8500"},{"path":"downloading-files.html","id":"arrange-2","chapter":"51 Downloading files","heading":"51.8.3.2 arrange","text":"next function range simple purpose. ’s basically used reorder rows data frame based values column.use tail function look last couple rowsTo arrange rows descending order, desc function can useduse tail function look last couple rows","code":"\nchicago <- arrange(chicago, date)\nhead(select(chicago, date, pm25tmean2), 3)##         date pm25tmean2\n## 1 1987-01-01         NA\n## 2 1987-01-02         NA\n## 3 1987-01-03         NA\ntail(select(chicago, date, pm25tmean2), 3)##            date pm25tmean2\n## 6938 2005-12-29    7.45000\n## 6939 2005-12-30   15.05714\n## 6940 2005-12-31   15.00000\nchicago <- arrange(chicago, desc(date))\nhead(select(chicago, date, pm25tmean2), 3)##         date pm25tmean2\n## 1 2005-12-31   15.00000\n## 2 2005-12-30   15.05714\n## 3 2005-12-29    7.45000\ntail(select(chicago, date, pm25tmean2), 3)##            date pm25tmean2\n## 6938 1987-01-03         NA\n## 6939 1987-01-02         NA\n## 6940 1987-01-01         NA"},{"path":"downloading-files.html","id":"rename-2","chapter":"51 Downloading files","heading":"51.8.3.3 rename","text":"rename function simple. can used rename variable R.","code":"\nhead(chicago[, 1:5], 3)##   city tmpd dptp       date pm25tmean2\n## 1 chic   35 30.1 2005-12-31   15.00000\n## 2 chic   36 31.0 2005-12-30   15.05714\n## 3 chic   35 29.4 2005-12-29    7.45000\nchicago <- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nhead(chicago[, 1:5], 3)##   city tmpd dewpoint       date     pm25\n## 1 chic   35     30.1 2005-12-31 15.00000\n## 2 chic   36     31.0 2005-12-30 15.05714\n## 3 chic   35     29.4 2005-12-29  7.45000"},{"path":"downloading-files.html","id":"mutate-2","chapter":"51 Downloading files","heading":"51.8.3.4 mutate","text":"mutate function used simply transform existing variables create new variables. example want create new variable called pm25detrend. pm25 variable mean subtracted .","code":"\nchicago <- mutate(chicago, pm25detrend=pm25-mean(pm25, na.rm=TRUE))\nhead(select(chicago, pm25, pm25detrend))##       pm25 pm25detrend\n## 1 15.00000   -1.230958\n## 2 15.05714   -1.173815\n## 3  7.45000   -8.780958\n## 4 17.75000    1.519042\n## 5 23.56000    7.329042\n## 6  8.40000   -7.830958"},{"path":"downloading-files.html","id":"group_by-1","chapter":"51 Downloading files","heading":"51.8.3.5 group_by","text":"Finally group function allows split data frame according Certain categorical variables. example, going create temperature category variable indicate whether given day hot cold, depending whether temperature 80 degrees .can also categorize data set variables. example might want summary year data set. can create can use mutate function create year variable.","code":"\nchicago <- mutate(chicago, tempcat = factor(1 * (tmpd > 80), labels = c(\"cold\", \"hot\")))\nhotcold <- group_by(chicago, tempcat)\nsummarize(hotcold, pm25 = mean(pm25, na.rm = TRUE),\no3 = max(o3tmean2),\nno2 = median(no2tmean2))## # A tibble: 3 × 4\n##   tempcat  pm25    o3   no2\n##   <fct>   <dbl> <dbl> <dbl>\n## 1 cold     16.0 66.6   24.5\n## 2 hot      26.5 63.0   24.9\n## 3 <NA>     47.7  9.42  37.4\nchicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)\nyears <- group_by(chicago, year)\nsummarize(years, pm25 = mean(pm25, na.rm = TRUE),\no3 = max(o3tmean2, na.rm = TRUE),\nno2 = median(no2tmean2, na.rm = TRUE))## # A tibble: 19 × 4\n##     year  pm25    o3   no2\n##    <dbl> <dbl> <dbl> <dbl>\n##  1  1987 NaN    63.0  23.5\n##  2  1988 NaN    61.7  24.5\n##  3  1989 NaN    59.7  26.1\n##  4  1990 NaN    52.2  22.6\n##  5  1991 NaN    63.1  21.4\n##  6  1992 NaN    50.8  24.8\n##  7  1993 NaN    44.3  25.8\n##  8  1994 NaN    52.2  28.5\n##  9  1995 NaN    66.6  27.3\n## 10  1996 NaN    58.4  26.4\n## 11  1997 NaN    56.5  25.5\n## 12  1998  18.3  50.7  24.6\n## 13  1999  18.5  57.5  24.7\n## 14  2000  16.9  55.8  23.5\n## 15  2001  16.9  51.8  25.1\n## 16  2002  15.3  54.9  22.7\n## 17  2003  15.2  56.2  24.6\n## 18  2004  14.6  44.5  23.4\n## 19  2005  16.2  58.8  22.6"},{"path":"downloading-files.html","id":"section","chapter":"51 Downloading files","heading":"51.8.3.6 %>%","text":"dplyr package implements special operator. allows chain different operations. allows see operations happening readable way. ’s indicated percent symbol greater symbol percent symbol. call pipeline operator. idea take data set feed pipeline operations create new data set. Chicago data set want mutate create month variable want create summary pollutant variables month. want take output mutate. group use according month variable. finally want take output group run summarize.","code":"\nchicago %>% mutate(month = as.POSIXlt(date)$mon + 1) %>% group_by(month) %>%  summarize(pm25 = mean(pm25, na.rm = TRUE),\n    o3 = max(o3tmean2, na.rm = TRUE),\n    no2 = median(no2tmean2, na.rm = TRUE))## # A tibble: 12 × 4\n##    month  pm25    o3   no2\n##    <dbl> <dbl> <dbl> <dbl>\n##  1     1  17.8  28.2  25.4\n##  2     2  20.4  37.4  26.8\n##  3     3  17.4  39.0  26.8\n##  4     4  13.9  47.9  25.0\n##  5     5  14.1  52.8  24.2\n##  6     6  15.9  66.6  25.0\n##  7     7  16.6  59.5  22.4\n##  8     8  16.9  54.0  23.0\n##  9     9  15.9  57.5  24.5\n## 10    10  14.2  47.1  24.2\n## 11    11  15.2  29.5  23.6\n## 12    12  17.5  27.7  24.5"},{"path":"downloading-files.html","id":"connecting-to-sql-database","chapter":"51 Downloading files","heading":"51.8.4 Connecting to SQL DataBase","text":"part teach us connect SQL database R submit queries . need dbplyr RSQLite packages.Retrieve SQL file way want. case retrieve url stock data folder.connect R terminal SQL database.","code":"\ndownload.file(url = \"https://ndownloader.figshare.com/files/2292171\", destfile = \"data/db_sql.sqlite\", mode = \"wb\")\ndb <- DBI::dbConnect(RSQLite::SQLite(), \"resources/downloading_files/db_sql.sqlite\")"},{"path":"downloading-files.html","id":"querying-the-database-with-the-sql-syntax","chapter":"51 Downloading files","heading":"51.8.4.1 Querying the database with the SQL syntax","text":"sql function allows us submit queries database using SQL language.","code":"\ntbl(db, sql(\"SELECT year, species_id, plot_id FROM surveys\"))## # Source:   SQL [?? x 3]\n## # Database: sqlite 3.37.0\n## #   [/home/runner/work/cc21fall2/cc21fall2/resources/downloading_files/db_sql.sqlite]\n##     year species_id plot_id\n##    <int> <chr>        <int>\n##  1  1977 NL               2\n##  2  1977 NL               3\n##  3  1977 DM               2\n##  4  1977 DM               7\n##  5  1977 DM               3\n##  6  1977 PF               1\n##  7  1977 PE               2\n##  8  1977 DM               1\n##  9  1977 DM               1\n## 10  1977 PF               6\n## # … with more rows"},{"path":"downloading-files.html","id":"querying-the-database-with-the-dplyr-syntax","chapter":"51 Downloading files","heading":"51.8.4.2 Querying the database with the dplyr syntax","text":"result can achieved using dplyr syntax.","code":"\nsurveys <- tbl(db, \"surveys\")\nsurveys %>% select(year, species_id, plot_id)## # Source:   lazy query [?? x 3]\n## # Database: sqlite 3.37.0\n## #   [/home/runner/work/cc21fall2/cc21fall2/resources/downloading_files/db_sql.sqlite]\n##     year species_id plot_id\n##    <int> <chr>        <int>\n##  1  1977 NL               2\n##  2  1977 NL               3\n##  3  1977 DM               2\n##  4  1977 DM               7\n##  5  1977 DM               3\n##  6  1977 PF               1\n##  7  1977 PE               2\n##  8  1977 DM               1\n##  9  1977 DM               1\n## 10  1977 PF               6\n## # … with more rows"},{"path":"downloading-files.html","id":"other-benefit-of-dplyr","chapter":"51 Downloading files","heading":"51.8.5 Other benefit of dplyr","text":"dplyr can work data frame “backends”data.table large fast tablesSQL interface relational databases via DBI package","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"video-introduction-to-maps-with-ggmap","chapter":"52 Video introduction to maps with ggmap","heading":"52 Video introduction to maps with ggmap","text":"Ryan Rogers","code":"\n# install.packages(\"ggmap\")\nlibrary(ggmap)\n# You will also need to activate an API key! Use a line like the following:\n# register_google(\"<API KEY>\")"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"link-to-video-tutorial","chapter":"52 Video introduction to maps with ggmap","heading":"52.1 Link to Video Tutorial","text":"https://www.youtube.com/watch?v=AKEEAaCbtog","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"getting-started","chapter":"52 Video introduction to maps with ggmap","heading":"52.2 Getting Started","text":"demonstration show make static maps within tidyverse framework using ggmap. get started, make sure ggmap installed ready go, since may installed default.One thing can somewhat pesky working ggmap Google maps. 2018, service requires API key use. access full features demonstration, register Google API key. Details can found help file ‘register_google’.Note: wish run code found file, need register Google API key!","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"why-ggmap","chapter":"52 Video introduction to maps with ggmap","heading":"52.3 Why ggmap?","text":"ggmap useful demonstration, since use syntax may familiar already! ggmap designed quite compatible ggplot2 (fact, Hadley Wickham, one authors ggplot2, co-author!). fact, much ggmap , hood, largely comprised ggplot2 components. ggmap, however, features added benefit abstracting away many involved components setting ggplot2 visual maps, doubtless prove easier use base ggplot2 alone. ggmap also many options customizing resulting visuals, though explore many options depth.noted ggmap excellent tool static maps. looking dynamic maps provide potential user interaction, may find libraries, leaflet, better suit needs.","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"dataset-for-demonstration","chapter":"52 Video introduction to maps with ggmap","heading":"52.4 Dataset for Demonstration","text":"demonstration, utilize data public WiFi locations New York City. dataset can found :\nhttps://data.cityofnewyork.us/City-Government/NYC-Wi-Fi-Hotspot-Locations/yjub-udmw . save dataset ‘wifi_data’:","code":"\nwifi_data <- read.csv(\"https://data.cityofnewyork.us/api/views/yjub-udmw/rows.csv?accessType=DOWNLOAD\")\nhead(wifi_data)##   OBJECTID Borough         Type          Provider                   Name\n## 1    10604       4 Limited Free          SPECTRUM      Baisley Pond Park\n## 2    10555       4 Limited Free          SPECTRUM           Kissena Park\n## 3    12370       3         Free  Transit Wireless           Grand St (L)\n## 4     9893       3         Free Downtown Brooklyn                       \n## 5    10169       1         Free  Transit Wireless Lexington Av-63 St (F)\n## 6    10880       4 Limited Free          SPECTRUM           Kissena Park\n##                 Location Latitude Longitude         X        Y\n## 1         Park Perimeter 40.67486 -73.78412 1044131.9 185219.9\n## 2         Park Perimeter 40.74756 -73.81815 1034637.5 211685.2\n## 3           Grand St (L) 40.71193 -73.94067 1000698.1 198655.9\n## 4          125 Court St. 40.68999 -73.99200  986470.0 190656.7\n## 5 Lexington Av-63 St (F) 40.76463 -73.96612  993636.6 217853.9\n## 6         Park Perimeter 40.74243 -73.81151 1036481.4 209820.1\n##           Location_T                Remarks     City                   SSID\n## 1 Outdoor TWC Aerial 3 free 10 min sessions   Queens              GuestWiFi\n## 2 Outdoor TWC Aerial 3 free 10 min sessions   Queens              GuestWiFi\n## 3     Subway Station                 SN 123 Brooklyn    TransitWirelessWiFi\n## 4            Outdoor                        Brooklyn Downtown Brooklyn WiFi\n## 5     Subway Station                 SN 223 New York    TransitWirelessWiFi\n## 6 Outdoor TWC Aerial 3 free 10 min sessions   Queens              GuestWiFi\n##   SourceID  Activated BoroCode Borough.Name\n## 1        0 09/09/9999        4       Queens\n## 2        0 09/09/9999        4       Queens\n## 3          09/09/9999        3     Brooklyn\n## 4          09/09/9999        3     Brooklyn\n## 5          09/09/9999        1    Manhattan\n## 6        0 09/09/9999        4       Queens\n##   Neighborhood.Tabulation.Area.Code..NTACODE.\n## 1                                        QN02\n## 2                                        QN22\n## 3                                        BK90\n## 4                                        BK09\n## 5                                        MN40\n## 6                                        QN62\n##   Neighborhood.Tabulation.Area..NTA. Council.Distrcit Postcode BoroCD\n## 1          Springfield Gardens North               28    11434    412\n## 2                           Flushing               20    11355    407\n## 3                  East Williamsburg               34    11206    301\n## 4       Brooklyn Heights-Cobble Hill               33    11201    302\n## 5      Upper East Side-Carnegie Hill                4    10065    108\n## 6                    Queensboro Hill               20    11355    407\n##   Census.Tract BCTCB2010     BIN        BBL DOITT_ID\n## 1          294       294       0          0     1408\n## 2          845       845       0          0     1359\n## 3          495       495       0          0     1699\n## 4            9         9 3388736 3002777501      298\n## 5          120       120       0          0      599\n## 6         1215      1215       0          0     1347\n##              Location..Lat..Long.\n## 1 (40.6748599999, -73.7841200005)\n## 2 (40.7475599996, -73.8181499997)\n## 3 (40.7119259997, -73.9406699994)\n## 4 (40.6899850001, -73.9919950004)\n## 5 (40.7646300002, -73.9661150001)\n## 6 (40.7424300003, -73.8115100003)"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"creating-maps-no-plotting","chapter":"52 Video introduction to maps with ggmap","heading":"52.5 Creating Maps (No plotting)","text":"dive overlay datapoints visuals graphs, ’ll start going create maps first place.","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"by-location-name","chapter":"52 Video introduction to maps with ggmap","heading":"52.5.1 By location name","text":"first, perhaps approachable way can plot map simply specifying name location ’d like see. works geocode function. example city Paris, France:want verify coordinates, can try ! Simply plug 48.85661, 2.352222 Google Maps, see indeed location Paris, France (, location roughly center Paris). revgeocode mapdist two useful functions inverse geocode calculate distances, respectively.generate map using location name, run two steps. First use get_map(), specifying location name. gives ggmap object. Second, generate ggplot2-compatible plot, using ggmap() providing ggmap object step 1 input. demonstrate location lookup works abstract names, let us try finding Parthenon, Athens:Clearly Parthenon viewed space. Let us adjust variables give us clearer view. First, adjust ‘zoom’ parameter. default, get_map select us, can specify manually. , initially set 10, right city. Let us try 16 instead:better. style map may little distracting, show change later.","code":"\ngeocode(\"Paris\")## # A tibble: 1 × 2\n##     lon   lat\n##   <dbl> <dbl>\n## 1  2.35  48.9\nparthenon <- get_map(location = \"parthenon, athens\")\n\nparthenon_map <- ggmap(parthenon)\n\nparthenon_map\nparthenon <- get_map(location = \"parthenon, athens\", zoom = 16)\n\nparthenon_map <- ggmap(parthenon)\n\nparthenon_map"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"by-geographic-coordinates","chapter":"52 Video introduction to maps with ggmap","heading":"52.5.2 By geographic coordinates","text":"Another way plot map geographic coordinates directly. , input coordinates Grand Canyon:","code":"\ngrand_canyon <- get_map(location = c(-112, 36.1), zoom = 9)\n\ngrand_canyon_map <- ggmap(grand_canyon)\n\ngrand_canyon_map"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"using-shape-files","chapter":"52 Video introduction to maps with ggmap","heading":"52.5.3 Using shape files","text":"Shape files also option plotting maps ggmap, cover .","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"quick-map-plot","chapter":"52 Video introduction to maps with ggmap","heading":"52.6 Quick Map Plot","text":"event like quick visual plotted, given set latitude longitude points like superimpose map, easy option form qmplot. example, using sample dataset:looking quick visual, may suffice cases.","code":"\nqmplot(data=wifi_data, x = Longitude, y = Latitude, color = I('Blue'), alpha = I(0.05))"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"plotting-by-overlaying-ggplot2-visuals","chapter":"52 Video introduction to maps with ggmap","heading":"52.7 Plotting by Overlaying ggplot2 Visuals","text":"cases, want elaborate visuals, may easier accomplish ggplot2 ordinary ggmaps. examples:","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"scatterplot","chapter":"52 Video introduction to maps with ggmap","heading":"52.7.1 Scatterplot","text":"Let us start basic plotting. Note ‘lat’ ‘lon’ expected names x y values.see two issues. First, legend inset top left. can modified within ggmap. Second, map colors may confusing, given parks water colors several categories. Let us correct issues:used arguments extent = ‘device’ legend = “topleft” move legend, source = ‘stamen’ maptype = “toner” adjust map’s appearance.","code":"\nwifi_data_renamed <- mutate(wifi_data, lat = Latitude, lon = Longitude)\n\nnyc <- get_map(location = \"manhattan\", zoom = 12)\n\nggmap(nyc) +\n  geom_point(data = wifi_data_renamed, aes(color = Type), alpha=.2)\nnyc <- get_map(location = \"manhattan\", zoom = 12, source = 'stamen', maptype = \"toner\")\n\nggmap(nyc, extent = 'device', legend = \"topleft\") +\n  geom_point(data = wifi_data_renamed, aes(color = Type), alpha=.2)"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"heatmap-with-rectangular-bins","chapter":"52 Video introduction to maps with ggmap","heading":"52.7.2 Heatmap with Rectangular Bins","text":"event overplotting, rectangular heatmap can used. graph shows rectangular heatmap area manhattan north central park.","code":"\nnyc <- get_map(location = c(-73.96, 40.8), zoom = 14, source = 'stamen', maptype = \"toner\")\n\nggmap(nyc, extent = 'device', legend = \"topleft\") +\n  geom_bin_2d(data = wifi_data_renamed, aes(color = Type, fill = Type), bins = 40, alpha=.7)"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"contour-plots","chapter":"52 Video introduction to maps with ggmap","heading":"52.7.3 Contour Plots","text":"One final visual interest contour plot. example one:","code":"\nnyc <- get_map(location = \"manhattan\", zoom = 12, color = 'bw')\n\nggmap(nyc, extent = 'device', legend = \"topleft\") +\n  geom_density_2d_filled(data = wifi_data_renamed, alpha = 0.3)"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"overlaying-routes","chapter":"52 Video introduction to maps with ggmap","heading":"52.7.4 Overlaying Routes","text":"One neat feature ggmap ability superimpose routes. map shows available WiFi hotspots near walking route Columbia Central Park:Looks like pickings pretty slim!","code":"\nto_cp <- route(from = 'Columbia University', to = 'Frederick Douglass Circle', mode = 'walking')\n\nnyc <- get_map(location = c(-73.96, 40.803), zoom = 16, color = 'bw')\n\nggmap(nyc) +\n  geom_leg(aes(x = start_lon, y = start_lat, xend = end_lon, yend = end_lat), size = 2, color = 'blue', data=to_cp) +\n  geom_point(data = wifi_data_renamed, color = 'red')"},{"path":"video-introduction-to-maps-with-ggmap.html","id":"recommended-reading","chapter":"52 Video introduction to maps with ggmap","heading":"52.8 Recommended Reading","text":"’re looking additional reading ggmap, recommend https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf .","code":""},{"path":"video-introduction-to-maps-with-ggmap.html","id":"sources-4","chapter":"52 Video introduction to maps with ggmap","heading":"52.9 Sources","text":"L. Ellis. Map Plots Created R Ggmap. Little Miss Data. April 15, 2018. URL\nhttps://www.littlemissdata.com/blog/mapsD. Kahle H. Wickham. ggmap: Spatial Visualization ggplot2. R Journal, 5(1), 144-161. URL\nhttp://journal.r-project.org/archive/2013-1/kahle-wickham.pdfN. Voevodin. R, Best Practices. April 6, 2020. URL\nhttps://bookdown.org/voevodin_nv/R_Not_the_Best_Practices/maps.html","code":""},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"comparison-among-base-r-tidyverse-and-datatable","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53 Comparison among base R, tidyverse, and datatable","text":"Siyue Han","code":"\n# Load the tidyverse package\nlibrary(tidyverse)\n# Load the data.table package\nlibrary(data.table)"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"introduction-6","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.1 Introduction","text":"many ways read analyze data R, data.frame provided base R can handle situations. Therefore, using time far occasionally used tibble tidyverse. one time dealing large csv, found slow data.frame. help google, used data.table first time, amazing. Therefore, ’d like share way reading analyzing data people.","code":""},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"reading-data","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.2 Reading Data","text":"First, let’s see performance reading csv data among three environments.","code":""},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"data.frame","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.2.1 Data.Frame","text":"example reading csv data.frame.","code":"\nstart <- Sys.time()\ndf <- read.csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\")\nend <- Sys.time()\nprint(end - start)## Time difference of 1.393508 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"tibble","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.2.2 Tibble","text":"example reading csv tibble.","code":"\nstart <- Sys.time()\ntb <- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\")\nend <- Sys.time()\nprint(end - start)## Time difference of 1.306566 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"data.table","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.2.3 Data.Table","text":"example reading csv data.table.can see, data.table can read csv file super fast, especially file large. tibble tidyverse slightly faster data.frame base R, still much slower data.table.","code":"\nstart <- Sys.time()\ndt <- fread(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\")\nend <- Sys.time()\nprint(end - start)## Time difference of 0.1866136 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"processing-data","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3 Processing Data","text":", let’s see differences processing data among three environments.","code":""},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"selecting-columns-and-rows","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.1 Selecting column(s) and row(s)","text":"","code":"\nstart <- Sys.time()\nx1 <- df[101:110, c('Lat', 'Long_')]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.003000736 secs\nstart <- Sys.time()\nx2 <- select(tb, Lat, Long_) %>% slice(101:110)\nend <- Sys.time()\nprint(end - start)## Time difference of 0.01958561 secs\nstart <- Sys.time()\nx3 <- dt[101:110, .(Lat, Long_)]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.01274943 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"filtering-rows","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.2 Filtering row(s)","text":"","code":"\nstart <- Sys.time()\nx1 <- df[df$`X10.31.21` > 500000,]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.0188098 secs\nstart <- Sys.time()\nx2 <- filter(tb, `10/31/21` > 500000)\nend <- Sys.time()\nprint(end - start)## Time difference of 0.01417279 secs\nstart <- Sys.time()\nx3 <- dt[`10/31/21` > 500000,]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.003746748 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"sorting-the-table","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.3 Sorting the table","text":"","code":"\nstart <- Sys.time()\nx1 <- df[order(-df$`X10.31.21`), ]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.0241971 secs\nstart <- Sys.time()\nx2 <- arrange(tb, -`10/31/21`)\nend <- Sys.time()\nprint(end - start)## Time difference of 0.03213501 secs\nstart <- Sys.time()\nx3 <- dt[order(-`10/31/21`), ]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.1593106 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"summarizing-columns-by-group","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.4 Summarizing columns by group","text":"","code":"\nstart <- Sys.time()\nx1 <- aggregate(df$`X10.31.21`, list(df$Province_State), sum)\nend <- Sys.time()\nprint(end - start)## Time difference of 0.006551743 secs\nstart <- Sys.time()\nx2 <- group_by(tb, Province_State) %>% summarise(sum(`10/31/21`))\nend <- Sys.time()\nprint(end - start)## Time difference of 0.02136064 secs\nstart <- Sys.time()\nx3 <- dt[ , lapply(.(`10/31/21`), sum), by = Province_State] \nend <- Sys.time()\nprint(end - start)## Time difference of 0.003964424 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"pivoting-longer","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.5 Pivoting longer","text":"","code":"\nstart <- Sys.time()\nx1 <- reshape(df, \n              varying = 12:dim(df)[2],\n              timevar = \"Date\", v.names=\"Cases\",\n              direction = \"long\")\nx1 <- x1[, c('Combined_Key', 'Date', 'Cases')]\nend <- Sys.time()\nprint(end - start)## Time difference of 20.04066 secs\nstart <- Sys.time()\nx2 <- pivot_longer(tb, \n                   names_to = \"Date\", \n                   values_to = \"Cases\", \n                   -(1:11)) %>%\n      select(Combined_Key, Date, Cases)\nend <- Sys.time()\nprint(end - start)## Time difference of 0.9417338 secs\nstart <- Sys.time()\nx3 <- melt(dt, \n           id.vars = 1:11, \n           variable.name = \"Date\", \n           value.name = \"Cases\")[\n             , .(Combined_Key, Date, Cases)]\nend <- Sys.time()\nprint(end - start)## Time difference of 1.149003 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"joining-tables","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.6 Joining tables","text":"","code":"\nstart <- Sys.time()\nx1 <- merge(df[, 1:11], x1)\nend <- Sys.time()\nprint(end - start)## Time difference of 26.6534 secs\nstart <- Sys.time()\nx2 <- left_join(select(tb, 1:11), x2, by = \"Combined_Key\")\nend <- Sys.time()\nprint(end - start)## Time difference of 1.152521 secs\nstart <- Sys.time()\nx3 <- dt[, 1:11][x3, on = \"Combined_Key\"]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.7259252 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"chaining-structures","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.3.7 Chaining structures","text":"Base R chaining structure like tidyverse data.table. compare chaining structures tidyverse data.table.\ntidyverse uses %>% connect operations together.\ndata.table uses bracketed operations back back [...][...]., can see simple operations selecting, filtering sorting, Base R can finish fast. However, complex operations pivoting joining, Base R cost huge amount time. Comparing tidyverse data.table, can see data.table slightly faster speed tidyverse almost every task. Especially, using chaining structure, data.table finishes much faster tidyverse. probably data.table includes many different operations together one bracketed operation. example, use one bracketed operation group_by, summarise arrange task tidyverse. hand, since tidyverse one task function, task function easy understand name, code tidyverse readable data.table.","code":"\nstart <- Sys.time()\nx2 <- tb %>% \n      mutate(year = `10/31/21` - `10/31/20`) %>% \n      group_by(Province_State) %>% \n      summarise(year = sum(year)) %>% \n      arrange(-year)\n\nend <- Sys.time()\nprint(end - start)## Time difference of 0.03045535 secs\nstart <- Sys.time()\nx3 <- dt[, year := `10/31/21` - `10/31/20`, ][\n         order(-year), .(year = sum(year)), by = Province_State]\nend <- Sys.time()\nprint(end - start)## Time difference of 0.005479336 secs"},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"summary-of-key-functions","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.4 Summary of key functions","text":"","code":""},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"conclusion-1","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.5 Conclusion","text":"motivation project one day dealing large csv, found slow data.frame. help google, used data.table first time, amazing. Therefore, ’d like compare performance base R tidyverse reading analyzing data.project, learned use data.table analyze data. Also learned advantage using three ways. data.frame base R convenient easy way deal data analytics tasks, takes much time data large operation complex. Therefore, cases, better use data.table tidyverse. cases handling large dataset, data.table good choice since runs extremely fast. cases requiring speed much, especially collaborating others, can choose tidyverse since code readable.","code":""},{"path":"comparison-among-base-r-tidyverse-and-datatable.html","id":"reference-1","chapter":"53 Comparison among base R, tidyverse, and datatable","heading":"53.6 Reference","text":"[1] https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html[2] https://megapteraphile.wordpress.com/2020/03/25/data-frame-vs-data-table-vs-tibble--r/[3] https://mgimond.github.io/rug_2019_12/Index.html","code":""},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","text":"Jace Yang","code":""},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"introduction-7","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.1 Introduction","text":"Remember every time analyze dataset, torn whether density boxplot right one go? EDAV 2021F fellow, must thought Q5 HW1, one asked compare following one better.guidance solution suggests choose ridgeline plots density reveals information. high loan amounts $40,000 easier see using boxplots part b. However, ridgelines part c allow us see distributions data, gives us interesting insight. instance small business loans essentially uniformly distributed $10,000 $40,000, bump $40,000. Renewable energy loans tri-modal, modes $10,000, $27,500, $40,000. modes bumps lost boxplots, tell many outliers specific loan amount. goes along insight part , see majority loans multiples $5,000.However, mentioned highlighted sentence, boxplots also useful determining variable’s actual maximum:Aside , believe boxplots set useful features case well.clearly displays median ranking well differences . Even rank density plots Ridgeline plots, difficult identify groups bigger loan amounts median .clearly displays median ranking well differences . Even rank density plots Ridgeline plots, difficult identify groups bigger loan amounts median .Ridgeline plot allows us compare distribution variables, unnecessarily detailed due fact category variable contains 10 examples. box plot offers lot information glance interquatile range.Ridgeline plot allows us compare distribution variables, unnecessarily detailed due fact category variable contains 10 examples. box plot offers lot information glance interquatile range.However, way combine boxplot ridgeline(density) plots together, using raincloud plot:Now, can see density shape ridges, statistics boxplots, also scatter plot informs us loan amounts predominantly multiples $5,000, $35,000 $40,000, conclusion expected get 5. )!Without ado, lets break see created!","code":"\nloans_full_schema %<>%\n  mutate(loan_purpose = reorder(loan_purpose, loan_amount, FUN = median)) \nJACE_COLOR <- c(\"#FF5A5F\", \"#FFB400\", \"#007A87\", \n                 \"#8CE071\", \"#7B0051\", \"#00D1C1\", \"#FFAA91\", \"#B4A76C\", \n                 \"#9CA299\", \"#565A5C\", \"#00A04B\", \"#E54C20\")\n\nloans_full_schema %>%\n\nggplot() +\n  aes(y = loan_purpose,\n      x = loan_amount,\n      fill = loan_purpose) +\n  geom_boxplot(alpha = 0.75) +\n  theme_jace +\n  only_x +\n  scale_fill_manual(values = JACE_COLOR) +\n  labs(title = \"HW1 (5)b. Create horizontal boxplots of loan_amount, one for each level of loan_purpose\",\n       subtitle = \"Use loans_full_schema data from in openintro package\") +\n  scale_x_continuous(breaks = breaks_width(5000), labels = axis_unit_scaler) -> p1\n\nloans_full_schema %>%\n\nggplot() +\n  aes(x = loan_amount, \n      y = loan_purpose, \n      fill = loan_purpose) +\n  geom_density_ridges2(scale = 2) +\n  scale_fill_viridis_d(alpha = 0.9) +\n  theme_jace +\n  only_x +\n  labs(title = \"HW1 (5)c. Create ridgeline plots for the same data as in b).\",\n       subtitle = \"Use loans_full_schema data from in openintro package\") +\n  scale_x_continuous(breaks = breaks_width(5000), limits = c(0, NA))  -> p2\n\np1 / p2\nmax(loans_full_schema$loan_amount)## [1] 40000\nloans_full_schema %>%\n  \nggplot() +\n  aes(x = loan_purpose,\n      y = loan_amount,\n      fill = loan_purpose) +\n  geom_point(aes(color = loan_purpose),\n             position = position_jitter(w = .15),\n             size = 0.5,\n             alpha = 0.15) +\n  geom_boxplot(width = .24,\n               outlier.shape = NA,\n               alpha = 0.5) +\n  geom_flat_violin(position = position_nudge(x = .2),\n                   trim = TRUE, \n                   alpha = 0.7, \n                   scale = \"width\")  +\n  coord_flip() +\n  scale_x_discrete(expand = c(0,0)) +\n  scale_y_continuous(breaks = breaks_width(5000), label = axis_unit_scaler) +\n  scale_fill_manual(values = rev(JACE_COLOR)) +\n  scale_color_manual(values = rev(JACE_COLOR)) +\n  labs(x = \"\",\n       title = \"Raincloud Plot of loan amount for 11 different loan purpose\") +\n  theme_jace +\n  theme(legend.position = \"none\") +\n  only_x"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"components-of-raincloud-plot.","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.2 Components of raincloud plot.","text":"section, use variable verified_income less cardinality demonstrate one way generate raincloud plot:Essentially, raincloud plot just halved violin + jittered point + boxplot visulization combo.","code":"\nloans_full_schema %>%\n  select(verified_income, loan_amount)## # A tibble: 10,000 × 2\n##    verified_income loan_amount\n##    <fct>                 <int>\n##  1 Verified              28000\n##  2 Not Verified           5000\n##  3 Source Verified        2000\n##  4 Not Verified          21600\n##  5 Verified              23000\n##  6 Not Verified           5000\n##  7 Source Verified       24000\n##  8 Source Verified       20000\n##  9 Source Verified       20000\n## 10 Not Verified           6400\n## # … with 9,990 more rows"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"tailoring-out-the-cloud-halved-violin","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.2.1 Tailoring out the cloud: halved violin","text":"Remind violin plot cool wasting half space. symmetric half saying thing! , first thing ’d like cut half! simplest way know using geom_flat_violin PupillometryR pacakge.Note geom_flat_violin automatically enable trim = TRUE allows us see real min max default!Sometimes vertical raincloud plot better case visualizing high-cardinality variable hope keep wide yet short paper (show next section!). now, lets make look like “cloud” first flipping !Now clustered density curve! looks like ‘Documented’ Status larger density skewmess left, indicating applicants without verified income likely approved smaller loan amount. ’s hard read information !","code":"\nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(alpha = 0.7)\nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(alpha = 0.7) +\n  coord_flip() # New code added here"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"making-it-rain-jittered-points","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.2.2 Making it rain: jittered points","text":", creates “rain”.doesn’t looks right! Lets move cloud little bit higher., make rain falls:Now density well raw data generated density curve, allows us notice weird parallel dots. better understand median, quartiles, outliers data, lets add boxplots points!","code":"\nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(alpha = 0.7) +\n  geom_point(aes(color = verified_income)) + # New code added here\n  coord_flip() \nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(position = position_nudge(x = .2), # New code added here\n                   alpha = 0.7) +\n  geom_point(aes(color = verified_income)) +\n  coord_flip() \nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(position = position_nudge(x = .2),\n                   alpha = 0.7) +\n  geom_point(aes(color = verified_income),\n             position = position_jitter(w = .15)) + # New code added here\n  coord_flip() "},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"adding-the-box-to-collect-the-rain-boxplot","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.2.3 Adding the box to “collect” the rain: boxplot","text":"Oops, box large! Remember already move density curves higher, thus thing left now make boxplots smaller. remain aligned center points.Awesome! let’s apply magic !","code":"\nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(position = position_nudge(x = .2),\n                   alpha = 0.7) +\n  geom_point(aes(color = verified_income),\n             position = position_jitter(w = .15)) +\n  geom_boxplot() + # New code added here\n  coord_flip() \nggplot(loans_full_schema) +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_flat_violin(position = position_nudge(x = .2),\n                   alpha = 0.7) +\n  geom_point(aes(color = verified_income),\n             position = position_jitter(w = .15)) +\n  geom_boxplot(width = .25) + # New code added here\n  coord_flip() "},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"adjustment","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.2.4 Adjustment","text":"several parameters can tune make rain cloud plot looks good!geom_point:\nsize: larger data, smaller size point .\nalpha: larger data, lighter one data point set.\nmany data, pass data = . %>% sample_frac(0.33) include 33% generate dots (. refered data pipe ggplot()). sampling inside geom_point!\n\ngeom_point:size: larger data, smaller size point .size: larger data, smaller size point .alpha: larger data, lighter one data point set.\nmany data, pass data = . %>% sample_frac(0.33) include 33% generate dots (. refered data pipe ggplot()). sampling inside geom_point!\nalpha: larger data, lighter one data point set.many data, pass data = . %>% sample_frac(0.33) include 33% generate dots (. refered data pipe ggplot()). sampling inside geom_point!geom_boxplot\noutlier.shape: generally turn passing NA, raincloud scatter can distinguish outliers points lay outside boxplot lines.\nexample, last plot , boxplot shows 3 high outliers Verified class, geom_point already plotted jittered indicate many outlieres loan_amount = 40,000!\n\nwidth: width boxplot generally pass 0.25 looks good.\nalpha: see dots box plot, make transparent setting passing transparent color fill scaler.\nexample, alpha(\"blue\", 0.5) gives “#0000FF80”, “80” transparant effect added blue color).\n\ngeom_boxplotoutlier.shape: generally turn passing NA, raincloud scatter can distinguish outliers points lay outside boxplot lines.\nexample, last plot , boxplot shows 3 high outliers Verified class, geom_point already plotted jittered indicate many outlieres loan_amount = 40,000!\noutlier.shape: generally turn passing NA, raincloud scatter can distinguish outliers points lay outside boxplot lines.example, last plot , boxplot shows 3 high outliers Verified class, geom_point already plotted jittered indicate many outlieres loan_amount = 40,000!width: width boxplot generally pass 0.25 looks good.width: width boxplot generally pass 0.25 looks good.alpha: see dots box plot, make transparent setting passing transparent color fill scaler.\nexample, alpha(\"blue\", 0.5) gives “#0000FF80”, “80” transparant effect added blue color).\nalpha: see dots box plot, make transparent setting passing transparent color fill scaler.example, alpha(\"blue\", 0.5) gives “#0000FF80”, “80” transparant effect added blue color).geom_flat_violin\nscale: default area,results discernible difference density minority majority groups. already know difference simply want check -group distribution , pass scale = \"width\".\nalpha: personally like make little transparent aesthetic considerations.\nadjust: multiplicate bandwidth adjustment. example, adjust = 0.5 means use half default bandwidth.\ngeom_flat_violinscale: default area,results discernible difference density minority majority groups. already know difference simply want check -group distribution , pass scale = \"width\".scale: default area,results discernible difference density minority majority groups. already know difference simply want check -group distribution , pass scale = \"width\".alpha: personally like make little transparent aesthetic considerations.alpha: personally like make little transparent aesthetic considerations.adjust: multiplicate bandwidth adjustment. example, adjust = 0.5 means use half default bandwidth.adjust: multiplicate bandwidth adjustment. example, adjust = 0.5 means use half default bandwidth.scaler:\nchange colors, fill color aesthetics! can pick colors put scale_color_manual(values = YOUR_COLORS) scale_color_manual(values = YOUR_COLORS).\nscale_y_continuous: can create breaks utilizing scales pacakage. example,\nbreaks = breaks_width(5000): breaks every 5000.\nbreaks = pretty_breaks(10): breaks 10 axis.\n\nscaler:change colors, fill color aesthetics! can pick colors put scale_color_manual(values = YOUR_COLORS) scale_color_manual(values = YOUR_COLORS).change colors, fill color aesthetics! can pick colors put scale_color_manual(values = YOUR_COLORS) scale_color_manual(values = YOUR_COLORS).scale_y_continuous: can create breaks utilizing scales pacakage. example,\nbreaks = breaks_width(5000): breaks every 5000.\nbreaks = pretty_breaks(10): breaks 10 axis.\nscale_y_continuous: can create breaks utilizing scales pacakage. example,breaks = breaks_width(5000): breaks every 5000.breaks = breaks_width(5000): breaks every 5000.breaks = pretty_breaks(10): breaks 10 axis.breaks = pretty_breaks(10): breaks 10 axis.theme:\ngrid lines x axis (flip becomes y) now useless! x axis contains differnt categories, keeping grid lines yield unnecessary lines boxplots.\ntheme:grid lines x axis (flip becomes y) now useless! x axis contains differnt categories, keeping grid lines yield unnecessary lines boxplots.tips scale theme final “Additional Tips” section!tips scale theme final “Additional Tips” section!Now lets apply techniques get cool chart:Looks good right? Now get full picture dataset :Observing boxplots: median loan amount : Verified > Source Verified > Verified, roughly 3.5k~4k incremental difference.Observing boxplots: median loan amount : Verified > Source Verified > Verified, roughly 3.5k~4k incremental difference.Observing jitter: heaping effect occurs income verification statuses, loan amount appears numbers 1000, notably multiples 5,000.Observing jitter: heaping effect occurs income verification statuses, loan amount appears numbers 1000, notably multiples 5,000.Observing density: applicants without verified income, loan amount distribution unimodal around 10,000.Observing density: applicants without verified income, loan amount distribution unimodal around 10,000.Isn’t data talking graph? know code little bit complicated, hope code code 1.introduction begin make sense !","code":"\nloans_full_schema %>%\n  ggplot() +\n  aes(x = verified_income,\n      y = loan_amount,\n      fill = verified_income) +\n  geom_point(aes(color = verified_income),\n             position = position_jitter(w = .15),\n             size = 0.5,\n             alpha = 0.05) +\n  geom_boxplot(width = .25,\n               outlier.shape = NA,\n               alpha = 0.5) +\n  geom_flat_violin(position = position_nudge(x = .2),\n                   alpha = 0.7,\n                   adjust = 0.5)  +\n  coord_flip() +\n  scale_x_discrete(expand = c(0,0)) +\n  scale_y_continuous(breaks = breaks_width(5000), labels = axis_unit_scaler) +\n  scale_fill_manual(values = JACE_COLOR) +\n  scale_color_manual(values = JACE_COLOR) +\n  guides(fill = guide_legend(title = \"Type of verification of the applicant's income\",\n                             direction = \"horizontal\",\n                             reverse = T),\n         color = guide_legend(title = \"Type of verification of the applicant's income\",\n                              direction = \"horizontal\",\n                              reverse = T)) +\n  labs(x = \"\",\n       title = \"Raincloud Plot of loan amount by 3 different verified income status\") +\n  theme_jace +\n  only_x +\n  theme(legend.position = \"bottom\") -> p\np"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"more-examples","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.3 More Examples","text":"section, organized codes previous work provide examples! can select one template create ! Typically, can use raincloud plot :1 categorical variable, 1 numerical variable (already !)1 categorical variable, 1 numerical variable (already !)2 categorical variable 1 sub-group splitted , 1 numerical variable.2 categorical variable 1 sub-group splitted , 1 numerical variable.2 Categorical X + Numerical Y2 Categorical X + Numerical Y1 categorical X + 1 numerical X + 1 numerical Y1 categorical X + 1 numerical X + 1 numerical YLet’s check !","code":""},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"grouped-categorical-x-numerical-y","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.3.1 Grouped Categorical X + Numerical Y","text":"Data preparationThe Kaggle Walmart sales dataset provides us 30,000+ “time serieses” unit sales data 2011 2015, organized hierarchically store, category, state.following code, clean monthly total unit sales grouped 10 stores 3 states, store RData file easily reproduce plot!63 months Jan 2012 Apr 2016, multiplying 63 stores give us 630 rows.Original versionThis exactly created raincloud plot:looks okay, note color filled boxplot density curve redundant since store_id already mapped x axis! exactly case find legend unnecessary, probably aesthetic represents well.Better precticeA better way map fill “parent” group store_id, state_id!color contains state information now!","code":"\n# This chunk is not evaled! It just shows my preprocessing steps from a data you can download from https://www.kaggle.com/c/m5-forecasting-accuracy/data\nread_csv(\"resources/raincloud_plot_tutorial/sales_train_evaluation.csv\", col_types = cols())  %>%\n\n  # Calculate monthly mean sales by every store\n  group_by(state_id, store_id) %>% \n  summarise_at(vars(starts_with(\"d_\")), sum) %>% \n  pivot_longer(starts_with(\"d_\"), names_to = \"dates\", values_to = \"sales\") %>%\n  mutate(dates = as.integer(str_remove(dates, \"d_\")),\n         dates = date(\"2011-01-29\") + dates - 1,\n         store_id = str_remove(store_id, \"_validation|_evaluation\"),\n         month = month(dates),\n         year = year(dates)) %>% \n  \n  # Adjust it\n  group_by(month, year, state_id, store_id) %>% \n  summarise(sales = sum(sales),\n            dates = min(dates)) %>% \n  ungroup() %>% \n  filter(str_detect(as.character(dates), \"..-..-01\"),\n         dates != max(dates)) %>% \n  transmute(state_id,\n            store_id,\n            year,\n            month,\n            sales) -> walmart_monthly_sales\nsave(walmart_monthly_sales, file = \"resources/raincloud_plot_tutorial/walmart_monthly_sales.RData\")\nload(file = \"resources/raincloud_plot_tutorial/walmart_monthly_sales.RData\")\nwalmart_monthly_sales## # A tibble: 630 × 5\n##    state_id store_id  year month  sales\n##    <chr>    <chr>    <int> <int>  <dbl>\n##  1 CA       CA_1      2012     1 106579\n##  2 CA       CA_2      2012     1  77786\n##  3 CA       CA_3      2012     1 160276\n##  4 CA       CA_4      2012     1  58738\n##  5 TX       TX_1      2012     1  77448\n##  6 TX       TX_2      2012     1 107283\n##  7 TX       TX_3      2012     1  80685\n##  8 WI       WI_1      2012     1  51901\n##  9 WI       WI_2      2012     1  57600\n## 10 WI       WI_3      2012     1 115079\n## # … with 620 more rows\nJACE_COLOR_2 = c(\"#FF5A5F\", \"#FFB400\", \"#007A87\", \n                 \"#8CE071\", \"#7B0051\", \"#00D1C1\", \"#FFAA91\", \"#B4A76C\", \n                 \"#9CA299\", \"#565A5C\", \"#00A04B\", \"#E54C20\")\n\nggplot(walmart_monthly_sales) +\n  aes(x = store_id, \n      y = sales, \n      fill = store_id) +\n  geom_flat_violin(position = position_nudge(x = .2), \n                   alpha = .4) +\n  geom_point(aes(color = store_id), \n             position = position_jitter(w = .15), \n             size = 1,\n             alpha = 0.4,\n             show.legend = F) +\n  geom_boxplot(width = .25, \n               outlier.shape = NA,\n               alpha = 0.5) +\n  scale_fill_manual(values = JACE_COLOR_2) +\n  scale_color_manual(values = JACE_COLOR_2)  +\n  scale_y_continuous(labels = axis_unit_scaler, breaks = breaks_width(25e3)) +\n  labs(x = \"Store ID\",\n       y = \"Monthly Sales\",\n       fill = \"Store ID: \",\n       title = \"Raincloud plot of Walmart monthly sales from 10 stores in 3 states\") +\n  guides(fill = guide_legend(nrow=1,\n                             byrow=TRUE))+\n  theme_jace +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(-5, 0, 0, 0))\nwalmart_monthly_sales %>%\n\nggplot() +\n aes(x = store_id, \n     y = sales, \n     fill = state_id) + # Code changed here\n  geom_flat_violin(position = position_nudge(x = .2), \n                   alpha = .4) +\n  geom_point(aes(color = state_id), \n             position = position_jitter(w = .15), \n             size = 1,\n             alpha = 0.4,\n             show.legend = F) +\n  geom_boxplot(width = .25, \n               outlier.shape = NA,\n               alpha = 0.5) +\n  scale_fill_manual(values = JACE_COLOR_2) +\n  scale_color_manual(values = JACE_COLOR_2)  +\n  scale_y_continuous(labels = axis_unit_scaler, breaks = breaks_width(25e3)) +\n  labs(x = \"Store id\",\n       y = \"Monthly Sales\",\n       fill = \"State Name: \",\n       title = \"Raincloud plot of Walmart monthly sales from 10 stores in 3 states\") +\n  guides(fill = guide_legend(nrow=1,\n                             byrow=TRUE))+\n  theme_jace +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(-5, 0, 0, 0))"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"categorical-x1-numerical-x2-numerical-y","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.3.2 Categorical X1 + Numerical X2 + Numerical Y","text":"UCI Bank Marketing dataset provides us population characteristics object marketing campaign Portuguese financial institution, well whether successful binary target label.Lots interesting information graph, example:education: basic.4y seems group salesmen can earn clients among elder peopleFor education: basic.4y seems group salesmen can earn clients among elder peopleFor Job: younger students easily become new client!Job: younger students easily become new client!······","code":"\n# This chunk is not evaled! It just shows my preprocessing steps from `bank-additional-full.csv` that I downloaded from https://archive.ics.uci.edu/ml/machine-learning-databases/00222/\nread_delim(\"resources/raincloud_plot_tutorial/bank-additional-full.csv\", \";\", \n           escape_double = FALSE, \n           trim_ws = TRUE) %>%\n  mutate(education = factor(education, \n                            levels = c(\"illiterate\", \"basic.4y\", \"basic.6y\",\n                                       \"basic.9y\", \"high.school\", \"university.degree\", \n                                       \"professional.course\", \"unknown\")) %>% \n                       fct_relevel(\"unknown\", after = Inf),\n         job = factor(job) %>% fct_relevel(\"unknown\", after = Inf)) %>%\n  rename(result = y) %>%\n  arrange(education) %>% \n  select(job, education, age, result) -> bank\nsave(bank, file = \"resources/raincloud_plot_tutorial/bank.RData\")\nload(file = \"resources/raincloud_plot_tutorial/bank.RData\")\nbank %>%\n  \n  pivot_longer(job:education,\n               names_to = \"variable\",\n               values_to = \"class\") %>%\n  mutate(class = factor(class, levels = unique(c(levels(bank$education), levels(bank$job)))) %>%\n           fct_relevel(\"unknown\", after = Inf),\n         result = factor(result, levels = c(\"yes\", \"no\")) %>%\n           fct_recode(sucess = \"yes\", fail = \"no\")) %>%\n  \nggplot() +\n  aes(x = class,\n      y = age,\n      fill = result) + #\n  geom_flat_violin(position = position_nudge(x = .2), \n                   alpha = .4) +\n  geom_point(aes(color = result), \n             position = position_jitterdodge(jitter.width = .15, # to sepreate jitter into their group.\n                                             dodge.width = .3), \n             size = .3, \n             alpha = .03,\n             show.legend = F) +\n  geom_boxplot(width = .3, \n               outlier.shape = NA,\n               alpha = .5) +\n  facet_wrap(~variable, ncol = 1, scale = \"free_x\") +\n  labs(x = \"\", \n       y = \"Age\",\n       title = \"Raincloud plot of the age of clients, by their education and jobs\") +\n  guides(fill = guide_legend(title=\"Result of the call\")) +\n  scale_fill_manual(values = c(\"#56B4E9\", \"#E69F00\"), na.value = \"#5f5f5f\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\"), na.value = \"#5f5f5f\") +\n  scale_y_continuous(breaks = breaks_width(20)) +\n  theme_pubr(base_size = 11, x.text.angle = 0) +\n  theme(legend.position = \"right\") +\n  text_theme"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"grouped-numerical-x-grouped-numerical-y","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.3.3 Grouped Numerical X + Grouped Numerical Y","text":"Raincloud plots can also utilized two additional scatter plot subplots, one left y axis one bottom x axis!idea seeing folks add box plot density curve around axes, like “Add marginal plots” section ggpubr tutorial.Let use ames openintro package used homework 2! First divide 1~10 overall quality variable 3 groups:generate 3 subplots organize patchwork package.can observe regression line steep dashed line original figure splitting houses three categories. Lower quality houses house price area distributed lower range (3k size 270k price), also slowest rate house price growth along area, implying lowest average price per square meter.","code":"\names %>% \n  transmute(price,\n            Overall.Qual = cut(Overall.Qual, breaks = c(0, 4, 7, 10)),\n            area) -> ames_cutted\nhead(ames_cutted)## # A tibble: 6 × 3\n##    price Overall.Qual  area\n##    <int> <fct>        <int>\n## 1 215000 (4,7]         1656\n## 2 105000 (4,7]          896\n## 3 172000 (4,7]         1329\n## 4 244000 (4,7]         2110\n## 5 189900 (4,7]         1629\n## 6 195500 (4,7]         1604\nCOLOR_TEMP = c(\"#d5896f\",\"#dab785\",\"#70a288\")\names_cutted %>%\nggplot() +\n  aes(y = price, \n      x = area) +\n  geom_point(aes(color = Overall.Qual),\n             size = .9,\n             alpha = 0.25,\n             stroke = 0.7) +\n  geom_smooth(linetype = \"dashed\",\n              se = F,\n              color = 'darkgray',\n              formula = 'y ~ x',\n              method = \"lm\") +\n  geom_smooth(aes(color = Overall.Qual),\n              formula = 'y ~ x',\n              method = \"lm\") +\n  \n  scale_y_continuous(labels = axis_unit_scaler_1, breaks = pretty_breaks(10)) +\n  scale_x_continuous(labels = axis_unit_scaler_1, breaks = pretty_breaks(10)) +\n  scale_fill_manual(values = COLOR_TEMP) +\n  scale_color_manual(values = COLOR_TEMP) +\n  labs(x = \"Area\",\n       y = 'Price',\n       title = 'Scatter plot for the area and price of housing') +\n  theme_jace +\n  get_lightxy(0.6) -> p_scatter\n\names_cutted %>%\n \nggplot() +\n  aes(y = price, \n      x = Overall.Qual,\n      fill = Overall.Qual) + #\n  geom_flat_violin(position = position_nudge(x = .2), \n                   alpha = .8) +\n  geom_point(aes(color = Overall.Qual), \n             position = position_jitter(width = .15),\n             size = .3, \n             alpha = .5,\n             show.legend = F) +\n  \n  geom_boxplot(width = .3, \n               outlier.shape = NA,\n               alpha = .5) +\n  labs(x = \"Overall Quality\", y = \"Price\") +\n  scale_fill_manual(values = COLOR_TEMP, guide=FALSE) +\n  scale_color_manual(values = COLOR_TEMP) +\n  scale_y_continuous(labels = axis_unit_scaler_1, breaks = pretty_breaks(10)) +\n  theme_jace -> p_price\n\names_cutted %>%\n \nggplot() +\n  aes(y = area, \n      x = Overall.Qual,\n      fill = Overall.Qual) + #\n  geom_flat_violin(position = position_nudge(x = .2), \n                   alpha = .8) +\n  geom_point(aes(color = Overall.Qual), \n             position = position_jitter(width = .15),\n             size = .3, \n             alpha = .5,\n             show.legend = F) +\n  geom_boxplot(width = .3, \n               outlier.shape = NA,\n               alpha = .5) +\n  coord_flip() +\n  labs(x = \"Overall Quality\", y = \"Area\", fill = NULL) +\n  scale_fill_manual(values = COLOR_TEMP, guide=FALSE) +\n  scale_color_manual(values = COLOR_TEMP) +\n  scale_y_continuous(labels = axis_unit_scaler_1, breaks = pretty_breaks(10)) +\n  theme_jace +\n  only_x -> p_area\n\n\nlayout = \"11333333333\n          11333333333\n          11333333333\n          11333333333\n          11333333333\n          11333333333\n          11333333333\n          ##222222222\n          ##222222222\"\n      \np_price + p_area + p_scatter + \n  plot_layout(design = layout) + \n  labs(title = \"House Price vs Area in Ames\",\n       subtitle = \"group by the over quality binned into 3 classes\")"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"additional-tips","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.4 Additional Tips","text":"Besides adjustment mentioned inthe 2.4, tips can also help create neat raincloud plot!","code":""},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"use-a-theme-without-grid","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.4.1 Use a theme without grid","text":"customized theme without grid base theme_clean()! reproduce cold, can run simply remove +theme_jace.","code":"\n# Create my own theme: theme_jace\nFONT = \"Times New Roman\"\ntheme_bw() -> themebw_help\ntheme(\n  text = element_text(family = FONT,\n                      color = \"black\"),\n  plot.title = element_text(face=\"bold\",\n                            hjust = 0.5,\n                            family = FONT,\n                            colour = \"black\",\n                            margin = margin(t = 10, r = 0, b = 10, l = 0),\n                            size = 15),\n  axis.text = element_text(family = FONT,\n                           color = \"black\"),\n  plot.subtitle = element_text(family = FONT,\n                               hjust = 0.5,\n                               size = 12),\n  axis.title = element_text(size = 12),\n  legend.title = element_text(size = 11,\n                              face = \"bold\",\n                              color = \"black\",\n                              family = FONT),\n  legend.text = element_text(size = 10,\n                             color = \"black\",\n                             family = FONT)) -> text_theme\n\ntheme(\n  panel.background = themebw_help$panel.background,\n  strip.background = element_rect(fill = alpha(\"lightgray\", 0.5), inherit.blank = T, colour = NA),\n  panel.border = themebw_help$panel.border,\n  legend.background = themebw_help$legend.background,\n  plot.background = element_rect(color = \"white\"),\n  panel.grid.major.y = element_line(linetype = \"dashed\", color = \"gray\")) -> background_theme\n\ntheme(\n  panel.background = element_rect(fill = \"transparent\", colour = NA), # bg of the panel\n  plot.background = element_rect(fill = \"transparent\", color = NA), # bg of the plot\n  panel.grid.major = element_blank(), # get rid of major grid\n  panel.grid.minor = element_blank(), # get rid of minor grid\n  panel.border = element_blank(),\n  legend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\n  legend.box.background = element_rect(fill = \"transparent\") # get rid of legend panel bg\n) -> empty_theme\n\ntheme_legend = theme(\n  legend.box.margin = margin(6, 6, 6, 6),\n  legend.background = element_rect(color = NA),\n  legend.box.background = element_blank()\n)\n\ntheme_clean() + text_theme + background_theme + empty_theme + theme_legend  -> theme_jace"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"adjust-the-axis","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.4.2 Adjust the axis","text":"quickly read value boxplots’ statsitics, density curves’ modality, dots concentrated raincloud, often :Disable minor lines can labeled, keep x’s major lines:\n\nonly_x = theme(\n  panel.grid.major.x = element_line(linetype = \"dashed\", color = \"lightgray\"),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor = element_blank()\n)\nadd simply + only_x plot!Disable minor lines can labeled, keep x’s major lines:add simply + only_x plot!Add breaks scale_y_continuous add amount labeled ticks. scales pacakage allowds :\nbreaks = breaks_width(5000): breaks every 5000.\nbreaks = pretty_breaks(10): breaks 10 ticks.\nAdd breaks scale_y_continuous add amount labeled ticks. scales pacakage allowds :breaks = breaks_width(5000): breaks every 5000.breaks = breaks_width(5000): breaks every 5000.breaks = pretty_breaks(10): breaks 10 ticks.breaks = pretty_breaks(10): breaks 10 ticks.axes large values, turn scitific notation options(scipen = 999). shorten ticks plugging function:\naxes large values, turn scitific notation options(scipen = 999). shorten ticks plugging function:, put scale_y_continuous like p + scale_y_continuous(labels = axis_unit_scaler, breaks = pretty_breaks(10))","code":"\nonly_x = theme(\n  panel.grid.major.x = element_line(linetype = \"dashed\", color = \"lightgray\"),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor = element_blank()\n)\naxis_unit_scaler <- function(n, digits = 1){\n  addUnits_leq_0 <- function(n){\n    labels <- ifelse(n < 1000, n,  # less than thousands\n                       ifelse(n < 1e6, paste0(round(n/1e3, digits = digits), 'k'),  # in thousands\n                              ifelse(n < 1e9, paste0(round(n/1e6, digits = digits), 'M'),  # in millions\n                                     ifelse(n < 1e12, paste0(round(n/1e9, digits = digits), 'B'), # in billions\n                                            ifelse(n < 1e15, paste0(round(n/1e12, digits = digits), 'T'), # in trillions\n                                                   'too big!'\n                                            )))))}\n  \n  labels <- ifelse(n < 0, \n                   paste0(\"-\", addUnits_leq_0(-n)),  # less than thousands\n                   ifelse(n >= 0, addUnits_leq_0(n),  \n                          \"NA\"))\n  return(labels)\n}\ncat(\"12,345 now bocomes\", axis_unit_scaler(12345),\n    \"6,666,666 now becomes\", axis_unit_scaler(6666666))## 12,345 now bocomes 12.3k 6,666,666 now becomes 6.7M"},{"path":"raincloud-plot-101-density-plot-or-boxplotwhy-not-do-both.html","id":"reference-2","chapter":"54 Raincloud plot 101: density plot or boxplot？Why not do both!","heading":"54.5 Reference","text":"Thank taking time read lengthy tutorial! Hope enjoy . Now, youwant know geniuses invented raincloud plot: Raincloud plots: multi-platform tool robust data visualization\nofficial tutorial R, matlab, python: Github. (learned fantastic technique first place!)\nwant know geniuses invented raincloud plot: Raincloud plots: multi-platform tool robust data visualizationTheir official tutorial R, matlab, python: Github. (learned fantastic technique first place!)want know ways create R: ggdistwant know ways create R: ggdistwant know create rain cloud plot python: Making Rain Raincloud Plotswant know create rain cloud plot python: Making Rain Raincloud Plotsknow better way (intended build one): please email jy3174@columbia.edu!know better way (intended build one): please email jy3174@columbia.edu!","code":""},{"path":"an-introduction-to-dygraph-in-r.html","id":"an-introduction-to-dygraph-in-r","chapter":"55 An Introduction to Dygraph in R","heading":"55 An Introduction to Dygraph in R","text":"Yanyun Chen","code":""},{"path":"an-introduction-to-dygraph-in-r.html","id":"these-are-the-codes-demonstrated-in-the-video-tutorial.-the-video-tutorial-can-be-found-at-httpswww.youtube.comwatchvewqdzu47yzs","chapter":"55 An Introduction to Dygraph in R","heading":"55.0.1 These are the codes demonstrated in the video tutorial. The video tutorial can be found at https://www.youtube.com/watch?v=ewQdzu47yZs","text":"Data Source: FRED, https://fred.stlouisfed.org/categories/9?t=cpi&ob=pv&od=desc#","code":"\nlibrary(readxl)\nlibrary(dygraphs)\nlibrary(dplyr)"},{"path":"an-introduction-to-dygraph-in-r.html","id":"project-description","chapter":"55 An Introduction to Dygraph in R","heading":"55.0.2 Project Description","text":"main motivation project ubiquity time series data. One thing \nnoticed past internship project experience need exploratory\nanalysis visualization data high, people less familiar \ntypes data. Moreover, r package dygraphs provides much easier \nintuitive functionality draw time series data compared ggplot plotly (\nintroduced class), additionally interactive features useful \nlong-term time series data. personally think video tutorial example codes \norganized rendered clean friendly way anyone r\nbackground understand learn. making process, ’ve also\nbenefitted lot exploring package, especially plotting prediction range \nactual data quick easy way. However, might differently next\ntime pay attention best practices visualizing time series data, \nfilling area.","code":""},{"path":"an-introduction-to-dygraph-in-r.html","id":"the-basic","chapter":"55 An Introduction to Dygraph in R","heading":"55.0.3 The Basic","text":"Note: video, fill demonstrated. filling time series graph generally recommended.","code":"\ncpi_data <- read_excel(\"resources/intro_to_dygraph/cpi.xlsx\", col_types = c(\"date\", \n    \"numeric\", \"numeric\"))\ncolnames(cpi_data) <- c(\"date\",\"us\",\"china\")\nus_cpi <- ts(cpi_data$us, frequency=12, start=c(1999,9))\ndygraph(us_cpi)\ndygraph(us_cpi, main = \"CPI, not seasonally adjusted: All Items for United States\") %>% \n  dySeries(\"V1\", label = \"United States CPI\",color='red') %>%\n  dyAxis(\"y\", label = \"Index (2015=100)\", valueRange = c(50, 120)) %>%\n  dyOptions(axisLineWidth = 1.5, \n            drawGrid = FALSE) %>%\n  dyRangeSelector(height = 20) %>%\n  dyLegend(show = \"follow\")"},{"path":"an-introduction-to-dygraph-in-r.html","id":"multiple-series","chapter":"55 An Introduction to Dygraph in R","heading":"55.0.4 Multiple Series","text":"","code":"\ncpi <- ts(cpi_data[,-1], frequency=12, start=c(1999,9))\ndygraph(cpi)\ndygraph(cpi,main=\"CPI, not Seasonally Adjusted: All Items\") %>%\n  dySeries(\"us\", label = \"United States\",color='red') %>%\n  dySeries(\"china\", label = \"China\",color='blue') %>%\n  dyAxis(\"y\", label = \"Index (2015=100)\") %>%\n  dyOptions(axisLineWidth = 1.5, drawGrid = TRUE) %>%\n  dyRangeSelector(height = 20)"},{"path":"an-introduction-to-dygraph-in-r.html","id":"events","chapter":"55 An Introduction to Dygraph in R","heading":"55.0.5 Events","text":"Note: video, drawing event line along shaded region demonstrated. Another option draw annotation along shaded region, demonstrated .","code":"\ndygraph(cpi, main=\"CPI, not Seasonally Adjusted: All Items\") %>%\n  dySeries(\"us\", label = \"United States\") %>%\n  dySeries(\"china\", label = \"China\") %>%\n  dyRangeSelector(height = 20) %>%\n  dyEvent(\"2019-12-01\", \"COVID-19\", labelLoc = \"bottom\") %>%\n  dyAnnotation(\"2008-05-01\", \"2008 Financial Crisis\", attachAtBottom = TRUE, width = 150) %>%\n  #dyEvent(\"2008-05-01\", \"2008 Financial Crisis\", labelLoc = \"bottom\") %>%\n  dyShading(from = \"2008-01-01\", to = \"2008-12-01\")"},{"path":"an-introduction-to-dygraph-in-r.html","id":"prediction","chapter":"55 An Introduction to Dygraph in R","heading":"55.0.6 Prediction","text":"","code":"\nchina_cpi <- ts(cpi_data$china, frequency=12, start=c(1999,9))\nhw <- HoltWinters(china_cpi)\npredicted <- predict(hw, n.ahead = 36, prediction.interval = TRUE)\nall <- cbind(china_cpi, predicted)\n\ndygraph(all, \"CPI, not Seasonally Adjusted: All Items for China\") %>%\n  dySeries(\"china_cpi\", label = \"Actual\") %>%\n  dySeries(c(\"predicted.lwr\", \"predicted.fit\", \"predicted.upr\"), label = \"Predicted\")\nchina_cpi_200 <- ts(cpi_data$china[1:200], frequency=12, start=c(1999,9))\nhw <- HoltWinters(china_cpi_200)\npredicted <- predict(hw, n.ahead = 64, prediction.interval = TRUE)\nall <- cbind(china_cpi, predicted)\n\ndygraph(all, \"CPI, not Seasonally Adjusted: All Items for China\") %>%\n  dySeries(\"china_cpi\", label = \"Actual\") %>%\n  dySeries(c(\"predicted.lwr\", \"predicted.fit\", \"predicted.upr\"), label = \"Predicted\")"},{"path":"streamlit-tutorial.html","id":"streamlit-tutorial","chapter":"56 Streamlit Tutorial","heading":"56 Streamlit Tutorial","text":"Elijah Flomen Blake HartungFor community contribution assignment, providing tutorial video use extremely powerful Python package, Streamlit. Streamlit allows users generate interactive dashboards data analyses visualizations people use. key package enables people little--knowledge Python conduct exploratory analysis dataset. package used properly, someone working knowledge Python Streamlit, dashboard made intuitive use anyone required technical programming knowledge. Streamlit’s built-UI intuitive clean, making tool powerful anyone wishes build rapid prototype web application present interactive data visualizations.tutorial cover basic functions Streamlit offer highlight usefulness package. Namely, demonstrate data visualization analysis tools Streamlit’s dashboards support via Iris Dataset well dataset including Uber Pickup data NYC.Link video: https://youtu./OH2QhK6xovw\nLink Github repo: https://github.com/blake-hartung/community_contrib_bh_ef.git\nhttps://github.com/blake-hartung/community_contrib_bh_ef.gitSources:Streamlit documentation (https://docs.streamlit.io/)\nIris Dataset (https://archive.ics.uci.edu/ml/datasets/iris)\nUber Pickup Dataset (https://www.kaggle.com/fivethirtyeight/uber-pickups--new-york-city)","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"hive-plots-with-the-ggraph-and-hiver-packages","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57 Hive plots with the ggraph and hiver packages","text":"Ben Zimnick","code":"\nlibrary('ggraph')\nlibrary('igraph')\nlibrary('ggplot2')\nlibrary('tidygraph')\nlibrary('dplyr')\nlibrary('HiveR')\nlibrary('grid')"},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"networks","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1 Networks","text":"","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"network-definition","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.1 Network Definition","text":"network collection connected objects. Networks often called graphs.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"node","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.2 Node","text":"Nodes connected objects network. Nodes often called vertices.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"edge","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.3 Edge","text":"Edges connections nodes. Edges often called links.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"directed-graph","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.4 Directed Graph","text":"edges directions. b nodes, can connected edge b /edge b . Directed edges often represented arrows pointing one node another.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"undirected-graph","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.5 Undirected Graph","text":"edges directions; relationship goes ways. b connected nodes, edge goes b b .","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"degree","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.6 Degree","text":"degree node number edges incident node. directed graphs, -degree number edges going node; -degree number edges exiting node.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"general-graph-example","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.7 General Graph Example","text":"New York City subway system network. stations (nodes) connected tunnels (edges). trains run directions, subway undirected graph. Otherwise, directed graph.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"directed-graph-example","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.8 Directed Graph Example","text":"family tree directed graph. people (nodes) connected relationships (edges). relationships go parent child. person parent child, /parent child person.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"undirected-graph-example","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.1.9 Undirected Graph Example","text":"network friends undirected graph. friends (nodes) connected relationships (edges). relationships inherent ordering like relationships parents children.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"hive-plots","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2 Hive Plots","text":"","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"hive-plot-definition","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.1 Hive Plot Definition","text":"hive plot method visualizing networks. nodes located lines distributed radially around center plot (like hands clock). Edges represented curved lines nodes.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"creating-hive-plots","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.2 Creating Hive Plots","text":"clear method organizing nodes. person creating graph decide works best data. nodes typically ordered degree axes.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"ggraph","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.3 ggraph","text":"ggraph recent method creating hive plots R. cran documentation updated February 23, 2021.two main ways create hive plots using ggraph. first relies igraph package; second relies dplyr tidygraph packages.first example (needed adjust slightly–included explanation ‘note’ section ) r-bloggers.com. second example rdocumentation.org. sources ‘sources’ section.examples use highschool data set, built ggraph package. data set shows friendships among high school boys. 1957 1958, boys asked boys friends . hive plots, nodes boys, edges friendships.used highschool data set resources explaining network plots, including ggraph cran documentation, use. Additionally, one three data sets built ggraph package (two flare whigs data sets), highschool data set convenient work .","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"igraph-method","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.4 igraph method","text":"method usses ggraph igraph packages.graph_from_data_frame function igraph package creates igraph graph data frame.degree degree node (vertex) number adjacent edges. node, degree function gives number edges going node (‘’), number edges going node (‘’), total degree (‘’).nodes going located three axes–, many, medium–corresponding number edges going node. example, nodes kids; edges friendships.ggraph extension ggplot2 deals graphs. ggraph sets type plot (hive) axis variable (friends). ggraph also sorts nodes degree.geom_edge_hive draws edges nodes. colour one many adjustable parameters.geom_axis_hive edits axes.","code":"\ngraph <- graph_from_data_frame(highschool)\nV(graph)$friends <- degree(graph, mode = 'in')\nV(graph)$friends <- ifelse(V(graph)$friends < 5, 'few', \n                           ifelse(V(graph)$friends >= 15, 'many', 'medium'))\nggraph(graph, 'hive', axis = friends, sort.by = 'degree') + \n  geom_edge_hive(aes(colour = factor(year))) + \n  geom_axis_hive(aes(colour = friends), size = 3, label = FALSE) + \n  coord_fixed()"},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"tidygraph-method","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.5 tidygraph method","text":"step uses ggraph, ggplot2, tidygraph, dplyr packages.as_tbl_graph converts data set tbl_graph, wrapper makes possible manipulate igraph objects tidy API.centrality_degree gives degree degree node.Create friends column divides nodes (kids) degrees (number friends).ggraph extension ggplot2 deals graphs. ggraph sets type plot (hive) axis variable (friends). ggraph also sorts nodes degree. Unlike previous example, degree quotes.","code":"\ngraph <- as_tbl_graph(highschool) %>% \n  mutate(degree = centrality_degree())\ngraph <- graph %>% \n  mutate(friends = ifelse(\n    centrality_degree(mode = 'in') < 5, 'few',\n    ifelse(centrality_degree(mode = 'in') >= 15, 'many', 'medium')\n  ))\nggraph(graph, 'hive', axis = friends, sort.by = degree) + \n  geom_edge_hive(aes(colour = factor(year))) + \n  geom_axis_hive(aes(colour = friends), size = 3, label = FALSE) + \n  coord_fixed()"},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"faceting","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.6 Faceting","text":"facet_edges allows split plots based edge attributes. nodes repeated every subplot. facet_edges equivalent facet_wrap ggplot.facet_nodes allows create subplots based node attributes.","code":"\nggraph(graph, 'hive', axis = friends, sort.by = degree) + \n  geom_edge_hive(aes(colour = factor(year))) + \n  geom_axis_hive(aes(colour = friends), size = 3, label = FALSE) + \n  coord_fixed() +\n  facet_edges(~year)"},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"note-1","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.2.7 Note","text":"resources igraph example quotes around degree first line ggraph section code. needed remove quotes get code work.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"hiver","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.3 HiveR","text":"HiveR another package creating hive plots. HiveR seem popular ggraph. HiveR first published 2011; updated 2020.HiveR contains two plant-pollinator data sets–Arroyo Safari– give number visits plant-pollinator pair. data sets taken 2003 paper Vazquez Simberloff. Safari data taken undisturbed area; Arroyo data collected area grazed cattle.used Arroyo data example.step uses HiveR grid packages.Load Arroyo data.plotHive creates hive plot HivePlotData object.rank method orders nodes degree. bkgnd sets background color. axLabs sets axis labels. axLab.pos sets axis label positions. axLab.gpar sets features (color, size, etc.) labels.","code":"\ndata(\"Arroyo\")\nplotHive(Arroyo,\n         method = 'rank', bkgnd = 'white',\n         axLabs = c('pollinators','plants'),\n         axLab.pos = c(10,7),\n         axLab.gpar = gpar(col = \"black\"))"},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"sources-5","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.4 Sources","text":"","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"networks-1","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.4.1 Networks","text":"“Math Insight.” Introduction Networks - Math Insight, https://mathinsight.org/network_introduction.“Math Insight.” Introduction Networks - Math Insight, https://mathinsight.org/network_introduction.Networks Graphs. Columbia University, https://kc.columbiasc.edu/ICS/icsfs/Networks.pdf?target=a96539a9-4cfd-44a5-baf2-fd798e1772ea.Networks Graphs. Columbia University, https://kc.columbiasc.edu/ICS/icsfs/Networks.pdf?target=a96539a9-4cfd-44a5-baf2-fd798e1772ea.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"hive-plots-1","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.4.2 Hive Plots","text":"Krzywinski, Martin. Hive Plots: Rational Network Visualization - Simple, Informative Pretty Linear Layout Network Analytics. http://www.hiveplot.com/.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"ggraph-1","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.4.3 ggraph","text":"“GGRAPH.” RDocumentation, https://www.rdocumentation.org/packages/ggraph/versions/2.0.5.“GGRAPH.” RDocumentation, https://www.rdocumentation.org/packages/ggraph/versions/2.0.5.Pedersen, Thomas L. Package ‘Ggraph’. CRAN, 23 Feb. 2021, https://cran.r-project.org/web/packages/ggraph/ggraph.pdf.Pedersen, Thomas L. Package ‘Ggraph’. CRAN, 23 Feb. 2021, https://cran.r-project.org/web/packages/ggraph/ggraph.pdf.“Introduction Ggraph: Layouts.” Data Imaginist, 6 Feb. 2017, https://www.data-imaginist.com/2017/ggraph-introduction-layouts/.“Introduction Ggraph: Layouts.” Data Imaginist, 6 Feb. 2017, https://www.data-imaginist.com/2017/ggraph-introduction-layouts/.“Introduction Ggraph: Layouts: R-Bloggers.” R-Bloggers, 18 Feb. 2017, https://www.r-bloggers.com/2017/02/introduction--ggraph-layouts/.“Introduction Ggraph: Layouts: R-Bloggers.” R-Bloggers, 18 Feb. 2017, https://www.r-bloggers.com/2017/02/introduction--ggraph-layouts/.","code":""},{"path":"hive-plots-with-the-ggraph-and-hiver-packages.html","id":"hiver-1","chapter":"57 Hive plots with the ggraph and hiver packages","heading":"57.4.4 HiveR","text":"Hanson, Bryan . Package ‘HiveR’. Depauw University, 15 Feb. 2013, http://www2.uaem.mx/r-mirror/web/packages/HiveR/HiveR.pdf.Hanson, Bryan . Package ‘HiveR’. Depauw University, 15 Feb. 2013, http://www2.uaem.mx/r-mirror/web/packages/HiveR/HiveR.pdf.Hanson, Bryan . HiveR Package. Depauw University, 25 June 2012, http://www2.uaem.mx/r-mirror/web/packages/HiveR/vignettes/HiveR.pdf.Hanson, Bryan . HiveR Package. Depauw University, 25 June 2012, http://www2.uaem.mx/r-mirror/web/packages/HiveR/vignettes/HiveR.pdf.","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"introduction-to-xai-explainable-ai-in-r","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58 Introduction to XAI (Explainable AI) in R","text":"Masataka Koga","code":"\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(doParallel)\nlibrary(caret)\nlibrary(iml)\nlibrary(patchwork)"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"motivation-3","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.1 Motivation","text":"Unlike linear models decision trees, machine learning models often described black-box models due complexity. difficult describe relationship independent dependent variables humans can intuitively understand. reason, machine learning models tend avoided cases goal explain mechanism event occurrence, causes model’s prediction must explained convince affected decision. words, machine learning mainly used tasks prediction important.However, simple interpretable models limit events can described. , utilize machine learning models, investigations researches methods explain model structure format easy humans understand progressed. addition, General Data Protection Regulation (GDPR) EU, strictest privacy security law world, puts premium transparent AI. Therefore, interpretability machine learning models becoming crucial required society.","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"introduction-8","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.2 Introduction","text":"Typical cases interpretability important can divided four types . cases, usage XAI (Explainable Artificial Intelligence), methods explaining interpreting black-box models, promising option.conclusions need reasoned convince affected machine learning model’s decisions:\nmodel makes conclusions decisions significant financial, psychological, physical impacts people, loan approvals diagnoses diseases, conclusion depends mainly model results, affected people likely seek satisfactory explanations.conclusions need reasoned convince affected machine learning model’s decisions:\nmodel makes conclusions decisions significant financial, psychological, physical impacts people, loan approvals diagnoses diseases, conclusion depends mainly model results, affected people likely seek satisfactory explanations.model may provide new knowledge humans, scientific laws:\nbuilding machine learning models, projects higher possibility model learn patterns anticipated. case, becomes critical elucidate mechanism pattern generation derive laws.model may provide new knowledge humans, scientific laws:\nbuilding machine learning models, projects higher possibility model learn patterns anticipated. case, becomes critical elucidate mechanism pattern generation derive laws.risks model faces measures risk required manage:\ncases, projects self-driving cars, risk model predictions wrong significant possible events must examined advance. cases, necessary comprehend events predictable events difficult predict machine learning models.risks model faces measures risk required manage:\ncases, projects self-driving cars, risk model predictions wrong significant possible events must examined advance. cases, necessary comprehend events predictable events difficult predict machine learning models.potential bias models enormous impacts:\nusing machine learning models describe events require ethical considerations, sexism, racism, etc., must recognize biases model may learn, correctly detect learned biases, take necessary actions.potential bias models enormous impacts:\nusing machine learning models describe events require ethical considerations, sexism, racism, etc., must recognize biases model may learn, correctly detect learned biases, take necessary actions.XAI (Explainable Artificial Intelligence), also called IML (Interpretable Machine Learning), term implies, refers machine learning models processes leading predicted estimated results can explained interpreted humans, technologies research fields related models. introduce methods explain black-box models one technologies. R packages XAI, iml DALEX well known methods follows, respectively. focus ones can implement R using iml package.","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"model-training","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.3 Model Training","text":"first, created neural network model (NN model) black-box model. following code, utilized ames dataset openintro package, analyzed PSet2, selected eight numeric variables ames independent variables, sampled 20% data training data introduce XAI simply. rsample package enables us split data random sampling train test data easily. Also, set set.seed(0) reproducible analysis. selected variables follows.Parameter tuning NN models often time-consuming due huge amount calculations. cases, can parallelize computation increase computation speed doParallel package. PC 8 cores. can check number cores launching command prompt typing “set NUMBER_OF_PROCESSORS.” R code , executing R function called “detectCores(),” R automatically sets number cores makePSOCKcluster() function according PC’s specifications.implementing NN model, can use caret package. package allows efficient implementation many different machine learning models R without learning management various related packages. Basically, need know predict(), calculates prediction values using trained models, train() functions caret. train NN model nnet package, set argument method “nnet”.can normalize data specify “preProcess = c(”center“,”scale“),” find best hyperparameter based cross-validation setting “trControl = trainControl(method =”cv“),” decide range cross-validation “tuneGrid = expand.grid(size = sequence_1, decay = sequence_2)” arbitrary desired sequences sequence_1 sequence_2. Also, can decide number parameter combinations test “tuneLength = n” desired number n. case, number tested patterns \\(n^2\\) caret searches grid points n points respectively size decay. , can set “lineout = TRUE” run regression NN model set “trace = FALSE” hide results step. save time tune model, specify large tuneLength tuneGrid patterns. Finally, changed metric default setting, “RMSE” (Root Mean Squared Error), “MAE” (Mean Absolute Error) MAE robust outliers.searched best hyperparameters 400 combinations, took 5 minutes. $results $bestTune parameter fitted model object shows us tuned hyperparameter. carel seemed work well MAE best model 0.3 times standard deviation price data though selected eight independent variables. Now, learning process NN model finished. following sections, explain XAI methods iml package implement .","code":"\nset.seed(0)\n\names_8 <- ames[c(\"price\", \"area\", \"Lot.Frontage\",\n                 \"Overall.Qual\", \"Year.Built\", \"Year.Remod.Add\",\n                 \"Total.Bsmt.SF\", \"TotRms.AbvGrd\", \"Fireplaces\")] %>% na.omit\n\n# split the ames data to train and test data by the rsample package\names_split <- initial_split(ames_8, prop = 0.2)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n# parallelize the computation by the doParallel package\ncl <- makePSOCKcluster(detectCores())\nregisterDoParallel(cl)\n\nmodelNnet <- caret::train(\n  price ~ ., \n  data = ames_train,\n  method = \"nnet\", \n  preProcess = c(\"center\", \"scale\"),\n  metric = \"MAE\",\n  trControl = trainControl(method = \"cv\"),\n  tuneLength = 20,\n  linout = TRUE,\n  trace = FALSE\n)\n\nmodelNnet$results %>% arrange(MAE) %>% round(5) %>% slice(1:20)##    size   decay     RMSE Rsquared      MAE    RMSESD RsquaredSD    MAESD\n## 1    37 0.00015 35726.60  0.82030 25468.22  5471.058    0.05909 3680.527\n## 2    27 0.00316 37465.14  0.80790 26166.02 10150.167    0.04451 5531.278\n## 3    35 0.10000 36430.76  0.80661 26332.16  7035.994    0.07227 3998.467\n## 4    33 0.10000 37664.35  0.79549 26342.81  9245.159    0.06696 4735.856\n## 5    37 0.00681 38331.92  0.79081 26512.93  9500.188    0.07179 5819.955\n## 6    37 0.03162 36909.45  0.79301 26549.28  8640.992    0.06349 4990.229\n## 7    39 0.00032 37427.54  0.80997 26599.77  7912.271    0.04127 4359.057\n## 8    37 0.04642 38513.41  0.78878 26610.00  8620.747    0.07060 4338.640\n## 9    37 0.06813 37844.89  0.79944 26640.74  8528.319    0.04049 4379.730\n## 10   39 0.00000 38725.90  0.79066 26788.45  8098.097    0.05364 3376.867\n## 11   35 0.00464 39356.09  0.78475 26810.90  6316.756    0.04519 3659.406\n## 12   39 0.04642 37050.99  0.81239 26846.04  7463.946    0.04654 4696.565\n## 13   29 0.00068 39368.11  0.78116 26854.55  9906.432    0.06423 4506.193\n## 14   39 0.00022 37567.39  0.79511 26905.47 10603.506    0.06394 5740.192\n## 15   33 0.00215 37523.85  0.79714 26945.15  6616.930    0.08212 4546.715\n## 16   39 0.00046 38245.61  0.79858 26988.93  8444.528    0.04124 4478.493\n## 17   39 0.00068 38054.64  0.79257 27019.82  8938.735    0.04386 4265.150\n## 18   31 0.00010 37041.91  0.80137 27094.78  6346.569    0.04844 4037.853\n## 19   39 0.00015 38397.60  0.78882 27125.06  9873.664    0.06774 6165.891\n## 20   39 0.01468 38973.02  0.78150 27158.82  6957.925    0.06207 5068.639\nprice_sd <- ames_train %>% summarise(sd(price)) %>% round(2)\nprice_sd %>% str_c(\"SD of Price in Train Data: \", .) %>% print## [1] \"SD of Price in Train Data: 82290.72\"\n(summarise(modelNnet$results, min(MAE)) / price_sd) %>% round(2) %>%\n  str_c(\"MAE / Price_SD: \", .) %>% print## [1] \"MAE / Price_SD: 0.31\""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"entire-model-interpretation","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.4 Entire Model Interpretation","text":"XAI can roughly divided three types policy, Entire Model Interpretation, Features Interpretation, Local Interpretation. first, briefly explain two methods entire model interpretation, Permutation Feature Importance Global Surrogate Model, intuitive means comprehend whole structure learned black-box models, implementation using iml package. need detailed information, Interpretable Machine Learning (e-book) Reference section offers methods iml package author e-book, Christoph Molnar, one authors iml.","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"permutation-feature-importance","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.4.1 Permutation Feature Importance","text":"Permutation Feature Importance method devised Breiman 2001 interpret random forests order ascertain magnitude importance independent variable. technique improved, model-agnostic method devised Fisher et al. 2018. method, feature values interest randomly shuffled dataset, prediction error calculated see much worsens. method, model error significant, importance independent variable important model.execute XAI methods, including permutation feature importance, iml package, need set Predictor object, basically requires fitted model object data used analyzing prediction model. Employing XAI methods classification models, need specify class type Predictor object. Apart predictor, FeatureImp three arguments, loss, compare, n.reptitions, follows.loss: loss function character indicate name loss function like “mae,” chose argument MAE criterion training NN modelcompare:“ratio” “difference,” adopt “error.permutation/error.orig” “error.permutation - error.orig” measure importance permutation respectivelyn.repetitions: number feature shuffling. higher repetition number leads stable accurate resultsIn graph , line shows range 5% 95 % quantile importance values repetitions, point estimated importance variable, median importance. result means Total.Bsmt.SF significant effect Lot.Frontage least effect price trained model.advantages method gives highly condensed information behavior model, comparable problems different units features choosing ratio errors instead difference errors, calculation automatically takes account interactions features, require re-training model, results can obtained fast. disadvantages clear whether training test data selected measure feature importance, calculation requires access true results, results may change significantly many calculations due randomness estimated results, results may distorted correlation features.","code":"\npredictor <- Predictor$new(modelNnet, data = ames_train)\nimp <- FeatureImp$new(predictor, loss = \"mae\", compare = \"ratio\")\nplot(imp)\nimp$results##          feature importance.05 importance importance.95 permutation.error\n## 1   Overall.Qual      1.332271   1.375708      1.445636          34619.43\n## 2  Total.Bsmt.SF      1.281268   1.342420      1.381296          33781.74\n## 3           area      1.181727   1.221719      1.238896          30744.32\n## 4     Year.Built      1.129118   1.143510      1.150887          28776.22\n## 5     Fireplaces      1.094286   1.138405      1.177194          28647.73\n## 6 Year.Remod.Add      1.100516   1.107295      1.150158          27864.87\n## 7  TotRms.AbvGrd      1.073049   1.092501      1.121043          27492.59\n## 8   Lot.Frontage      1.001445   1.022589      1.033454          25733.27"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"global-surrogate-model","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.4.2 Global Surrogate Model","text":"approximate model created trained machine learning model using training data without bias called Global Surrogate Model. adopt simple interpretable models linear models decision trees surrogate models makes possible interpret black-box models. also way create surrogate models compact models reduce feature numbers machine learning models. hand, approximate models created parts training data focusing data called Local surrogate models, introduced later section Local Interpretable Model-Agnostic Explanations (LIME).iml package provides TreeSurrogate method global surrogate models decision trees. Apart predictor, TreeSurrogate two arguments .maxdepth: number maximum tree depth. Default 2. deeper trees can grasp fitted models accurately less explainabilitytree.args: arguments conditional inference trees party::ctree(), specify ctree arguments optional requiredIn graph , can confirm Overall.Qual root node, second nodes area, distributions values predicted learned model (target model tree surrogate model) leaf node fat-tailed, distribution distinctive mean values leaf node. words, graph allows us interpret structure trained black-box model tree model approximates adequately. hand, MAE tree global surrogate model minor compared NN model. Therefore, results mean interpretable tree model level prediction accuracy actual values NN. forget target values tree model actual values values predicted fitted NN model.advantages global surrogate models addition interpretable model, black-box models can also adopted surrogate models, method intuitive easy explain data scientists. disadvantages surrogate models predict actual results, need careful conclusions, clear threshold determine whether approximation sufficient, approximate much, end just new black-box models.","code":"\ntree <- TreeSurrogate$new(predictor, maxdepth = 4)\nplot(tree)\nmae(modelNnet, ames_test) %>% round %>% str_c(\"MAE of NN model: \", .) %>% print## [1] \"MAE of NN model: 26435\"\n(abs(predict(modelNnet, ames_test) - predict(tree, ames_test)))[,1] %>%\n  mean %>% round %>% str_c(\"MAE of GS model: \", .) %>% print## [1] \"MAE of GS model: 25005\""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"features-interpretation","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.5 Features Interpretation","text":"section, introduce five methods feature interpretation, Partial Dependence Plot (PDP), Individual Conditional Expectation (ICE) Plot, Centered ICE (c-ICE) Plot, Accumulated Local Effects (ALE) Plot, Feature Interaction, grasp characteristics features learned black-box models, usage methods iml. feature interaction methods, d-ICE plot m-plot, less valuable implemented iml package. , explain , please refer e-book Interpretable Machine Learning interested.","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"pdp-ice-plot-and-c-ice-plot","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.5.1 PDP, ICE Plot, and c-ICE Plot","text":"J. H. Friedman proposed Partial Dependence Plot (PDP) 2001, calculates marginal effect independent variables interest dependent variables trained models; words, PDP draws much predicted values changes response value shift variable. Goldstein et al. introduced Individual Conditional Expectation (ICE) plot 2015. plot depicts fitted model predictions change variable values variable interest fixed value one sample, variable interest changed. average ICE samples value PDP. depicting ICE, can see effect interaction variables predictions, PDP obscure.draw using iml, can utilize FeatureEffects() method arguments predictor.features: names features compute PDP ICE plotmethod: “pdp+ice” PDP ICE plot within plotcenter.: use PDP ICE plotgrid.size: size grid evaluating predictionsfeature: feature name index compute PDP ICE plot. set using features insteadIn figure , PDPs depicted yellow curves, ICE plots black curves, distributions variable short lines x-axis. PDP, can say variables may positive correlations price. hand, Lot.Frontage Total.Bsmt.SF seem nonlinear relationships. Also, ICE roughly shows relationship independent variables price consistent sample heterogeneous due interactions independent variables.However, like graphs, ICE plots sometimes complicated see curves differ sample start different predictions. solve problem, can use centered ICE (c-ICE) plot, graph centered ICE curves one point variable (often selected lower end feature) shows difference prediction point.center.argument FeatureEffects() function gives easy way make c-ICE graphs assigning numeric value plot centered.Although, case, find apparent patterns changes c-ICE plots, c-ICE plots often make easier compare curves individual samples. can comprehend types shifts cases, samples go go others’ changes opposite.","code":"\npdp <- FeatureEffects$new(predictor,\n                          features = predictor$model$coefnames[1:4],\n                          method = \"pdp+ice\")\nplot(pdp)\npdp <- FeatureEffects$new(predictor,\n                          features = predictor$model$coefnames[5:8],\n                          method = \"pdp+ice\")\nplot(pdp)\nmin_val <- predictor$model$trainingData[,predictor$model$coefnames] %>% \n  summarize_each(min) %>% as.numeric\n\nfor(i in 1:length(min_val)){\n  pdp <- FeatureEffects$new(predictor,\n                            feature = predictor$model$coefnames[i],\n                            method = \"pdp+ice\",\n                            center.at = min_val[i])$plot()\n  plot(pdp)\n}"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"accumulated-local-effects-ale-plot","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.5.2 Accumulated Local Effects (ALE) Plot","text":"D. W. Apley presented Accumulated Local Effects (ALE) Plot 2016. ALE Plot calculates average effect independent variables predictions constructed machine learning models like PDP, ALE plots faster unbiased alternative . Unlike PDP, ALE plot can take account interactions variables even correlated, theory makes average output values aligned zero.way PDP, can quickly implement ALE FeatureEffects() method argument ‘method = “ale”.’ALE plot tells lot distinctive nonlinear relationships variables price, PDP represent. Therefore, depict ALE plot well feature interpretation methods. hand, interpret effect across intervals variables strongly correlated. Also, ALE plots sometimes many small ups downs, accompanied ICE curves, unlike PDPs, can reveal heterogeneity feature effect, less intuitive compared PDP much intricate implementation.","code":"\nale <- FeatureEffects$new(predictor,\n                          features = predictor$model$coefnames[1:4],\n                          method = \"ale\")\nplot(ale)\nale <- FeatureEffects$new(predictor,\n                          features = predictor$model$coefnames[5:8],\n                          method = \"ale\")\nplot(ale)"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"feature-interaction","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.5.3 Feature Interaction","text":"Feature Interaction method express strength interaction two features, iml package adopts Friedman’s H-statistic measure . Friedman’s H-statistic statistic devised Friedman et al. 2008. takes values 0 (interaction two variables) 1 (influence predicted values derived via interaction ).written , can simply compute plot Interaction() function iml package arguments already introduced .table shows area, Fireplaces, Overall.Qual relatively stronger interaction variables, interaction effects Year.Built Lot.Frontage smaller. Indeed, can regard cause difference PDP ALE plot area Overall.Qual high levels interactions. Though, plots Fireplaces, see similar difference, can guess interactions may offset ALE plots. Likewise, graphs Year.Built Lot.Frontage similar appearances compared others, makes sense since H-statistic values less others.","code":"\ninteract <- Interaction$new(predictor)\nplot(interact)"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"local-interpretation","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.6 Local Interpretation","text":"Finally, explain Shapley Value Local Interpretable Model-Agnostic Explanations (LIME) methods Local Interpretation. can explain individual predictions black-box machine learning models. Like previous sections, e-book Interpretable Machine Learning mentions techniques omitted since iml package ways implement .","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"shapley-value","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.6.1 Shapley Value","text":"Shapley value method convinced based idea fairly evaluating contribution independent variables predictions applying Shapley value proposed Shapley 1953 cooperative game theory. Intuitively, method calculating contribution follows: single model prediction, independent variable changed zero actual value sample random order, one variable time, prediction change due addition variable interest calculated change patterns averaged. iml package adopts Monte Carlo sampling approximation introduced Strumbelj et al. 2014.Shapley() method iml estimates Shapley value using following arguments. patchwork package designed make graph composition simple powerful, enables us draw set graphs “+”.x.interest: data.frame single row variable explainedsample.size: number Monte Carlo samples estimating Shapley valueFor sample calculated Shapley values, graph clearly shows, comparison average predicted values, variables take values, contribution make predicted values, much (less) prominent resulting values . Unlike local models LIME, explained next section, Shapley value can compared subsets data even single data, addition explanatory method based solid rational theory.hand, requires lot computation time, almost real-world problems, can provide approximate solutions. cases, necessary explain event contains features, correlation features, Shapley value may inappropriate. developing technology, corrections considered response pointed-problems. , Shapley value returns simple value per feature, prediction model like LIME. Therefore, possible predict effects caused input changes, prediction effect, “income increases x dollar, credit score increase y points.”","code":"\nshapley_f <- function(x.interest){\n  shapley <- Shapley$new(predictor,\n                         x.interest = x.interest)\n  plot(shapley) +\n      theme(axis.text = element_text(size = 10))\n}\n\n# draw a set of graphs by the patchwork package\nshapley_f(x.interest = ames_train[1,]) + shapley_f(x.interest = ames_train[2,]) +\nshapley_f(x.interest = ames_train[3,]) + shapley_f(x.interest = ames_train[4,])\nshapley_f(x.interest = ames_train[5,]) + shapley_f(x.interest = ames_train[6,]) +\nshapley_f(x.interest = ames_train[7,]) + shapley_f(x.interest = ames_train[8,])"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"local-interpretable-model-agnostic-explanations-lime","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.6.2 Local Interpretable Model-Agnostic Explanations (LIME)","text":"LIME specific implementation local surrogate model proposed Ribeiro et al. 2016 explain prediction. name suggests, locally approximates learned model interpretable model uses approximated model explain individual predictions. idea intuitive: LIME examines predictions change variation added input data fitted model. creates new dataset variation, predicts , trains interpretable models, Lasso decision tree, based new dataset predicted values. However, aware trained model globally good approximation locally approximates black-box model predictions.LIME, iml package LocalModel() method new arguments . fits locally weighted linear regression models (logistic regression models classification) interpretable models explain single model predictions.dist.fun: distance function name computing proximities (weights linear model). Defaults “gower.” distance functions stats::dist can usedkernel.width: width kernel proximity computation. used dist.fun “gower”k: number independent variablesLooking figure, unlike Shapley value, distinctive differences estimated results sample, LIME seem working well case. fact, since LIME tabular data, giving correct definition neighborhood significant unsolved problem LIME challenges, results reasonable. issues sampling technique fatal instability explanation make LIME much reliable method far Shapley value. Many problems need solved LIME can used safely.hand, LIME promising method number unique advantages: can replace original machine learning model, still utilize local interpretable model explanation; easy adopt Lasso short decision trees simple, human-understandable explanations noticeable findings; one methods effective tabular, textual, image data.","code":"\nlime_f <- function(x.interest){\n  lime <- LocalModel$new(predictor,\n                         x.interest = x.interest,\n                         k = ncol(ames_train) - 1)\n  plot(lime)\n}\n\n# draw a set of graphs by the patchwork package\nlime_f(x.interest = ames_train[1,]) + lime_f(x.interest = ames_train[2,]) +\nlime_f(x.interest = ames_train[3,]) + lime_f(x.interest = ames_train[4,])\nlime_f(x.interest = ames_train[5,]) + lime_f(x.interest = ames_train[6,]) +\nlime_f(x.interest = ames_train[7,]) + lime_f(x.interest = ames_train[8,])"},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"conclusion-2","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.7 Conclusion","text":"briefly explained XAI methods iml package, crucial ways meet social need interpreting machine learning models, implement R. topic vital current data science projects, tough follow many XAI methods now devised revised. want see techniques detailed information, Interpretable Machine Learning, authored Christoph Molnar, one best resources. also quite beneficial learn covered methods find best combination less cost XAI methods models ways easy implement models, tree models, others.","code":""},{"path":"introduction-to-xai-explainable-ai-in-r.html","id":"refenence","chapter":"58 Introduction to XAI (Explainable AI) in R","heading":"58.8 Refenence","text":"Reference manual iml package, Christoph Molnar Patrick Schratz: https://cran.r-project.org/web/packages/iml/iml.pdfReference manual DALEX package, Przemyslaw Biecek et al.: https://cran.r-project.org/web/packages/DALEX/DALEX.pdfReference manual rsample package, Julia Silge et al.:\nhttps://cran.r-project.org/web/packages/rsample/rsample.pdfReference manual doParallel package, Michelle Wallig et al.: https://cran.r-project.org/web/packages/doParallel/doParallel.pdfReference manual caret package, Max Kuhn et al.: https://cran.r-project.org/web/packages/caret/caret.pdfReference manual patchwork package, Thomas Lin Pedersen:\nhttps://cran.r-project.org/web/packages/patchwork/patchwork.pdfInterpretable Machine Learning, Christoph Molnar: https://christophm.github.io/interpretable-ml-bookHow use R model-agnostic data explanation DALEX & iml (Japanese), Satoshi Kato: https://www.slideshare.net/kato_kohaku/--use--r-modelagnostic-data-explanation--dalex-imlPermutation feature importance: Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001)Model-agnostic permutation feature importance: Fisher, Aaron, Cynthia Rudin, Francesca Dominici. “Models Wrong, Many Useful: Learning Variable’s Importance Studying Entire Class Prediction Models Simultaneously.” http://arxiv.org/abs/1801.01489 (2018).PDP: Friedman, Jerome H. “Greedy function approximation: gradient boosting machine.” Annals statistics (2001): 1189-1232.ICE plot: Goldstein, Alex, et al. “Peeking inside black box: Visualizing statistical learning plots individual conditional expectation.” Journal Computational Graphical Statistics 24.1 (2015): 44-65.ALE plot: Apley, Daniel W. “Visualizing effects predictor variables black box supervised learning models.” https://arxiv.org/abs/1612.08468v1 (2016).Feature Interaction (Friedman’s H-statistic): Friedman, Jerome H, Bogdan E Popescu. “Predictive learning via rule ensembles.” Annals Applied Statistics. JSTOR, 916–54. (2008).Original Shapley value cooperative game theory: Shapley, Lloyd S. “value n-person games.” Contributions Theory Games 2.28 (1953): 307-317.Shapley value Monte Carlo sampling approximation: Štrumbelj, Erik, Igor Kononenko. “Explaining prediction models individual predictions feature contributions.” Knowledge information systems 41.3 (2014): 647-665.LIME: Ribeiro, Marco Tulio, Sameer Singh, Carlos Guestrin. “trust ?: Explaining predictions classifier.” Proceedings 22nd ACM SIGKDD international conference knowledge discovery data mining. ACM (2016).","code":""},{"path":"tutorial-for-gg_cleveland.html","id":"tutorial-for-gg_cleveland","chapter":"59 Tutorial for gg_cleveland","heading":"59 Tutorial for gg_cleveland","text":"Mingyue XuIntroduction Tutorials R-package ggcleveland\nnew R-package data visualization born August, 2021, provides functions produce ‘ggplot2’ versions visualization tools described book William S. Cleveland, named ‘Visualizing Data’ (Cleveland W. S. (1993). “Visualizing Data”. Hobart Press.). Since brand new package, documents produced illustrate proper usage . document, show power package necessary explanations useful functions specific examples. specifically, use fastfood dataset reflect understandings algorithms package. original book William S. Cleveland main source also rough document CRAN reference. Package source can found ‘https://github.com/mpru/ggcleveland’. salute author Marcos Prunello marcosprunello@gmail.com.\nmay find original tutorials really compact lack vivid examples, purpose like give specific explanation ggcleveland. Indeed, also many useful datasets package like fusion, fly, etc. introduce document.\nequal_count\nfunction applies equal count algorithm divide set observations intervals \ncan certain level ovelapping. see following example intuitive comprehension.\n\nInputs: df: dataframe, vble: variable df need divided, n_int: number intervals result, frac: overlapping fraction required result.\nOutputs: can see example, two tables, first one shows divided groups variable fastfood\\(calories* counts overlaps (next group) group. shows corresponding dataframe original *fastfood* two columns added. *id* represents position original daraframe *intervel* indicates group belongs division. may find size dataframe increases 515 567, due overlapping observations, .e. observation appears many times number intervals belongs. Properties: may find groups divided *equal_count* algorithm monotonously increasing order *fastfood\\)calories. Moreover, note new dataframe returns arranged class fastfood$restaurant. Hence, using method, can simply generate dataframe show level calories different restaurant using skills ggplot2 package, see\n\nAlthough may find original histogram plot can also reflect level comparison calories different restaurant, histogram based equal_count implies information, e.g. Subway foods likely two extremes, either high-level calories low-level calories.\ngg_coplot\nfunction implements conditional plots, particularly powerful visualization tool studying response depends two factors. concretely, following examples, simple coplot example first plot. Moreover, can find difference relation plot conditional relation plot next two plots. Given (Conditioning ) total_fat, sat_fat trans_fat seem explicitly uniform relation compared result non-conditional plot. Hence, gg_coplot useful us determine conditional independence variables.\n\nexplanations important parameters function.\nInputs: df: dataframe, x: variable x-axis, y: variable y-axis, faceting: faceting numeric variable, number_bins: number groups required divide faceting variable, overlap: overlapping fraction required faceting variable result.\nOutputs: coplot (conditional plot)\n\nplot shows wealth information dependence sat_fat trans_fat. total, conditioning total_fat nonlinear pattern. Except subplot (2,2) (3,3), patterns seem positive correlation sat_fat trans_fat given total_fat. However, subplots (2,2) (3,3) fail conclusion also difference . Hence, part interaction two factors, words, seems uniform effect sat_fat trans_fat. sake comparison, let us see example conditional dependence follow.\nCoplots intervals must chosen compromise two competing criteria, .e. number points resolution. one side, lengths must sufficiently great dependence panels enough points effects seen, points dependence panel, noise data typically prevents points coalescing meaningful pattern. side, lengths must small enough maintain reasonable resolution, conditioning interval big, risk distorted view nature dependence changes dramatically value conditioning factor changes within interval.gg_pt\nfunction returns normal QQ plots set power transformations. Indeed, choose power value 1, returns exactly QQ plot, shown following example.\n\nInputs: df: dataframe, vble: variable need transformation, taus: vector values power transformations. Note 0-power transformation log transformation, .e. Power transformations class transformations includes logarithm.\nOutputs: corresponding QQ plot power transformation given power values.\nfollowing plots examples related different power transformation values.\n\nsimple power transformation applied plot, may find something interesting, example, may find power transformation brings distribution closest symmetry based property QQ plot. fastfood$calories case, best parameter seems 0.25 0.50, see .\ngg_rf\ngg_rf one practical functions package gg_cleveland, returns Residual-Fit plot, optionally including centered observed values. well-known, fitting data means finding mathematical descriptions structure data. structural description data can distributions differ location spread shape, can fitted estimating location, .e. computing location measure distribution. Simply saying, \\(p^{th}\\) part data, let \\(h_{pi}\\) \\(^{th}\\) measurement height let \\(\\bar{h}_{p}\\) mean height also fitted value \\(h_{pi}\\). residuals deviations heights fitted values, .e. \\(\\hat{\\epsilon}_{pi}=h_{pi}-\\bar{h}_{p}\\).\nfitted values account variation heights attributable variable fitting process. residuals remaining variation data variation due shifting means removed.\n\nsimple example let us understand plot showing. Since subtraction means removed effect individuals, centered fitted values zeros. right residual plots shows remaining parts subtraction means, .e. reflection level variation.\ncan also another complicated example based fastfood dataset, seems compressed residual plot previous one, obviously relation total_fat sat_fat clearly far linearity.\nmake_coplot_df\ncreates dataframes used coplot, pretty useful causal inference tasks. start simple example see kind dataset can derived function.\n\ncan see, make_coplot_df returns list two dataframes. first dataframe data1 original fastfood dataset new column grouping intervels added end. dataframe data2 implies information grouping. Since inputs function similar previous, leave part show use created dataframes. Note dataframe, can immediately use basic coplot function, leads result gg_coplot, .e. following two plots exactly .\ngg_quantiles\nfunction returns quantile-quantile plot compare given number groups. Let us recall original QQ plot library ggplot2, following example, draw QQ plot calories data different restaurants.\n\ngg_quantiles function similar task ? answer , can see definition gg_quantiles, function aims compare given number groups QQ plot, .e. focus relations differences groups. provide following example using gg_quantiles show campares calories differnet restaurants.\n\nIndeed, large amount information can derived graph. Clearly, know QQ plots symmetric along diagonal. may find closely similar pairs, e.g. (Dairy Queen, Sonic), (Burger King, Subway), … Also, obvious higher calories pairs, e.g. (Dairy Queen, Mcdonalds), (Arbys, Mcdonalds), …\ngg_tmd\nfunction returns Tukey’s Mean-Difference plot one-way data. Tukey mean-difference plot (simply m-d plot) can add substantially visual assessment shift two distributions. Suppose two variables \\(b_{}\\) \\(t_{}\\) compared QQ plot, corresponding m-d plot, difference \\(b_{}-t_{}\\) graphed means \\(\\frac{b_{}+t_{}}{2}\\), line \\(b=t\\) QQ plot becomes zero line m-d plot. shift assessed judging deviations zero line. intuition behind m-d plot often enhances perception effects can readily judge deviations horizontal line line nonzero slope. concretely, see following example, directly extended QQ plot among restaurants.\n\nm-d plot, comparison two distributions can summarized simple statement distribution one variable heights greater lower one. good news since many examples show shifts distributions can complex.\n\nFinally, provide interesting example based environmental, one included datasets. brief description dataset can found CRAN document (ggcleveland) :\n1). measurements made 111 days May September 1973 sites New York City metropolitan region; one measurement variable day.\n2). Solar radiation amount 800 1200 frequency band 4000A-7700A, measured Central Park, New York City.\n3). Wind speed average values 700 1000, measured LaGuardia Airport, 7 km Central Park.\n4). Temperature daily maximum, also measured LaGuardia.\n5) .Ozone cube root average hourly values 1300 1500, measured Roosevelt Island, 2 km Central Park 5 km LaGuardia\".\n\nseems like environmental quite simple data, however, many works can done application ggcleveland. goal analyzing environmental data determine ozone depends variables. simple way visualize hypervariate data graph pair variables scatterplot, see\n\nlot information contained scatterplot matrix (spm) graph. example, subplot (1,3) Ozone Solar radiation, can receive two messages. First, highest values Ozone occur Solar radiation 200 300 langleys. Second, highest values Solar radiation, Ozone stays low levels. However, spm technique want focus , goal use algorithms ggcleveland. scatterplot matrix environmental data shows strong association Ozone Temprature Ozone Wind speed. seems likely factors explains variation data explained others.\nless evidence Solar radiation explains variation Ozone given Temprature Wind speed. example, saw upper envelope scatterplot Ozone Solar radiation explainable behavior Temprature Wind speed. photochemistry results Ozone requires Solar radiation, laws chemistry must obeyed, know principle Solar radiation causal factor. mean environmental data, Solar radiation measurements good predictors Ozone. Perhaps minimum amount Solar radiation needed, beyond , production sensitive amount.\nHence, need way see conditional dependence variables, .e. hypervariate coplot, see\n\n\\(\\textbf{refering source code *gg_coplot* **https://github.com/mpru/ggcleveland/blob/master/R/gg_coplot.R**, admit function unable draw conditional plot given two variables, indeed shortcoming newborn function}\\). Note conditioning intervals figure chosen using equal_count method target fraction overlap 0.4, thus, number values conditioning factor interval roughly constant. coplots shows rather convincingly Solar radiation explains variation Ozone measurements beyond explained Wind speed Temperature. Clearly, three coplots three factors Solar radiation, Temperature Wind speed.\n\nFitting essential visualizing hypervariate data fit hypervariate dataset hypervariate surface, function three variables, function Solar radiation, Wind speed Temperature. strategy displaying fitted hypervariate surface, exactly use conditioning. Suppose u one factors. can assign values factors, obtain graph u given assigned, conditioning values. \\(\\textbf{However, certain difficulty arose *coplot* unable add fitting curve (surface) *gg_coplot* able ; hand, *coplot* can condition two variables *gg_coplot* unable }\\). following example, address dilemma applying previous mentioned make_coplot_df method.\n\nalso many interesting experiments can conducted based gg_cleveland package. Visualization critical data analysis tools matter. exceptionally powerful visualization tools, also others, well known, rarely outperform best ones. Hope R-packages can created stage powerful Data Visualization methods.\n","code":"\ndata(fastfood, package = \"openintro\")\nhelp(fastfood)\nequal_count(df=fastfood, vble=calories, n_int=10, frac=0)## $intervals\n## # A tibble: 10 × 5\n##        n lower upper count   overlap\n##    <int> <dbl> <dbl> <table>   <int>\n##  1     1    15   215 54            6\n##  2     2   205   305 57            7\n##  3     3   295   355 56            0\n##  4     4   355   425 53            7\n##  5     5   415   495 64            8\n##  6     6   485   565 58            6\n##  7     7   555   645 59           10\n##  8     8   635   735 58            7\n##  9     9   725   895 56            1\n## 10    10   885  2435 52           NA\n## \n## $df_long\n## # A tibble: 567 × 19\n##    restaurant item      calories cal_fat total_fat sat_fat trans_fat cholesterol\n##    <chr>      <chr>        <dbl>   <dbl>     <dbl>   <dbl>     <dbl>       <dbl>\n##  1 Mcdonalds  Artisan …      380      60         7       2       0            95\n##  2 Mcdonalds  Single B…      840     410        45      17       1.5         130\n##  3 Mcdonalds  Double B…     1130     600        67      27       3           220\n##  4 Mcdonalds  Grilled …      750     280        31      10       0.5         155\n##  5 Mcdonalds  Crispy B…      920     410        45      12       0.5         120\n##  6 Mcdonalds  Big Mac        540     250        28      10       1            80\n##  7 Mcdonalds  Cheesebu…      300     100        12       5       0.5          40\n##  8 Mcdonalds  Cheesebu…      300     100        12       5       0.5          40\n##  9 Mcdonalds  Classic …      510     210        24       4       0            65\n## 10 Mcdonalds  Double C…      430     190        21      11       1            85\n## # … with 557 more rows, and 11 more variables: sodium <dbl>, total_carb <dbl>,\n## #   fiber <dbl>, sugar <dbl>, protein <dbl>, vit_a <dbl>, vit_c <dbl>,\n## #   calcium <dbl>, salad <chr>, id <int>, interval <fct>\ndf <- equal_count(df=fastfood, vble=calories, n_int=10, frac=0)\nggplot(df[[2]], aes(interval)) +\n  geom_histogram(stat=\"count\", color=\"blue\", fill=\"lightblue\") +\n  facet_grid(cols = vars(restaurant))\nfastfood$class <- as.factor(fastfood$restaurant)\nfastfood$class <- as.numeric(fastfood$class)\ngg_coplot(fastfood, x=cal_fat, y=calories, faceting=class, number_bins=8, overlap=0,\nylabel = \"Calories\", xlabel = \"Calories from fat\", loess_family = \"gaussian\", size = 2)\nggplot(data=fastfood, aes(x=trans_fat, y=sat_fat)) +\n  geom_point() +\n  geom_smooth()\ngg_coplot(fastfood, x=sat_fat, y=trans_fat, faceting=total_fat, number_bins=9, overlap=0,\nylabel = \"Saturated Fat\", xlabel = \"Trans Fat\", loess_family = \"gaussian\", size = 2)\ngg_coplot(fastfood, x=cal_fat, y=total_fat, faceting=calories, number_bins=9, overlap=0,\nylabel = \"Total Fat\", xlabel = \"Calories from Fat\", loess_family = \"gaussian\", size = 2)\ngg_pt(df=fastfood, vble=calories, taus = c(1),\nxlabel = \"Calories\", ylabel = \"Valores transformados\", color = \"red\")\nggplot(fastfood, aes(sample=calories)) +\n  stat_qq(color='blue')\ngg_pt(df=fastfood, vble=calories, taus = c(-0.5, -1, 0, 1.2),\nxlabel = \"Calories\", ylabel = \"Valores transformados\",\nnrow = 2, color = \"red\")\ngg_pt(df=fastfood, vble=calories, taus = c(-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1),\nxlabel = \"Calories\", ylabel = \"Valores transformados\",\nnrow = 3, color = \"blue\")\nrfData <-\nfastfood %>%\nmutate(meanFat=mean(total_fat), res=total_fat-meanFat)\ngg_rf(df=rfData, vble=total_fat, fitted=meanFat, res=res, color=\"red\")\nggplot(data=fastfood, aes(x=sat_fat, y=total_fat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  stat_poly_eq(formula = y ~ x, \n               aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")), \n               parse = TRUE)\nnewRfData <-\nfastfood %>%\nmutate(meanSF=mean(sat_fat), res=total_fat-2.43*meanSF)\ngg_rf(df=newRfData, vble=total_fat, fitted=2.43*meanSF, res=res, color=\"red\")\ndataCoplot <- make_coplot_df(df=fastfood, vble=calories, number_bins=9, overlap=0)\ndata1 <- dataCoplot$df_expanded\ndata2 <- dataCoplot$intervals\ncoplot(total_fat ~ cal_fat | interval, data = data1, col=\"blue\")\n\ngg_coplot(fastfood, x=cal_fat, y=total_fat, faceting=calories, number_bins=9, overlap=0,\nylabel = \"Total Fat\", xlabel = \"Calories from Fat\", loess_family = \"gaussian\", size = 2)\nggplot(fastfood, aes(sample=calories, color = restaurant)) + \n  stat_qq() + \n  stat_qq_line()\ngg_quantiles(df=fastfood, vble=calories, group=restaurant, color=\"red\") +\n  theme(panel.spacing = unit(2, \"lines\")) +\n  theme_bw()\ngg_tmd(df=fastfood, vble=calories, group=restaurant, color=\"red\")\ndata(\"environmental\", package = \"ggcleveland\")\ndata(\"environmental\")\nspm(environmental[2:5], smooth=FALSE)\nenvironmental1 <- environmental\nenvironmental1$ozono <- environmental1$ozono^(1/3)\ncoplot(ozono ~ radiacion | temperatura * viento, data=environmental1, number=4, overlap=0.4, xlab = \"Solar radiation\", ylab = \"Cube Root Ozone\", col = \"blue\", bg = \"pink\", pch=21)\ndata_coplot <- make_coplot_df(df=environmental1, vble=temperatura, number_bins=4, overlap=0.4)\ndf1 <- data_coplot$df_expanded\ndf1$group <- df1$interval\ndata_coplot <- make_coplot_df(df=df1, vble=viento, number_bins=4,overlap=0.4)\ndf2 <- data_coplot$df_expanded\ndf2$class1 <- as.factor(df2$interval)\ndf2$class1 <- as.numeric(df2$class1)\ndf2$class2 <- as.factor(df2$group)\ndf2$class2 <- as.numeric(df2$class2)\ndf2$class <- df2$class1*10+df2$class2\nclassDf <- data.frame(temp_interval=df2$group, wind_interval=df2$interval, class_label=df2$class)\nclassDf <- unique(classDf)\nshow(classDf)##                    temp_interval           wind_interval class_label\n## 1   temperatura = [56.50, 75.50]   viento = [2.05, 8.85]          11\n## 6   temperatura = [68.50, 81.50]   viento = [2.05, 8.85]          12\n## 19  temperatura = [76.50, 86.50]   viento = [2.05, 8.85]          13\n## 38  temperatura = [81.50, 97.50]   viento = [2.05, 8.85]          14\n## 66  temperatura = [56.50, 75.50]  viento = [7.15, 10.55]          21\n## 79  temperatura = [68.50, 81.50]  viento = [7.15, 10.55]          22\n## 97  temperatura = [76.50, 86.50]  viento = [7.15, 10.55]          23\n## 117 temperatura = [81.50, 97.50]  viento = [7.15, 10.55]          24\n## 135 temperatura = [56.50, 75.50]  viento = [8.95, 12.25]          31\n## 157 temperatura = [68.50, 81.50]  viento = [8.95, 12.25]          32\n## 177 temperatura = [76.50, 86.50]  viento = [8.95, 12.25]          33\n## 195 temperatura = [81.50, 97.50]  viento = [8.95, 12.25]          34\n## 206 temperatura = [56.50, 75.50] viento = [10.65, 20.95]          41\n## 231 temperatura = [68.50, 81.50] viento = [10.65, 20.95]          42\n## 251 temperatura = [76.50, 86.50] viento = [10.65, 20.95]          43\n## 266 temperatura = [81.50, 97.50] viento = [10.65, 20.95]          44\ngg_coplot(df2, x=radiacion, y=ozono, faceting=class, number_bins=16, overlap=0, show_intervals = FALSE,\nylabel = \"Cube Root Ozone\", xlabel = \"Solar radiation\", loess_family = \"gaussian\", size = 2)"},{"path":"neural-network-tools-in-r.html","id":"neural-network-tools-in-r","chapter":"60 Neural network tools in R","heading":"60 Neural network tools in R","text":"Yuan Wang","code":"\n#install.packages(\"NeuralNetTools\")\n#install.packages(\"nnet\")\n#install.packages(\"RSNNS\")\n#install.packages(\"neuralnet\")\n\nlibrary(\"NeuralNetTools\")\nlibrary(\"RSNNS\")\nlibrary(\"nnet\")\nlibrary(\"neuralnet\")"},{"path":"neural-network-tools-in-r.html","id":"introduction-9","chapter":"60 Neural network tools in R","heading":"60.1 Introduction","text":"Deep learning plays key role today’s data science, given advance accessible computing resources algorithm’s performance versatility. However, building efficient neural network requires careful architecture\ndesign crucial improving output accuracy network’s\nprediction. Since neural network designed train extremely large\ndatasets, ’s often difficult come ideas fine tuning \nmodel. Thus, providing visual representation allows us inspect \nstructure clearly. help R packages NeuralNetTools, \ncan visualize network’s structure, evaluate variable importance \nconduct sensitivity analysis, makes easier interpret \ngives insights fine-tuning strategies.","code":""},{"path":"neural-network-tools-in-r.html","id":"packages-will-be-introduced","chapter":"60 Neural network tools in R","heading":"60.1.1 Packages will be introduced:","text":"Basics: Neuralnettools, nnet, Neuralnet, RSNNS,Additional: keras","code":""},{"path":"neural-network-tools-in-r.html","id":"some-background-knowledge-of-neural-networks","chapter":"60 Neural network tools in R","heading":"60.1.2 Some background knowledge of neural networks:","text":"tutorial focuses supervised learning problems goal \nneural network map inputs given labels. supervised\nlearning setting, datasets contain: 1. Observations: - data\ncollected trained testedInputs: random observations standard normal distributionInputs: random observations standard normal distributionInput variables (X’s regression): features may represent \nlabelsInput variables (X’s regression): features may represent \nlabelsOutputs (Y’s regression): predictions targeted labels using\nlinear combinations input variables additional random\ncomponents (scaled 0 1 using softmax function)Outputs (Y’s regression): predictions targeted labels using\nlinear combinations input variables additional random\ncomponents (scaled 0 1 using softmax function)","code":""},{"path":"neural-network-tools-in-r.html","id":"data-pre-processing","chapter":"60 Neural network tools in R","heading":"60.1.3 Data pre-processing","text":"","code":""},{"path":"neural-network-tools-in-r.html","id":"common-approach-for-deep-learning","chapter":"60 Neural network tools in R","heading":"60.1.3.1 Common approach for deep learning:","text":"Normalize input variables (inputs —> X’s)Normalize input variables (inputs —> X’s)Standardize response variables (predictions/fitted values —>\nY’s)Standardize response variables (predictions/fitted values —>\nY’s)images, also need flattened new shape.images, also need flattened new shape.purpose pre-processing data standardize data make\nrange 0 1. demonstrate approach, use MNIST\ndataset consists 28 x 28 grayscale images handwritten\ndigits.Loading well-known neural net dataset MNIST requires package keras R’s interface \nKeras, widely used high-level neural networks API built python. Note R’s\nminiconda also required TensorFlow installed “install_tensorflow()” prevent loading errors. Datasets neural networks keras already standardized, still need extra step reshaping .","code":""},{"path":"neural-network-tools-in-r.html","id":"reshape","chapter":"60 Neural network tools in R","heading":"60.1.3.2 Reshape","text":"Neural networks heavily used computer vision datasets \nimages 3-dimensional array (images,width,height) grayscale\nvalues. first need flatten 3-d array matrix \nreshaping single dimension. MNIST images 28x28 array\nflatten vector length 784. , use keras\nfunction array_reshape(data, newshape). “data” parameter \narray-like “newshape” parameter integer array.\n’s integer, say n, “data” shaped matrix n rows\n#columns = size(data)/n. ’s array, compatible\noriginal 3-d array, size(newshape) = size(data). Keras provides array_shape() function reshaping. MNIST data, reshaping can accomplished “array_reshape(x_train, c(nrow(x_train), 784))”.goal neural network correctly identify written\ndigit image. Since tutorial focuses visualization, \ncan find information code train MNIST using keras \nhttps://rdrr.io/cran/keras/f/vignettes/index.Rmd.","code":""},{"path":"neural-network-tools-in-r.html","id":"visualization-of-neural-networks","chapter":"60 Neural network tools in R","heading":"60.2 Visualization of Neural Networks","text":"","code":""},{"path":"neural-network-tools-in-r.html","id":"neuralnettools-package","chapter":"60 Neural network tools in R","heading":"60.2.1 Neuralnettools package","text":"set visualization analysis tools used interpret \nstructure neural net models. main package plotting \nneural interpretation diagram (NID) nodes, layers edges, \nhelp visually understand architecture training progress. \nvisualization can give insights fine tune \nmodel.","code":""},{"path":"neural-network-tools-in-r.html","id":"four-core-functions-of-neuralnettools","chapter":"60 Neural network tools in R","heading":"60.2.2 Four core functions of NeuralNetTools","text":"plotnet: plot visualizationplotnet: plot visualizationgarson & olden: evaluate importance input variablesgarson & olden: evaluate importance input variableslekprofile: conduct sensitivity analysis (sensitive \ninput variable?)lekprofile: conduct sensitivity analysis (sensitive \ninput variable?)functions built upon nnet, neuralnet, RSNNS, caret.","code":""},{"path":"neural-network-tools-in-r.html","id":"neural-interpretation-diagram-nid","chapter":"60 Neural network tools in R","heading":"60.2.3 Neural Interpretation Diagram (NID)","text":"Neural interpretatiion diagram (NID) graph representation \nnodes edges divided vertical layers. stages training \nnetwork can seen horizontally. Nodes represent three types \nvariables underlying neural network 1. Input variables (1st\nlayer) 2. Hidden variables (layers 1st last layers) 3.\nOutput variables (last layer)nodes called units neural networks. connection \npair nodes like directed edge node former layer\npointed node next layer. represent existence \ncontribution predecessor node next node. example, \nconnection node x node y, x included linear\ncombination y.diagram illustrates idea feedforward neural network \ninformation moves one direction horizontal\ndirection NID. ’s best illustrated actual NID.Load data train dataset using three different\nfunctions.simple dataset neuraldat used purpose \ndemonstration. Summaries training results assigned “mod#”\nvariable used arguments visualization later.Example: neuraldat (normalized dataset NerualNetTools). contains -\n2000 rows observations - 3 input variables (X1, X2, X3) - 5\ncolumns two response variables (Y1 Y2)(Note: package loading Rcpp error occurs, try update.packages() \nupdate packages r environment)can plot NIDs models using plotnet:Notice run models several time, run give \ndifferent result thus different NID plot. difference \ndirection, color thickness connections discussed\ndetails later.","code":"\nset.seed(123)\n\nx <- neuraldat[, c(\"X1\", \"X2\", \"X3\")]\ny <- neuraldat[, \"Y1\"]\n\nmod1 <- mlp(x, y, size = 5)\n\nmod2 <- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)\n\nmod3 <- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)## # weights:  26\n## initial  value 102.608189 \n## iter  10 value 0.420687\n## iter  20 value 0.230286\n## iter  30 value 0.161667\n## iter  40 value 0.042549\n## iter  50 value 0.019999\n## iter  60 value 0.016232\n## iter  70 value 0.013683\n## iter  80 value 0.011934\n## iter  90 value 0.011545\n## iter 100 value 0.010132\n## final  value 0.010132 \n## stopped after 100 iterations\nplotnet(mod1)\nplotnet(mod2)\nplotnet(mod3)"},{"path":"neural-network-tools-in-r.html","id":"when-to-terminate-the-training","chapter":"60 Neural network tools in R","heading":"60.2.3.1 When to terminate the training","text":"Running nnet() output list loss tenth iteration \n100. can see traning stops error 0.010132\ntarget (ground truth) fitted value. \nsatisfy default absolute tolerance fitting criterion, \ntraining reaches default maximum number iterations 100 \nstops.words, training process stop two conditions:1. loss smaller fitting criterion, or2. reaches maximum number iterations allowed.can customize two specifying tolerance maxit nnet().Clearly see outputs mod4 runs 200 iterations \nstopped loss 0.002367 reach fitting\ncriterion, mod5, specifies absolute tolerance 0.01 \nmaximum 300 iterations, converges final loss value \n0.009917 terminates 110 iterations (use 300\niterations fitting criterion satisfied).","code":"\nmod4 <- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5, maxit=200)## # weights:  26\n## initial  value 74.693524 \n## iter  10 value 0.458543\n## iter  20 value 0.180504\n## iter  30 value 0.030452\n## iter  40 value 0.025113\n## iter  50 value 0.021237\n## iter  60 value 0.017340\n## iter  70 value 0.009561\n## iter  80 value 0.005930\n## iter  90 value 0.005084\n## iter 100 value 0.004484\n## iter 110 value 0.004201\n## iter 120 value 0.004032\n## iter 130 value 0.003599\n## iter 140 value 0.002484\n## iter 150 value 0.002059\n## iter 160 value 0.002014\n## iter 170 value 0.002009\n## iter 180 value 0.001882\n## iter 190 value 0.001719\n## iter 200 value 0.001654\n## final  value 0.001654 \n## stopped after 200 iterations\nmod5 <- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5, maxit=300, abstol=0.01)## # weights:  26\n## initial  value 95.238709 \n## iter  10 value 0.639464\n## iter  20 value 0.111136\n## iter  30 value 0.034965\n## iter  40 value 0.027499\n## iter  50 value 0.022252\n## iter  60 value 0.017605\n## iter  70 value 0.014490\n## iter  80 value 0.011656\n## final  value 0.009720 \n## converged"},{"path":"neural-network-tools-in-r.html","id":"further-explaination-of-the-three-model-functions-used-above","chapter":"60 Neural network tools in R","heading":"60.2.3.2 Further explaination of the three model functions used above:","text":"“mlp”:multilayer perceptron (MLP), fully connected feedforward\nnetworks (probably common network architecture practice)multilayer perceptron (MLP), fully connected feedforward\nnetworks (probably common network architecture practice)useful options:\nhiddenActFunc: activation function hidden units\n(popular choice: ReLU)\noutputActFunc: activation function output units\n(popular choice: softmax)\ninputsTest: matrix testing set examine fitting\nmodel\npruneFunc & pruneFunParams: pruning function parameters\n(explained later)\nsize: specifies number units hidden layers.\nusing multiple hidden layers, specify size argument\nvector whose length equals number layers \n-th element vector number units -th\nhidden layer.\nex. mlp(x, y, size = c(5, 6)) –> two hidden layers \n1st layer 5 units 2nd layer 6 units.\n\nmod6 <- mlp(x, y, size = c(5, 6))\n# visualize model using plotnet\nplotnet(mod6)\n\nuseful options:hiddenActFunc: activation function hidden units\n(popular choice: ReLU)hiddenActFunc: activation function hidden units\n(popular choice: ReLU)outputActFunc: activation function output units\n(popular choice: softmax)outputActFunc: activation function output units\n(popular choice: softmax)inputsTest: matrix testing set examine fitting\nmodelinputsTest: matrix testing set examine fitting\nmodelpruneFunc & pruneFunParams: pruning function parameters\n(explained later)pruneFunc & pruneFunParams: pruning function parameters\n(explained later)size: specifies number units hidden layers.\nusing multiple hidden layers, specify size argument\nvector whose length equals number layers \n-th element vector number units -th\nhidden layer.\nex. mlp(x, y, size = c(5, 6)) –> two hidden layers \n1st layer 5 units 2nd layer 6 units.\n\nmod6 <- mlp(x, y, size = c(5, 6))\n# visualize model using plotnet\nplotnet(mod6)\nsize: specifies number units hidden layers.using multiple hidden layers, specify size argument\nvector whose length equals number layers \n-th element vector number units -th\nhidden layer.ex. mlp(x, y, size = c(5, 6)) –> two hidden layers \n1st layer 5 units 2nd layer 6 units.“neuralnet”:also supports multi-layer networks specifying “hidden”\nargument (just like “size” argument mlp)flexible terms setting loss activation functionBut provide minimal visual interpretation network\nbasic structure mainly illustrates connections\nlayersUsing neuralnet multilayer:Ex. three hidden layers 3 units, 5 units 7 units respectively.“nnet(x, …)”Provides customization visualization demonstrate variable\nimportance reflected units edgesProvides customization visualization demonstrate variable\nimportance reflected units edgesBut supports feedforward neural networks single hidden\nlayerBut supports feedforward neural networks single hidden\nlayerSyntax:\nnnet(formula, data, weights, …, subset, na.action, contrasts =\nNULL)\nnnet(x, y, weights, size, Wts, mask, linout = FALSE, entropy =\nFALSE, softmax = FALSE, censored = FALSE, skip = FALSE, rang = 0.7,\ndecay = 0, maxit = 100, Hess = FALSE, trace = TRUE, MaxNWts = 1000,\nabstol = 1.0e-4, reltol = 1.0e-8, …)Syntax:nnet(formula, data, weights, …, subset, na.action, contrasts =\nNULL)nnet(x, y, weights, size, Wts, mask, linout = FALSE, entropy =\nFALSE, softmax = FALSE, censored = FALSE, skip = FALSE, rang = 0.7,\ndecay = 0, maxit = 100, Hess = FALSE, trace = TRUE, MaxNWts = 1000,\nabstol = 1.0e-4, reltol = 1.0e-8, …)useful nnet options:useful nnet options:Note differe “abstol” “reltol” \nAbs(prediction - target) <= absTol, Abs(preiction - target) <=\nrelTol * Max(Abs(prediction), Abs(target))","code":"\nmod6 <- mlp(x, y, size = c(5, 6))\n# visualize the model using plotnet\nplotnet(mod6)\nmod7 <- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = c(3, 5, 7))\n# visualize the model using plotnet\nplotnet(mod7)"},{"path":"neural-network-tools-in-r.html","id":"neuralnettoolsplotnet","chapter":"60 Neural network tools in R","heading":"60.2.4 NeuralNetTools::plotnet","text":"plotting function visualize neural networks \nmodel. plot diagram net structure (NID), support \ncustomization. Features NID includes:Examples plotnet’s advanced customization:\nPrimary skip layer network:\nskip bypasses hidden layers, demonstrate logical\nconnections input layer output layer\n\nPrune connections nodes (RSNNS package):\nidea pruning remove connections nodes \nnetwork, contribute netwaork’s\nperformance predicting targets (hteir weights close\nzero). improves network’s performance, since\nreduces number estimations wieghts.\n\nExamples plotnet’s advanced customization:Primary skip layer network:\nskip bypasses hidden layers, demonstrate logical\nconnections input layer output layer\nPrimary skip layer network:skip bypasses hidden layers, demonstrate logical\nconnections input layer output layerPrune connections nodes (RSNNS package):\nidea pruning remove connections nodes \nnetwork, contribute netwaork’s\nperformance predicting targets (hteir weights close\nzero). improves network’s performance, since\nreduces number estimations wieghts.\nPrune connections nodes (RSNNS package):idea pruning remove connections nodes \nnetwork, contribute netwaork’s\nperformance predicting targets (hteir weights close\nzero). improves network’s performance, since\nreduces number estimations wieghts.Code:RSNNSs provides several prune functions, including MagPruning,\nOptimalBrainSurgeon, OptimalBrainDamage, Noncontributing_Units, \nSkeletonization. can review original OBS paper \nhttps://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf,\ncan give perspectives pruning.pruneFuncParames arguments pass prune function. \nexample, code , parameters uses :","code":"\n# skip\nmod1 <- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)## # weights:  26\n## initial  value 37.536429 \n## iter  10 value 3.473930\n## iter  20 value 0.300459\n## iter  30 value 0.110746\n## iter  40 value 0.075740\n## iter  50 value 0.029781\n## iter  60 value 0.021122\n## iter  70 value 0.012062\n## iter  80 value 0.011070\n## iter  90 value 0.009380\n## iter 100 value 0.006579\n## final  value 0.006579 \n## stopped after 100 iterations\nplotnet(mod1, skip = TRUE) \n# prune\n# pruned model using code from RSSNS pruning demo\npruneFuncParams <- list(max_pr_error_increase = 10.0,\n                        pr_accepted_error = 1.0,\n                        no_of_pr_retrain_cycles = 1000,\n                        min_error_to_stop = 0.01, \n                        init_matrix_value = 1e-6,\n                        input_pruning = TRUE, \n                        hidden_pruning = TRUE)\n\nmod <- mlp(x, y, size = 5, pruneFunc = \"OptimalBrainSurgeon\",\n           pruneFuncParams = pruneFuncParams)\n\nplotnet(mod, rel_rsc = c(3, 8))\nplotnet(mod, prune_col = \"lightblue\", rel_rsc = c(3, 8))"},{"path":"neural-network-tools-in-r.html","id":"alternative-to-nid","chapter":"60 Neural network tools in R","heading":"60.2.5 Alternative to NID:","text":"neural network often complex large numbers layers, nodes,\nedges, makes difficult interpret. case, can\nuse second third main functions packages, garson \nolden alternatives deconstructing weights interpret\nvariable importance.named two approaches:Garson’s\nanalyzes relative importance variable magnitude.\nLimitation:\nSometimes responses determined\ncan evaluate neural nets one hidden one\noutput node\n\nGarson’sIt analyzes relative importance variable magnitude.Limitation:\nSometimes responses determined\ncan evaluate neural nets one hidden one\noutput node\nLimitation:Sometimes responses determinedSometimes responses determinedIt can evaluate neural nets one hidden one\noutput nodeIt can evaluate neural nets one hidden one\noutput nodeOlden’s\nflexible approach measuring variable importance \nsummed product input-hidden hidden-output onnection weights\ninput output\nAdvantages:\nmaintain magnitude sign. Thus, canceling effect\nresulted different signs incoming coming\nconnections node maintained.\ncan evaluate multiple hidden layers responses.\n\nOlden’sA flexible approach measuring variable importance \nsummed product input-hidden hidden-output onnection weights\ninput outputAdvantages:\nmaintain magnitude sign. Thus, canceling effect\nresulted different signs incoming coming\nconnections node maintained.\ncan evaluate multiple hidden layers responses.\nAdvantages:maintain magnitude sign. Thus, canceling effect\nresulted different signs incoming coming\nconnections node maintained.can evaluate multiple hidden layers responses.bar chars importance input variable provides \nstraightforward visual presentation influential variable\noutputs.","code":"\ngarson(mod1)\ngarson(mod2)\ngarson(mod5)\nolden(mod1)\nolden(mod2)\nolden(mod5)"},{"path":"neural-network-tools-in-r.html","id":"sensitivity-analysis","chapter":"60 Neural network tools in R","heading":"60.2.6 Sensitivity analysis:","text":"Sensitivity analysis shows sensitive input variable (feature) \nnecessary us understand relationship influence\ninput parameter outputs.","code":""},{"path":"neural-network-tools-in-r.html","id":"lekprofile","chapter":"60 Neural network tools in R","heading":"60.2.6.1 Lekprofile","text":"function uses Lek’s profile method evaluate behavior \noutputs across different values input variables. ’s generic \ncan applied predictive method.Two options setting constant values unevaluated explanatory\nvariables:vector calues 0-1 indicating quantiles\ninput variables hold constant.ORA single value indicating number groups.\nkmeans\nclustering used value greater 1.plots show influence varying input variable \ndifferent quantiles value response outputs, holding\ninput variables constant. plots lines, x-axis\nrepresents values input variables, y-axis values \nresponse, different lines different values input variables.\nexample, first plot shows X1 X3 sensitive \nX2 since changes values result different response\nvalues respectively. last plot shows X1 sensitive since\nvalue changes dramatically across groups.","code":"\nlekprofile(mod3)\nlekprofile(mod3, group_show = TRUE)\nlekprofile(mod3, group_vals = 6)\nlekprofile(mod3, group_vals = 6, group_show = TRUE)"},{"path":"neural-network-tools-in-r.html","id":"miscellaneous","chapter":"60 Neural network tools in R","heading":"60.3 Miscellaneous","text":"","code":""},{"path":"neural-network-tools-in-r.html","id":"other-useful-functions-from-the-package","chapter":"60 Neural network tools in R","heading":"60.3.0.1 Other useful functions from the package:","text":"“neuralweights”: retrieve model weights\nReturn two-element summary list network’s structure\nvector number nodes layer\nnamed list model weights\n\nused implicitly main functions. can use \nexplicitly numerical summary model weights \nneeded.\n“neuralweights”: retrieve model weightsReturn two-element summary list network’s structure\nvector number nodes layer\nnamed list model weights\nReturn two-element summary list network’s structurea vector number nodes layera vector number nodes layera named list model weightsa named list model weightsIt used implicitly main functions. can use \nexplicitly numerical summary model weights \nneeded.used implicitly main functions. can use \nexplicitly numerical summary model weights \nneeded.“pred_sens”: retrieve predicted values Lek Profile method\nReturn list predictions whose element corresponds \ngroup Lek Profile method.\nused implicitly “lefprofile”\n“pred_sens”: retrieve predicted values Lek Profile methodReturn list predictions whose element corresponds \ngroup Lek Profile method.Return list predictions whose element corresponds \ngroup Lek Profile method.used implicitly “lefprofile”used implicitly “lefprofile”","code":""},{"path":"neural-network-tools-in-r.html","id":"load-popular-datasets-for-neural-networks-from-rs-keras-package","chapter":"60 Neural network tools in R","heading":"60.3.0.2 Load popular datasets for neural networks from R’s keras package:","text":"https://rdrr.io/cran/keras/f/vignettes/index.Rmd","code":""},{"path":"neural-network-tools-in-r.html","id":"reference-3","chapter":"60 Neural network tools in R","heading":"60.4 Reference:","text":"<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6262849/><https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6262849/>https://cran.r-project.org/web/packages/NeuralNetTools/NeuralNetTools.pdfhttps://cran.r-project.org/web/packages/NeuralNetTools/NeuralNetTools.pdfhttps://cran.r-project.org/web/packages/nnet/nnet.pdfhttps://cran.r-project.org/web/packages/nnet/nnet.pdfhttps://cran.r-project.org/web/packages/RSNNS/RSNNS.pdfhttps://cran.r-project.org/web/packages/RSNNS/RSNNS.pdfhttps://www.ra.cs.uni-tuebingen.de/SNNS/UserManual/node308.htmlhttps://www.ra.cs.uni-tuebingen.de/SNNS/UserManual/node308.htmlhttps://www.rdocumentation.org/packages/RSNNS/versions/0.4-14/topics/mlphttps://www.rdocumentation.org/packages/RSNNS/versions/0.4-14/topics/mlp","code":""},{"path":"readable-plots.html","id":"readable-plots","chapter":"61 Readable Plots","heading":"61 Readable Plots","text":"Jiawen Zhou","code":""},{"path":"readable-plots.html","id":"introduction-10","chapter":"61 Readable Plots","heading":"61.0.1 Introduction","text":"ggplot2 already lot beautiful comprehensive cheat sheets. focus collection tricks make plots readable, sizing, arrangement positioning plots, axis, labels, legends. also include ways customize plots unique. hope helpful R users find easier fix plot issues make beautiful meaningful visualizations.","code":""},{"path":"readable-plots.html","id":"axis","chapter":"61 Readable Plots","heading":"61.0.2 Axis","text":"Axis LabelsFirst part, consider label text sizes. helpful way make faceted plots label-rich plots less crowded.example HW2, utilizing ames dataset openintro package. observe faceting Neighborhood, plot labels x axis crowded overlap. solution make easier readers read plot, shrink size labels. change theme() function setting axis.text.x text size 5.Another example HW2 Dog name plots. 30 names y axis, better shrink text size leave readable gaps labels. another approach labels unchanged size different arrangements.way, font size unchanged, name can read clearly.Another way make axis labels readable change rotation labels. example, want draw bar plot popular dog names.names readable text size gaps unchanged. turning labels 90 degrees, x axis readable name labels.Axis contentsSometimes default axis labels fail present appropriate information, need way change axis contents specific needs.Example HW1 histogram. change x axis labels age y axis labels count.want axis labels, can theme(). Although labels necessary cases important information readers .want x y axis range. use coor_equal()Axis stylesIf circumstances, graph needs different styles axis labels, check examples. Data fastfood openintro package. example HW.changed x labels bold, pink color, angled 45 degrees. Y labels italic, green color, smaller sized. Axis lines colored blue solid line.","code":"\nlibrary(openintro)\names %>%\n  group_by(Neighborhood) %>%\n  ggplot(aes(x = area , y = price))  + \n  geom_point()  +\n  facet_wrap(~Neighborhood) + \n  theme(axis.text.x = element_text(size = 5))\n# 30 most popular dog names \ndogname <- seattlepets %>%\n  select(animal_name,species) %>%\n  filter(species == \"Dog\")  %>%\n  count(animal_name) %>%\n  arrange(desc(n)) %>% \n  filter(!is.na(animal_name)) \nggplot(dogname[1:50,], aes(x = n, y = fct_reorder(animal_name, n))) +\n  geom_point(color = \"blue\") +\n  scale_y_discrete(guide = guide_axis(n.dodge=2)) +\n  ggtitle(\"50 most popular dog names\") +\n  ylab(\"\") +\n  xlab(\"count\")\nggplot(dogname[1:50,], aes(x = fct_rev(fct_reorder(animal_name, n)), y = n)) +\n  geom_col() +\n  xlab('Dog Names') +\n  ggtitle('Top 50 most common dog names with number of counts') +\n  theme(axis.text.x = element_text(angle = 90))\nggplot(mtl, aes(age)) + \n  geom_histogram(color = \"blue\", fill = \"lightblue\", binwidth = 5,\n                 boundary = 52.5) +\n  scale_x_continuous(breaks = seq(42.5, 77.5, 5)) + \n  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, 2))\nggplot(mtcars, aes(am)) +\n  geom_bar() +\n  theme(axis.text.x = element_blank())\nggplot(mtcars, aes(disp, disp+rnorm(nrow(mtcars), sd=20)))+geom_point()+\n  xlim(c(70,500))+ylim(c(70,500))+\n  coord_equal() +\n  ylab('Noisy disp')\nggplot(fastfood, aes(x = calories, y = restaurant))+geom_boxplot() +\n  theme(axis.text.x = element_text(face=\"bold\", color=\"pink\", \n                           size=10, angle=45),\n          axis.text.y = element_text(face=\"italic\", color=\"green\", \n                           size=7),\n        axis.line = element_line(colour = \"blue\", \n                      size = 1, linetype = \"solid\"))"},{"path":"readable-plots.html","id":"title","chapter":"61 Readable Plots","heading":"61.0.3 Title","text":"Start adding title ggplot2.Two ways produce title.Like axis labels, can customize title looks like. Change text size, bold italic, horizontal vertical positions.","code":"\ng = ggplot(mtcars, aes(am)) +\n  geom_bar()\ng + ggtitle(\"Barplot of Transmission of Cars\")\ng + labs(title=\"Barplot of Transmission of Cars\")\ng + ggtitle(\"Barplot of Transmission of Cars\") +\n  theme(plot.title = element_text(size=18, face='italic',\n                                  hjust=0.5))"},{"path":"readable-plots.html","id":"legend","chapter":"61 Readable Plots","heading":"61.0.4 Legend","text":"Legend used many cases. Sometimes annoying see legend takes much space unneccesary information. examples might want check .default plot mtcars.changed several variables legend. changed title, style, background edge color, alignment, position, keys’ filling.","code":"\ng = ggplot(mtcars, aes(mpg,disp,color=factor(cyl))) +\n  geom_point()\ng\ng + theme(legend.title = element_text(colour=\"Red\", size=16, face=\"bold\"),\n          legend.background = element_rect(color = 'steelblue'),\n          legend.title.align = 1,\n          legend.position =  'bottom',\n          legend.direction = 'horizontal',\n          legend.key = element_rect(fill='pink')) +\n  scale_color_discrete(name = \"Cyl\")"},{"path":"readable-plots.html","id":"background-1","chapter":"61 Readable Plots","heading":"61.0.5 Background","text":"times panel colors, grid lines, plot colors customized styles needs.","code":"\ng + theme(panel.background = element_rect(fill = 'grey40'),\n          panel.grid.major = element_line(color = 'light yellow', size=2),\n          panel.grid.minor = element_line(color = 'green', size = 0.5),\n          plot.background = element_rect(fill = 'slategray3'))"},{"path":"readable-plots.html","id":"theme","chapter":"61 Readable Plots","heading":"61.0.6 Theme","text":"Apart existing themes ggplot2, can get great built-themes ggthemes package.themes ggthemes packages theme_base, theme_calc, theme_clean, theme_excel, theme_excel_new, theme_few, theme_fivethirtyeight, theme_foundation, theme_gdocs, theme_hc, theme_igray, theme_map, theme_pander, theme_par, theme_solarized, theme_solid, theme_stata.also R packages designed themes use. packages include hrbrthemes, ggthemr, ggtech, ggdark.","code":"\nlibrary(ggthemes)\n\ncyls <- as.factor(mtcars$cyl)\nggplot(mtcars, aes(x = mpg, fill = cyls)) +\n  geom_density(alpha = 0.7) +\n  theme_economist() +\n  scale_fill_economist() +\n  theme(legend.position = \"top\")"},{"path":"readable-plots.html","id":"reference-4","chapter":"61 Readable Plots","heading":"61.0.7 Reference","text":"HW STAT 5702http://zevross.com/blog/2014/08/04/beautiful-plotting--r--ggplot2-cheatsheet-3/http://www.sthda.com/english/wiki/ggplot2-axis-ticks--guide--customize-tick-marks--labelshttps://ggplot2.tidyverse.org/referencehttp://sape.inf.usi.ch/quick-reference/ggplot2/colourhttps://r-charts.com/ggplot2/themes/","code":""},{"path":"introduction-to-exploratory-spatial-data-analysis-and-visualization-using-qgis.html","id":"introduction-to-exploratory-spatial-data-analysis-and-visualization-using-qgis","chapter":"62 Introduction to exploratory spatial data analysis and visualization using QGIS","heading":"62 Introduction to exploratory spatial data analysis and visualization using QGIS","text":"Yao Xiao","code":""},{"path":"introduction-to-exploratory-spatial-data-analysis-and-visualization-using-qgis.html","id":"concepts-and-definitions","chapter":"62 Introduction to exploratory spatial data analysis and visualization using QGIS","heading":"62.1 Concepts and definitions","text":"tutorial covers fundamental knowledge geospatial data analysis, way using analytical methods tools visualize, examine interact data involving location environment. unique important able identify spatial characteristics, processes, dependence data may otherwise ignored traditional exploratory data analysis. subject aims addressing interesting questions may useful research policy. examples.event question happen? extent/severity?event question happen? extent/severity?spatial relationship different places? connected, intersected, disjoint?spatial relationship different places? connected, intersected, disjoint?associations/differences places nearby/faraway?associations/differences places nearby/faraway?tutorial also introduces several QGIS skills. QGIS powerful tool advantages free open-source software made available multiple languages, operating systems versions. link installer downloads https://qgis.org/en/site/forusers/download.htmlWe begin learning two main types geographical data: vector raster data. Raster data made pixels grid cells. often represented “surface”. Satellite imagery can example raster data. purposes, just focus vector (shapefile) data include points, lines polygons.Point: data represent feature specify coordinates feature. Examples include schools, city halls, bike sharing stations, etc…Point: data represent feature specify coordinates feature. Examples include schools, city halls, bike sharing stations, etc…Line: data represent linear feature connects points. Examples include rivers, streets, trails, etc…Line: data represent linear feature connects points. Examples include rivers, streets, trails, etc…Polygon: data represent areas delineate boundaries. Examples include neighborhoods, cities, states, etc…Polygon: data represent areas delineate boundaries. Examples include neighborhoods, cities, states, etc…Another key concept awareness “distortions reality” introduced process mapmaking. try represent Earth’s surface two-dimensional space, map, must understand underlying processes conversion adjust setups data correctly represented map. Often times, shape earth approximated spheroid, datum specifies system defines position spheroid relative center Earth provides frame reference locations surface Earth. coordinates datum represented also depends projection, uses mathematical calculations systematically arrange/transform set coordinates formulate flat, 2-d map. Datum projection important determine degree way distortion real Earth’s surface affect visual representation measurement accuracy data.Sometimes interested learning central tendency distribution particular feature. example, given able visualize number bike shares different county map, may wonder central location event points spread . , need apply centrographic analysis, descriptive statistics summarizes point patterns. introduce two statistics: mean center standard deviational ellipse. point data, mean center computes mean coordinates longitude latitude points. standard deviational ellipse computes directional average distance points mean center along two axes, one goes maximum dispersion spatial distribution another perpendicular .","code":""},{"path":"introduction-to-exploratory-spatial-data-analysis-and-visualization-using-qgis.html","id":"data-setup-and-preparation","chapter":"62 Introduction to exploratory spatial data analysis and visualization using QGIS","heading":"62.2 Data setup and preparation","text":"Next, cover example step--step instructions explore use QGIS answer aforementioned research questions. download following files:ZCTA_2010Census_DP1.zip: shapefile data 2010 Census Demographic Profile US zipcode tabulation area US census website.\nLink: https://www2.census.gov/geo/tiger/TIGER2010DP1/ZCTA_2010Census_DP1.zip: shapefile data 2010 Census Demographic Profile US zipcode tabulation area US census website.\nLink: https://www2.census.gov/geo/tiger/TIGER2010DP1/nyu_2451_34509.zip: shapefile data New York City zipcode tabulation areas. dropdown menu, select “EPSG:4326 Shapefile”, click “Downloads”  “Original Shapefile”.\nLink: https://geo.nyu.edu/catalog/nyu_2451_34509nyu_2451_34509.zip: shapefile data New York City zipcode tabulation areas. dropdown menu, select “EPSG:4326 Shapefile”, click “Downloads”  “Original Shapefile”.\nLink: https://geo.nyu.edu/catalog/nyu_2451_34509NYPD Complaint Data Current (Year Date).zip: shapefile data New York City felony, misdemeanor, violation crimes reported New York City Police Department. Select “Export” click “Shapefile”. Also download data dictionary NYPD_Complaint_YTD_DataDictionary.xlsx attachment section page.\nLink: https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Current-Year--Date-/5uac-w243NYPD Complaint Data Current (Year Date).zip: shapefile data New York City felony, misdemeanor, violation crimes reported New York City Police Department. Select “Export” click “Shapefile”. Also download data dictionary NYPD_Complaint_YTD_DataDictionary.xlsx attachment section page.\nLink: https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Current-Year--Date-/5uac-w243We use datasets understand:\n* spatial relationship homeownership demographic characteristics zipcode level NYC?\n* spatial relationship different crimes NYC?\n* Can compare distributions central spatial locations features map?","code":""},{"path":"introduction-to-exploratory-spatial-data-analysis-and-visualization-using-qgis.html","id":"homeownership-in-relation-to-other-demographic-characteristics","chapter":"62 Introduction to exploratory spatial data analysis and visualization using QGIS","heading":"62.3 Homeownership in Relation to Other Demographic Characteristics","text":"First, need import datasets. open QGIS, select “Layer” – “Add Layer” – “Add Vector Layer”. window pop ask path datasets. import ZCTA_2010Census_DP1.zip nyu_2451_34509.zip software.Fig 1Note import nyu_2451_34509.zip, select layer imported. use shapefile “.shp” end. Click OK.Fig 2Second, recall datasets use particular datum Coordinate Reference System CRS. set CRS layers “EPSG:4326”.\nThird, since interested features NYC. want limit datasets include areas NYC. joining zipcode two datasets. layer New York City shapefile nyu_2451_34509 contains demographic characteristics ZCTA_2010Census_DP1 interested . select “Toolbox” (gear icon) menu, type “Join” click “Join attributes field value”. specify input layers join “zcta” “ZCTA5CE10”. Click “Run”.Fig 4Fig 5We want find proportion housing units owned vs. rented New York postcode area. take look “DP_TableDescriptions.xls” inside ZCTA_2010Census_DP1.zip understand meaning different feature names. find number owner-occupied housing units coded “DP0210002” total number housing units coded “DP0210001”. use ratio calculate proportion homeownership.Fig 6We right click joined layer go “Open Attribute Table” – “Open field Calculator”, enter expression, chose output type decimal number click “OK”.Fig 7Mapping density owner-occupied housing units shows clear pattern clustering: places significantly higher percentages owner-occupied housing units others. noticeable parts can observed map Staten Island, Long Island, majority Queens, southern region Brooklyn, northern region Bronx. areas map shaded red crimson, representing higher ratio number owner-occupied housing units total housing units. also want know African American Black population, socioeconomically disadvantaged average due presence explicit structural racism, lower rates homeownership races well. can easily find pattern following map. nearly regions higher percentage owner-occupied housing units mentioned , concentration African American Black population lower. Also, want investigate whether neighborhoods families average higher rates ownership need stable housing families may stronger individuals live alone. surely find conjecture plausible. Staten Island, eastern Queens, southern Brooklyn higher concentrations family households owner-occupied housing units.","code":""},{"path":"introduction-to-exploratory-spatial-data-analysis-and-visualization-using-qgis.html","id":"spatial-pattern-of-different-crimes","chapter":"62 Introduction to exploratory spatial data analysis and visualization using QGIS","heading":"62.4 Spatial Pattern of Different Crimes","text":"answer second question, need load crime dataset QGIS. purposes, want filter datasets felony crimes NY included. dataset loaded, open attribute table select “Advanced Filter (Expression)” dropdown menu. data dictonary, observe “law_cat_cd” categorizes different crime incidents enter following expression felony data selected. right click layer export file contains selected data using “Save Selected Features …”Fig 13Fig 14Import selected data new layer filter rows specific crimes interested make comparisons.Fig 18We can use “NUMPOINTS ROBBERY” attribute make similar choropleth map.\nNext, want compute mean center points. can visualize center selecting “Mean Coordinate(s)”\nvisualize standard deviational ellipse, install extra plugin. Go “Plugins” – “Manage Install Plugins” search “Standard Deviational Ellipse”. go “Vector” – “Standard Deviational Ellipse” choose right layer.completing steps, can create map shows mean centers standard deviational ellipses three different felony crimes.Fig 25From map, can compare contrast similarities differences central tendency overall spread spatial distributions. notice mean centers crimes related robbery, dangerous weapons, dangerous drugs close one another lie region Long Island City. may due high counts felony crimes northern Brooklyn Bronx means gravitate toward center two regions. mean center dangerous weapon crimes south two, suggesting higher proportion incidents related dangerous weapons occurred somewhere south Long Island City, Brooklyn. Comparing standard deviational ellipses allows us gain insights spread crimes . standard deviational ellipse robberies smallest suggests robberies concentrated around mean center. standard deviational ellipse dangerous weapons dangerous drugs also display different shapes. standard deviational ellipse dangerous drugs elongated vertical direction dangerous weapons elongated horizontal direction.","code":""},{"path":"base-r-tutorial.html","id":"base-r-tutorial","chapter":"63 Base R Tutorial","heading":"63 Base R Tutorial","text":"Siqin Shen","code":""},{"path":"base-r-tutorial.html","id":"overview-2","chapter":"63 Base R Tutorial","heading":"63.1 Overview","text":"Base R Cheatsheet/Tutorial, includes common Base R graphic functions parameters.use data SleepStudy Lock5withR package.","code":"\nSleepStudy <- Lock5withR::SleepStudy"},{"path":"base-r-tutorial.html","id":"section-1---functions","chapter":"63 Base R Tutorial","heading":"63.2 Section 1 - Functions","text":"","code":""},{"path":"base-r-tutorial.html","id":"histogram","chapter":"63 Base R Tutorial","heading":"63.2.1 Histogram","text":"","code":"\nhist(SleepStudy$GPA, breaks = 10)"},{"path":"base-r-tutorial.html","id":"barplot","chapter":"63 Base R Tutorial","heading":"63.2.2 Barplot","text":"Make sure data matrix vector plot barplot.easy way use table() function transform data.","code":"\nLarkOwl <- table(SleepStudy$LarkOwl)\nbarplot(LarkOwl, names.arg = \"Lark-Owl\", border = \"blue\" , col = \"grey\", horiz = FALSE)"},{"path":"base-r-tutorial.html","id":"scatter-plot","chapter":"63 Base R Tutorial","heading":"63.2.3 Scatter plot","text":"","code":"\nplot(SleepStudy$StressScore, SleepStudy$GPA)"},{"path":"base-r-tutorial.html","id":"dot-plot","chapter":"63 Base R Tutorial","heading":"63.2.4 Dot Plot","text":"dot plot just bar chart uses dots represent individual quanta. source: https://www.mvorganizing.org/--dot-plot-----scatter-plot/#:~:text=%20dot%20plot%20is%20just,height%20and%20one%20represented%20weight.","code":"\ntable_data <- table(SleepStudy$DepressionStatus, SleepStudy$LarkOwl)\ndotchart(table_data)"},{"path":"base-r-tutorial.html","id":"box-plot","chapter":"63 Base R Tutorial","heading":"63.2.5 Box Plot","text":"","code":"\nboxplot(PoorSleepQuality ~ DepressionStatus, data = SleepStudy, horizontal = FALSE, names = c(\"moderate\", \"normal\", \"severe\"), col = \"gold\")"},{"path":"base-r-tutorial.html","id":"mosaic-plot","chapter":"63 Base R Tutorial","heading":"63.2.6 Mosaic Plot","text":"Make sure data matrix vector plot mosaic plot.easy way use table() function transform data.","code":"\ntable_data <- table(SleepStudy$ClassYear, SleepStudy$NumEarlyClass)\nmosaicplot(table_data, color = TRUE)"},{"path":"base-r-tutorial.html","id":"density-plot-1","chapter":"63 Base R Tutorial","heading":"63.2.7 Density Plot","text":"Make sure transform data density plot density plot.easy way use density() function transform data.","code":"\ndensity_gpa <- density(SleepStudy$GPA)\nplot(density_gpa)"},{"path":"base-r-tutorial.html","id":"section-2---parameters","chapter":"63 Base R Tutorial","heading":"63.3 Section 2 - Parameters","text":"","code":""},{"path":"base-r-tutorial.html","id":"main","chapter":"63 Base R Tutorial","heading":"63.3.1 Main","text":"adds title graph.","code":"\nhist(SleepStudy$GPA, breaks = 10, main = \"GPA distribution\")"},{"path":"base-r-tutorial.html","id":"labels","chapter":"63 Base R Tutorial","heading":"63.3.2 Labels","text":"adds x-axis y-axis labels graph.","code":"\nhist(SleepStudy$GPA, breaks = 10, main = \"GPA distribution\", xlab = \"GPA\", ylab = \"Number of Students\")"},{"path":"base-r-tutorial.html","id":"add-color","chapter":"63 Base R Tutorial","heading":"63.3.3 Add Color","text":"","code":"\nhist(SleepStudy$GPA, breaks = 10, main = \"GPA distribution\", xlab = \"GPA\", ylab = \"Number of Students\",col.lab = \"light blue\", col = \"gold\")"},{"path":"base-r-tutorial.html","id":"point-shape","chapter":"63 Base R Tutorial","heading":"63.3.4 Point Shape","text":"","code":"\nplot(SleepStudy$StressScore, SleepStudy$GPA, main = \"Stress Score vs GPA\", pch = 6, xlab = \"StressScore\", ylab = \"GPA\",col.lab = \"light blue\")"},{"path":"base-r-tutorial.html","id":"legend-1","chapter":"63 Base R Tutorial","heading":"63.3.5 Legend","text":"","code":"\nplot(SleepStudy$StressScore, SleepStudy$GPA, main = \"Stress Score vs GPA\", col = SleepStudy$LarkOwl, pch = 6, xlab = \"StressScore\", ylab = \"GPA\",col.lab = \"light blue\")\nlegend(\"topleft\", legend = levels(SleepStudy$LarkOwl), col = 1:nlevels(SleepStudy$LarkOwl), pch = 6, title = \"LarkOwl\")"},{"path":"base-r-tutorial.html","id":"pointlabel-size","chapter":"63 Base R Tutorial","heading":"63.3.6 Point/label Size","text":"","code":"\nplot(SleepStudy$StressScore, SleepStudy$GPA, main = \"Stress Score vs GPA\", col = SleepStudy$LarkOwl, pch = 6, xlab = \"StressScore\", ylab = \"GPA\",col.lab = \"light blue\", cex.lab = 1.5, cex = 0.75)\nlegend(\"topleft\", legend = levels(SleepStudy$LarkOwl), col = 1:nlevels(SleepStudy$LarkOwl), pch = 6, title = \"LarkOwl\")"},{"path":"base-r-tutorial.html","id":"line-type","chapter":"63 Base R Tutorial","heading":"63.3.7 Line Type","text":"","code":"\nplot(density_gpa, main = \"GPA Density Distribution\", lty = 4, col = \"red\")"},{"path":"base-r-tutorial.html","id":"line-width","chapter":"63 Base R Tutorial","heading":"63.3.8 Line Width","text":"","code":"\nplot(density_gpa, main = \"GPA Density Distribution\", lty = 4, col = \"red\", lwd = 2)"},{"path":"base-r-tutorial.html","id":"section-3---others","chapter":"63 Base R Tutorial","heading":"63.4 Section 3 - Others","text":"Add best fit Lines scatter plotWe need get function line first.\ncan use Loess smoother loess() simulate line.source: https://dcgerard.github.io/stat234/base_r_cheatsheet.html","code":"\nloess_fit <- loess(GPA ~ StressScore, data = SleepStudy)\nxnew <- seq(min(SleepStudy$StressScore), max(SleepStudy$StressScore), length = 100)\nynew <- predict(object = loess_fit, newdata = data.frame(StressScore = xnew))\n\nplot(SleepStudy$StressScore, SleepStudy$GPA, main = \"Stress Score vs GPA\", col = SleepStudy$LarkOwl, pch = 6, xlab = \"StressScore\", ylab = \"GPA\",col.lab = \"light blue\", cex.lab = 1.5, cex = 0.75)\nlegend(\"topleft\", legend = levels(SleepStudy$LarkOwl), col = 1:nlevels(SleepStudy$LarkOwl), pch = 6, title = \"LarkOwl\")\nlines(xnew, ynew, col = \"blue\", lty = 1, lwd = 2)"},{"path":"base-r-tutorial.html","id":"facet-wrap","chapter":"63 Base R Tutorial","heading":"63.4.1 Facet Wrap","text":"","code":"\nwrap <- par(mfrow = c(2, 3))\n\nhist(SleepStudy$GPA, breaks = 10, main = \"GPA distribution\")\nbarplot(LarkOwl, names.arg = \"Lark-Owl\", border = \"blue\" , col = \"grey\", horiz = FALSE, main = \"Lark Owl Count\")\nplot(SleepStudy$StressScore, SleepStudy$GPA, main = \"Stress Score-GPA Relation\")\ndotchart(table_data, main = \"DepressionStatus-LarkOwl Relation\")\nboxplot(PoorSleepQuality ~ DepressionStatus, data = SleepStudy, horizontal = FALSE, names = c(\"moderate\", \"normal\", \"severe\"), col = \"gold\", main = \"PoorSleepQuality vs DepressionStatus\")\nmosaicplot(table_data, color = TRUE, main = \"ClassYear vs Num Early Class\")"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"introduction-to-models-and-prediction-evaluation","chapter":"64 Introduction to models and prediction evaluation","heading":"64 Introduction to models and prediction evaluation","text":"Yi DuanIn introduction, introduce profit model, logit model tree models. Moreover, use specific example show used R. compare models, introduce ROC evaluate predictions example.","code":"\nlibrary(ISLR)\nlibrary(dplyr)\nlibrary(pROC)\nlibrary(tree)\nlibrary(rpart)\nlibrary(randomForest)"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"models-for-binomial-data","chapter":"64 Introduction to models and prediction evaluation","heading":"64.1 Models for binomial data","text":"","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"profit-model","chapter":"64 Introduction to models and prediction evaluation","heading":"64.1.1 Profit model","text":"statistics, probit model type regression dependent variable can take two values. word portmanteau, coming probability + unit. purpose model estimate probability observation particular characteristics fall specific one categories; moreover, classifying observations based predicted probabilities type binary classification model.probit model popular specification binary response model. treats set problems logistic regression using similar techniques. viewed generalized linear model framework, probit model employs probit link function. often estimated using maximum likelihood procedure, estimation called probit regression.underlying mechanism, variable y, distributed normally (denoted \\(\\phi\\)()). y greater threshold, get observation. probability changes mean \\(\\mu\\) linear function conditioning variables =x’\\(\\beta\\).","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"logit-model","chapter":"64 Introduction to models and prediction evaluation","heading":"64.1.2 Logit model","text":"statistics, logistic model (logit model) used model probability certain class event existing pass/fail, win/lose, alive/dead healthy/sick. can extended model several classes events determining whether image contains cat, dog, lion, etc. object detected image assigned probability 0 1, sum one.Logistic regression statistical model basic form uses logistic function model binary dependent variable, although many complex extensions exist. regression analysis, logistic regression estimating parameters logistic model (form binary regression). Mathematically, binary logistic model dependent variable two possible values, pass/fail represented indicator variable, two values labeled “0” “1”. logistic model, log-odds (logarithm odds) value labeled “1” linear combination one independent variables (“predictors”); independent variables can binary variable (two classes, coded indicator variable) continuous variable (real value). defining characteristic logistic model increasing one independent variables multiplicatively scales odds given outcome constant rate, independent variable parameter; binary dependent variable generalizes odds ratio.log odds linear function conditioning variables, ln(p/(1-p)) =x’\\(\\beta\\)\np=exp(x’\\(\\beta\\))/(1+exp(x’\\(\\beta\\)))","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"how-to-fit-these-and-choose-models","chapter":"64 Introduction to models and prediction evaluation","heading":"64.1.3 How to fit these and choose models","text":"LikelihoodThe product densities (probability mass function) evaluated data\n\\(L(x)=f(x_1)f(x_2)...f(x_n)=\\prod_{=1}^{n} f(x_i)\\)\nwant maximize .\nproduct densities (probability mass function) evaluated data\\(L(x)=f(x_1)f(x_2)...f(x_n)=\\prod_{=1}^{n} f(x_i)\\)want maximize .Log likelihood works better\\(l(x)=ln(f(x_1)f(x_2)...f(x_n))=\\sum_{=1}^{n} ln(f(x_i))\\)\n\\(l(x)=ln(f(x_1)f(x_2)...f(x_n))=\\sum_{=1}^{n} ln(f(x_i))\\)f contained g2*ln(L(x|f)/L(x|g))~\\(\\chi^2_{n(g)-n(f)}\\), n(f)= number parameters f. like sums squares, just need base likelihood g. package glm, provides . nested models can compare likelihood differences.Deviance(2(ln(x,f)-ln(x,x)) allows direct comparison models.","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"we-will-use-glm-to-fit-and-analyze-models","chapter":"64 Introduction to models and prediction evaluation","heading":"64.1.4 We will use GLM to fit and analyze models","text":"GLM used fit generalized linear models, specified giving symbolic description linear predictor description error distribution. Family objects provide convenient way specify details models used functions glm. Link specification model link function.Let’s take look Default data first.Student non-student status may significant effect, can model effects default balance income.\n1. data\n2. Separately student non-student dataLogit ModelTo use logit model, need set family binomial link logit.Logit, just studentsLogit, studentsModeling studentNeither students non students dependence income. Balance nearly coefficient models student model, may drop income.Modeling without incomeUsing logit model probability default seems function student status balance, income.Probit ModelTo use probit model, need set family binomial link probit.Probit incomeLogit mildly sharper dependence balance student status fit Probit.\nIncome doesn’t matter, student status .\nelse matters balance owed.\nIncreased balance: default likely.\nStudent status: default less likely.","code":"\nDefault[1:3,]##   default student   balance   income\n## 1      No      No  729.5265 44361.63\n## 2      No     Yes  817.1804 12106.13\n## 3      No      No 1073.5492 31767.14\nDefault.logit.out<-glm(default~balance+income,family=binomial(link=logit),Default)\nsummary(Default.logit.out)## \n## Call:\n## glm(formula = default ~ balance + income, family = binomial(link = logit), \n##     data = Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4725  -0.1444  -0.0574  -0.0211   3.7245  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***\n## balance      5.647e-03  2.274e-04  24.836  < 2e-16 ***\n## income       2.081e-05  4.985e-06   4.174 2.99e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2920.6  on 9999  degrees of freedom\n## Residual deviance: 1579.0  on 9997  degrees of freedom\n## AIC: 1585\n## \n## Number of Fisher Scoring iterations: 8\nStudent.Default<-filter(Default,student==\"Yes\")\nDefault.logit.out.stu<-glm(default~balance+income,family=binomial(link=logit),Student.Default)\nsummary(Default.logit.out.stu)## \n## Call:\n## glm(formula = default ~ balance + income, family = binomial(link = logit), \n##     data = Student.Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1244  -0.1674  -0.0691  -0.0265   3.3252  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.151e+01  8.107e-01 -14.194   <2e-16 ***\n## balance      5.598e-03  3.774e-04  14.832   <2e-16 ***\n## income       1.585e-05  2.610e-05   0.607    0.544    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1046.85  on 2943  degrees of freedom\n## Residual deviance:  563.03  on 2941  degrees of freedom\n## AIC: 569.03\n## \n## Number of Fisher Scoring iterations: 8\nNonStudent.Default<-filter(Default,student==\"No\")\nDefault.logit.out.nstu<-glm(default~balance+income,family=binomial(link=logit),NonStudent.Default)\nsummary(Default.logit.out.nstu)## \n## Call:\n## glm(formula = default ~ balance + income, family = binomial(link = logit), \n##     data = NonStudent.Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4897  -0.1316  -0.0502  -0.0181   3.7600  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.094e+01  5.752e-01 -19.015   <2e-16 ***\n## balance      5.818e-03  2.938e-04  19.803   <2e-16 ***\n## income       1.597e-06  8.670e-06   0.184    0.854    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1861.8  on 7055  degrees of freedom\n## Residual deviance: 1008.0  on 7053  degrees of freedom\n## AIC: 1014\n## \n## Number of Fisher Scoring iterations: 8\nDefault.logit.out.allvar<-glm(default~balance+income+student,family=binomial(link=logit),Default)\nsummary(Default.logit.out.allvar)## \n## Call:\n## glm(formula = default ~ balance + income + student, family = binomial(link = logit), \n##     data = Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\n## balance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\n## income       3.033e-06  8.203e-06   0.370  0.71152    \n## studentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2920.6  on 9999  degrees of freedom\n## Residual deviance: 1571.5  on 9996  degrees of freedom\n## AIC: 1579.5\n## \n## Number of Fisher Scoring iterations: 8\nDefault.logit.out.noincome<-glm(default~balance+student,family=binomial(link=logit),Default)\nsummary(Default.logit.out.noincome)## \n## Call:\n## glm(formula = default ~ balance + student, family = binomial(link = logit), \n##     data = Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4578  -0.1422  -0.0559  -0.0203   3.7435  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***\n## balance      5.738e-03  2.318e-04  24.750  < 2e-16 ***\n## studentYes  -7.149e-01  1.475e-01  -4.846 1.26e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2920.6  on 9999  degrees of freedom\n## Residual deviance: 1571.7  on 9997  degrees of freedom\n## AIC: 1577.7\n## \n## Number of Fisher Scoring iterations: 8\nDefault.logit.out.allvar<-glm(default~balance+income+student,family=binomial(link=probit),Default)\nsummary(Default.logit.out.allvar)## \n## Call:\n## glm(formula = default ~ balance + income + student, family = binomial(link = probit), \n##     data = Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2226  -0.1354  -0.0321  -0.0044   4.1254  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -5.475e+00  2.385e-01 -22.960   <2e-16 ***\n## balance      2.821e-03  1.139e-04  24.774   <2e-16 ***\n## income       2.101e-06  4.121e-06   0.510   0.6101    \n## studentYes  -2.960e-01  1.188e-01  -2.491   0.0127 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2920.6  on 9999  degrees of freedom\n## Residual deviance: 1583.2  on 9996  degrees of freedom\n## AIC: 1591.2\n## \n## Number of Fisher Scoring iterations: 8\nDefault.probit.out.noincome<-glm(default~balance+student,family=binomial(link=probit),Default)\nsummary(Default.probit.out.noincome)## \n## Call:\n## glm(formula = default ~ balance + student, family = binomial(link = probit), \n##     data = Default)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2056  -0.1353  -0.0322  -0.0044   4.1374  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -5.3918818  0.1728730 -31.190  < 2e-16 ***\n## balance      0.0028215  0.0001138  24.784  < 2e-16 ***\n## studentYes  -0.3429201  0.0743964  -4.609 4.04e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2920.6  on 9999  degrees of freedom\n## Residual deviance: 1583.5  on 9997  degrees of freedom\n## AIC: 1589.5\n## \n## Number of Fisher Scoring iterations: 8"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"receiver-operating-characteristic","chapter":"64 Introduction to models and prediction evaluation","heading":"64.2 Receiver Operating Characteristic","text":"Definitions:\ncondition positive (P): number real positive cases data\ncondition negative (N): number real negative cases data\ntrue positive (TP): eqv. hit\ntrue negative (TN): eqv. correct rejection\nfalse positive (FP): eqv. false alarm, type error underestimation\nfalse negative (FN): eqv. miss, type II error overestimationROC plots true positive rate vs false positive rate function threshold.true positive rate TPR=TP/(TP+FN)\nfalse positive rate FPR=FP/(FP+TN)","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"roc-in-r","chapter":"64 Introduction to models and prediction evaluation","heading":"64.2.1 ROC in R","text":"function ROC can build ROC curve return “roc” object, list class “roc”, can printed plotted. function ROC, first parameter response, second parameter predictor.plot, Sensitivity=True Positive Rate, Specificity=False Positive Rate.","code":"\n#Create a test and train data set\nv1<-sort(sample(10000,6000))\nDefault.train<-Default[v1,]\nDefault.test<-Default[-v1,]\n#Create a logit model using glm\nDefault.train.out.noincome<-glm(default~balance+student,family=binomial(link=logit),Default.train)\n#Predict the test data set\nDefault.pred<-predict(Default.train.out.noincome,Default.test,type=\"response\")\nroc(Default.test$default,Default.pred,plot=T)## \n## Call:\n## roc.default(response = Default.test$default, predictor = Default.pred,     plot = T)\n## \n## Data: Default.pred in 3874 controls (Default.test$default No) < 126 cases (Default.test$default Yes).\n## Area under the curve: 0.9574\nDefault.pred.self<-predict(Default.train.out.noincome,type=\"response\")\nroc.train.str<-roc(Default.train$default,Default.pred.self)\nlines.roc(roc.train.str,type=\"lines\",col=2)"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"classification-and-regression-trees","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3 Classification and Regression Trees","text":"","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"recursive-partitioning","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3.1 Recursive Partitioning","text":"Recursive partitioning can used either regression binary data.introduce three choices.\n* tree - options\n* rpart - nicer graphics\n* randomForest - different approachLet’s see models work Auto data.","code":""},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"basic-trees","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3.2 Basic Trees","text":"p dimensions, n data points.Step 1: cycle p dimensions, n-1 possible splits dimensions, Choose 1 now gives improvement :\n* Sum squares regression\n* Likelihood modelsStep 2: data set created, redo step 1 select 1 best division.Go criteria met.Problems: Easy overfit\nStep 1, prune back using cross validation evaluate error. (chop parts tree somewhat automatic)","code":"\n#Create a training sample for Auto\nI1<-sample(392,200)\nAuto1<-Auto[I1,]\nAuto2<-Auto[-I1,]\ndum0tree<-tree(mpg~cylinders+displacement+horsepower+weight+year+acceleration,data=Auto1)\nplot(dum0tree)\ntext(dum0tree)\nmpg2tree<-predict(dum0tree,newdata=Auto2)\nplot(mpg2tree,Auto2[,1])"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"rpart-recursive-partitioning-and-regression-trees","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3.3 Rpart: Recursive Partitioning and Regression Trees","text":"can see tree rpart thing regression.","code":"\ndum0rpart<-rpart(mpg~cylinders+displacement+horsepower+weight+year+acceleration,data=Auto1)\nplot(dum0rpart)\ntext(dum0rpart)\nmpg2rpart<-predict(dum0rpart,newdata=Auto2)\nplot(mpg2rpart,Auto2[,1])"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"random-forests","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3.4 Random Forests","text":"p dimensions, n data points.Step 1: cycle random selection dimensions, random selection n-1 possible splits dimensions, Choose 1 now gives improvement :\n* Sum squares regression\n* Likelihood modelsStep 2: data set created, redo step 1 new randomly selected dimensions divisions.Step 3: repeat 1 2 original data number random trees made.classification, tree gets 1 vote (averaged) prediction new data.NICE result, large n big forest, fit.Plot tells us forest large enough error converge.","code":"\ndum0forest<-randomForest(mpg~cylinders+displacement+horsepower+weight+year+acceleration,data=Auto1)\nplot(dum0forest)\nmpg2forest<-predict(dum0forest,newdata=Auto2)\nplot(mpg2forest,Auto2[,1])"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"we-can-use-tree-rpart-and-random-forest-on-binary-data-too","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3.5 We can use tree, rpart, and random forest on binary data too","text":"Let’s go back Default data create model train data set using different methods.Basic TreesRpartRandom Forests","code":"\nDefault.tree.train.out.noincome<-tree(default~balance+student,Default.train)\nplot(Default.tree.train.out.noincome)\ntext(Default.tree.train.out.noincome)\nDefault.rpart.train.out.noincome<-rpart(default~balance+student,Default.train)\nplot(Default.rpart.train.out.noincome)\ntext(Default.rpart.train.out.noincome)\nDefault.rf.train.out.noincome<-randomForest(default~balance+student,Default.train)\nplot(Default.rf.train.out.noincome)"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"roc-on-tree-rpart-and-random-forest","chapter":"64 Introduction to models and prediction evaluation","heading":"64.3.6 ROC on tree, rpart, and random forest","text":"can compare models evaluate predictions ROC Area Curve plot.\nROC AUC tree model 0.9297, ROC AUC rpart model 0.7479, ROC AUC random forest model 0.8172. 0.9297 > 0.8172 > 0.7479, tree model performs best, rpart model performs worst.\nplot, black line represents tree model, red line represents rpart model, green line represents random forest model. can observe tree model best performance, rpart model worst performance.\ntwo results consistent.","code":"\n#Predict the test data set\ntree.test.pred<-predict(Default.tree.train.out.noincome,Default.test)\nrpart.test.pred<-predict(Default.rpart.train.out.noincome,Default.test)\nDefault.rf.test.pred<-predict(Default.rf.train.out.noincome,Default.test,type=\"prob\")\n#roc for tree\nroc(Default.test$default,tree.test.pred[,2],plot=T)## \n## Call:\n## roc.default(response = Default.test$default, predictor = tree.test.pred[,     2], plot = T)\n## \n## Data: tree.test.pred[, 2] in 3874 controls (Default.test$default No) < 126 cases (Default.test$default Yes).\n## Area under the curve: 0.9534\n#roc for rpart\nroc.rpart.default.pred<-roc(Default.test$default,rpart.test.pred[,2])\nlines.roc(roc.rpart.default.pred,type=\"lines\",col=2)\nprint(roc.rpart.default.pred)## \n## Call:\n## roc.default(response = Default.test$default, predictor = rpart.test.pred[,     2])\n## \n## Data: rpart.test.pred[, 2] in 3874 controls (Default.test$default No) < 126 cases (Default.test$default Yes).\n## Area under the curve: 0.7961\n#roc for random forest\nrf.roc.pred<-roc(Default.test$default,Default.rf.test.pred[,2])\nlines.roc(rf.roc.pred,type=\"lines\",col=3)\nprint(rf.roc.pred)## \n## Call:\n## roc.default(response = Default.test$default, predictor = Default.rf.test.pred[,     2])\n## \n## Data: Default.rf.test.pred[, 2] in 3874 controls (Default.test$default No) < 126 cases (Default.test$default Yes).\n## Area under the curve: 0.828"},{"path":"introduction-to-models-and-prediction-evaluation.html","id":"reference-5","chapter":"64 Introduction to models and prediction evaluation","heading":"64.4 Reference","text":"https://en.wikipedia.org/wiki/Probit_modelhttps://en.wikipedia.org/wiki/Logistic_regression#Definition_of_the_logistic_functionhttps://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glmhttps://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/familyhttps://en.wikipedia.org/wiki/Receiver_operating_characteristichttps://www.rdocumentation.org/packages/pROC/versions/1.16.2/topics/roc","code":""},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","text":"Ruopu Fandygraphs useful package visualizing time series data. audience-friendly visualization package takes xts data gives interactive graphs users can easily look data particular date interested . various highlighting, annotation graphing choices also make easier authors display data differently emphases.","code":""},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"before-you-plot","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.1 Before you plot","text":"","code":""},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"set-up","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.1.1 Set up","text":"","code":"\nlibrary(xts)\nlibrary(dygraphs)\nlibrary(tidyverse)\nlibrary(RColorBrewer)"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"data-processing","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.1.2 Data processing","text":"tutorial, use COVID-19 Daily Counts dataset retrieved NYC OpenData example. , look daily case counts NYC whole cases Manhattan Aug 1st Oct 24th, 2021. Remember transfer form data xts first:","code":"\ncovid_data = read_csv(\"https://data.cityofnewyork.us/resource/rc75-m7u3.csv\")\ncovid = covid_data %>%\n  mutate(date = as.Date(date_of_interest, format = \"%m/%d/%Y\")) %>%\n  filter(date >= as.Date(\"08/01/2021\", format = \"%m/%d/%Y\") & \n           date <= as.Date(\"10/24/2021\", format = \"%m/%d/%Y\")) %>%\n  select(date, case_count, mn_case_count) %>%\n  rename_with(toupper)\ncovid = xts(x = covid[,2:3], order.by = covid$DATE)"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"creating-your-own-plot","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2 Creating your own plot","text":"","code":""},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"a-base-graph","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2.1 A base graph","text":"dygraph function returns basic interactive time series plot. place mouse graph, show exact values series certain day. circle representing data point move along mouse movement. Please knit html file interact graphs!","code":"\ndygraph(covid)"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"series","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2.2 Series","text":"plot automatically draws two lines, want change default, many alternates.One can add individual points day customize point size dyOptions function:Point available different shapes like squares, triangles, etc.can also fill areas lines adjust alpha make transparent.Sometimes might want plot step charts:dashed bold line:can combine various types customization one graph well differentiate series visually:","code":"\ndygraph(covid) %>%\n  dyOptions(drawPoints = TRUE, pointSize = 2)\ndygraph(covid) %>%\n  dyOptions(drawPoints = TRUE, pointSize = 2, pointShape = \"square\")\ndygraph(covid) %>%\n  dyOptions(fillGraph = TRUE, fillAlpha = 0.1)\ndygraph(covid) %>%\n  dyOptions(stepPlot = TRUE)\ndygraph(covid) %>%\n  dySeries(\"CASE_COUNT\", strokeWidth = 2, strokePattern = \"dashed\")\ndygraph(covid) %>%\n  dySeries(\"CASE_COUNT\", strokeWidth = 2, strokePattern = \"dashed\") %>%\n  dySeries(\"MN_CASE_COUNT\", stepPlot = TRUE, fillGraph = TRUE)"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"title-axis-label-legends","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2.3 Title, axis, label & legends","text":"series, can start adding details . title can added directly dygraph function. change display axes, can use dyAxis dyOptions:use dySeries customize labels dyLegend decide, example, want show legends mouse leaves: setting hideOnMouseOut FALSE, keep point values graph even mouse left plot.legends set “follow”, values show mouse point ; also can make legend wider display values one line without wrapping :","code":"\ndygraph(covid[,2], main = \"COVID Counts in Manhattan\") %>%\n  dyAxis(\"x\", drawGrid = FALSE) %>%\n  dyAxis(\"y\", label = \"counts\", valueRange = c(0, 500)) %>%\n  dyOptions(includeZero = TRUE, axisLineWidth = 1.5)\ndygraph(covid) %>%\n  dySeries(\"CASE_COUNT\", label = \"NYC\") %>%\n  dySeries(\"MN_CASE_COUNT\", label = \"Manhattan\") %>%\n  dyLegend(show = \"always\", hideOnMouseOut = FALSE)\ndygraph(covid) %>%\n  dySeries(\"CASE_COUNT\", label = \"NYC\") %>%\n  dySeries(\"MN_CASE_COUNT\", label = \"Manhattan\") %>%\n  dyLegend(show = \"follow\", width = 280)"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"range-selector","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2.4 Range selector","text":"can add time range selector users choose time want observe closely.","code":"\ndygraph(covid) %>% \n  dyRangeSelector(height = 30, strokeColor = \"\")"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"hightlight-annotation-shadings","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2.5 Hightlight, annotation & shadings","text":"dyHighlight function provides users interaction place mouse graph. example fading non-highlighted series larger circles point highlighting:label specific point highlight time period, can use annotation shading graph:","code":"\ndygraph(covid) %>% \n  dyHighlight(highlightCircleSize = 5, \n              highlightSeriesBackgroundAlpha = 0.3,\n              highlightSeriesOpts = list(strokeWidth = 2))\ndygraph(covid) %>% \n  dyAnnotation(\"2021-09-15\", text = \"A\") %>%\n  dyShading(from = \"2021-09-01\", to = \"2021-10-01\")"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"colors-1","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.2.6 Colors","text":"can change colors series using RColorBrewer colors choice.use colors choice series also axis line, grid line, shading, etc. overall example time series visualization customized colors:","code":"\ndygraph(covid) %>%\n  dyOptions(colors = brewer.pal(3, \"Set1\"))\ndygraph(covid, main = \"COVID Counts in NYC\") %>%\n  dySeries(\"CASE_COUNT\", label = \"NYC\", color = \"dodgerblue\", drawPoints = TRUE, pointSize = 2, pointShape = \"square\") %>%\n  dySeries(\"MN_CASE_COUNT\", label = \"Manhattan\", color = \"royalblue\", stepPlot = TRUE, fillGraph = TRUE) %>%\n  dyLegend(show = \"follow\", width = 280) %>%\n  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.4) %>%\n  dyShading(from = \"2021-09-11\", to = \"2021-09-25\", color = \"aliceblue\") %>%\n  dyOptions(axisLineColor = \"navy\", gridLineColor = \"lightgrey\")"},{"path":"introduction-to-interactive-time-series-visualizations-with-dygraphs-in-r.html","id":"references-4","chapter":"65 Introduction to Interactive Time Series Visualizations with dygraphs in R","heading":"65.3 References","text":"Original data: COVID-19 Daily Counts Cases, Hospitalizations, DeathsOriginal data: COVID-19 Daily Counts Cases, Hospitalizations, Deathsdygraphs package description: dygraphs Rdygraphs package description: dygraphs R","code":""},{"path":"gganimate-tutorial.html","id":"gganimate-tutorial","chapter":"66 Gganimate Tutorial","heading":"66 Gganimate Tutorial","text":"Xuan HeThis tutorial gganimate, grammar animated graphics.\ntutorial, introduce basic functions making animated graphs.Youtube video link: https://youtu./c57-AGFE8aM","code":""},{"path":"get-nba-dataset-in-r.html","id":"get-nba-dataset-in-r","chapter":"67 Get NBA dataset in R","heading":"67 Get NBA dataset in R","text":"Yanbing Chen","code":""},{"path":"get-nba-dataset-in-r.html","id":"motivation-4","chapter":"67 Get NBA dataset in R","heading":"67.1 Motivation","text":"final project course analysis professional basketball, specifically, transformation game style professional basketball. great dataset prerequisite project official dataset viewable NBA.com available downloading csv file. Fortunately, useful package R called ‘nbastatR’. Hence, trying explore package R, also great complete tutorial share information help others also interested NBA basketball analysis R.thing needed noticed package might satisfy needs project. ways obtain data, scraping data NBA official website using python, also investing .Since lot functions package use, mainly focus functions related topic. information, please refer https://www.rdocumentation.org/packages/nbastatR/versions/0.1.110202031 package maintainer Alex Bresler.","code":""},{"path":"get-nba-dataset-in-r.html","id":"installation-2","chapter":"67 Get NBA dataset in R","heading":"67.2 installation","text":"","code":"\ndevtools::install_github(\"abresler/nbastatR\")\nlibrary(nbastatR)"},{"path":"get-nba-dataset-in-r.html","id":"functions","chapter":"67 Get NBA dataset in R","heading":"67.3 Functions","text":"agents_players() Players Agentsall_nba_teams() NBA Teamsall_star_games() NBA Star Gamesassign_bref_data() Assign nested BREF data environmentassign_nba_players() Assign NBA player dictionary environmentassign_nba_teams() Assign NBA teams environmentbeyond_the_numbers() NBA.com Beyond Numbers Articlesbox_scores() NBA box scoresbref_awards() Basketball reference awardsbref_awards_votes() Basketball Reference award votesbref_bios() Basketball Reference players biosbref_injuries() Active injuriesbref_players_stats() Basketball Reference Player Season Tablesbref_teams_stats() Basketball Reference teams seasons datacoaching_staffs() NBA active coaching staffscurrent_schedule() NBA current season schedulecurrent_standings() Current standingsdays_scores() Get NBA Dates’ NBA Scoresdictionary_bref_awards() Basketball Reference Awardsdictionary_bref_coaches() Basketball Reference coach dictionarydictionary_bref_players() Basketball Reference player dictionarydictionary_bref_teams() Basketball Reference team dictionarydictionary_nba_names() Dictionary NBA Headers nbastatR namesdictionary_player_photos() Cached player photo dictionarydraft_combines() NBA draft combine datadrafts() NBA draftsfanduel_summary() Games fanduel summaryfranchise_leaders() Franchise leadersgame_logs() NBA game logs specified parametershoops_hype_salary_summary() HoopsHype NBA teams summary salarieshoopshype_salaries() Hoopshype teams players salariesmetrics_leaders() League leaders seasonnba_franchise_history() Get NBA franchise historynba_insider_salaries() NBA team salariesnba_player_ids() Players’ NBA player idsnba_players() NBA player dictionarynba_stats_api_items() NBA stats API parameters, teams itemsnba_teams() NBA team dictionarynba_teams_ids() NBA team idsnba_teams_seasons() NBA teams seasonsnbadraftnet_mock_drafts() NBADraft.net mock draftsnbastats_api_parameters() NBA Stats API Parametersplay_by_play() NBA games play-playplay_by_play_v2() NBA games play--play v2player_profiles() NBA.com player profilesplayers_agents() Get NBA Players Agentsplayers_awards() NBA players awardsplayers_bios() NBA.com biosplayers_careers() Player career statsplayers_rotowire() Players RotoWire newsplayers_tables() NBA players table dataplayoff_pictures() NBA seasons playoff picturepst_transaction() ProSports NBA transactionsscale_per_minute() Summarise data per minuteseasons_players() Seasons playersseasons_rosters() NBA teams seasons rostersseasons_schedule() NBA seasons schedulessl_players() Summer League Playerssl_teams() Summer League Teamsspread_data() Spread gathered data framestandings() Get seasons standing datasummarise_per_minute() Summarize data per minutesynergy() Get Synergy data specified seasonteam_season_roster() Team rosterteams_annual_stats() NBA teams yearly performanceteams_coaches() Seasons coaching staffsteams_details() NBA teams detailsteams_players_stats() NBA teams player statisticsteams_rankings() NBA teams rankingsteams_rosters() Teams seasons rostersteams_rotowire() Teams Rotowire newsteams_seasons_info() NBA teams seasons informationteams_shots() Get teams seasons shot chartsteams_tables() NBA Team table data seasontransactions() NBA transactions since 2012validate_nba_player_photos() Validate NBA Player photoswiden_bref_data() Widens basketball reference table datawin_probability() NBA games win probabilities","code":""},{"path":"get-nba-dataset-in-r.html","id":"guides-to-functions","chapter":"67 Get NBA dataset in R","heading":"67.4 Guides to Functions","text":"several examples approach goal analysis using package.looking players drafted year, figure kind players teams wanted past nowadays.looking total point leader season, discover games changing, maybe dominating centers guards.examining shot chart, find trends shooting longer shot distance.summarizing total points season, might see changes speed plays game.","code":"\ndrafts(draft_years = 2000:2010)\nmetrics_leaders(seasons = 2000:2010,\n                metric = \"pts\",\n                season_types = \"Regular Season\",\n                modes = \"PerGame\")\nteams_shots(teams = \"Brooklyn Nets\", seasons = 2000:2010)\nteams_annual_stats(teams = \"New York Knicks\",modes = c(\"Totals\"))"},{"path":"mtemplates.html","id":"mtemplates","chapter":"68 Mtemplates","heading":"68 Mtemplates","text":"Mahesh JindalThis package contains custom R Markdown Templates. Run following command R console install package:Using template:\n1. package installed, Click File > New File > R Markdown > Template > R Markdown Template - Columbia University.\n2. new template loaded. Change title, author, uni subject r markdown configuration properties can start writing custom code.[Package Source Code] (https://github.com/maheshjindal/mtemplates)","code":"\nremotes::install_github(\"maheshjindal/mtemplates\")"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"introduction-to-time-series-analysis-in-r","chapter":"69 Introduction to Time Series Analysis in R","heading":"69 Introduction to Time Series Analysis in R","text":"Chenqi Wang","code":""},{"path":"introduction-to-time-series-analysis-in-r.html","id":"overview-3","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.1 Overview","text":"introduce fundamental time series models including ARMA, GARCH etc. R focus model definitions, process generations visualizations.","code":""},{"path":"introduction-to-time-series-analysis-in-r.html","id":"arma-autoregressive-moving-average","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2 ARMA (Autoregressive-moving-average)","text":"ARMA(p, q): \\(y_t=\\alpha+\\sum_{= 1}^p\\beta_iy_{t-}+\\epsilon_t+\\sum_{=1}^q\\theta_i\\epsilon_{t-}\\)\\(y_t\\) time series, \\(\\alpha\\) intercept, \\(\\beta_i\\) \\(\\theta_i\\) coefficients, \\(\\epsilon_t\\) white noise.","code":""},{"path":"introduction-to-time-series-analysis-in-r.html","id":"white-noise","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.1 White Noise","text":"\\(\\epsilon_t\\) ~ ..d. N(0, \\(\\sigma^2\\)) \\(\\sigma^2>0\\).","code":"\n# generate white noise of length n and s.t.d. b\nWN <- function(n, b) {\n  x <- rnorm(n, 0, b)\n  plot.ts(x, main = paste(\"white noise of size\", n, \"and standard deviation\", b))\n  x\n}\n\ne <- WN(500, 1)"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"ar-autoregressive","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.2 AR (Autoregressive)","text":"ARMA model consists two parts: AR MA. Let’s first introduce AR model:AR(p): \\(y_t=\\alpha+\\sum_{= 1}^p\\beta_iy_{t-}+\\epsilon_t\\)generate AR(1) model:AR(1) model weakly stationary \\(|\\beta_1|<1\\), generated model \\(y_t=1+0.5y_{t-1}+\\epsilon\\) weakly stationary. can also see plot: mean data roughly remains , weak stationarity implies.","code":"\n# simulation an AR(1) process y_t = b_0 + b_1 * y_{t-1} + epsilon, with data length n\nAR1 <- function(b0, b1, b2, n) {\n  set.seed(100)\n  x <- rnorm(n, 0, b2)\n  y <- 0\n  y[1] <- b0 / (1 - b1) + x[1] \n  for (t in 2 : n) {\n    y[t] <- b0 + b1 * y[t - 1] + x[t]\n  }\n  \n  plot.ts(y, main = paste(\"y[t] =\", b0, \"+\", b1, \"* y[t - 1] + epsilon\"))\n  y\n}\n\ny <- AR1(1, 0.5, 0.2, 500)"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"pacf-partial-autocorrelation-function","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.3 PACF (Partial Autocorrelation Function)","text":"Given time series \\(y_t\\) generated AR(p) model, determine value p, can use PACF. lag-k PACF \\(\\beta_{kk}\\) weakly stationary time series measures contribution adding term \\(y_{t-k}\\) AR(k - 1) model, can estimated Ordinary Least Squares (OLS) estimator \\(\\hat{\\beta_{kk}}\\) AR(k) model \\(y_t=\\alpha_{k}+\\sum_{= 1}^k\\beta_{ki}y_{t-}+\\epsilon_{kt}\\).plot indicates lag \\(k\\geq 2\\) insignificant contribution, AR(1) model enough.","code":"\npacf(y) "},{"path":"introduction-to-time-series-analysis-in-r.html","id":"acf-autocorrelation-function","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.4 ACF (Autocorrelation Function)","text":"lag-k ACf \\(\\gamma(k):=cov(y_t, y_{t+k})\\). Assume time series \\(y_t\\) weakly stationary, \\(cov(y_t, y_{t+k})\\) depends k.ACF can also used determine value p AR(p) model.plot observe \\(y_t\\) \\(y_{t+k},k\\geq 2\\) insignificant correlation, AR(1) enough.","code":"\nacf(y)"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"unit-root-process","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.5 Unit Root Process","text":"random walk \\(y_t=y_{t-1}+\\epsilon_t\\) unit root process, stationary contains stochastic trend.stochastic trend can observed plot. Generally mean data goes , time series stationary.","code":"\n# generate the unit root process y[t] = y[t-1] + epsilon\nUR <- function(n) {\n  set.seed(100)\n  x <- rnorm(n, 0, 1)\n  y <- 0\n  y[1] <- x[1]\n  for (t in 2 : n) {\n    y[t] <- y[t - 1] + x[t]\n  }\n  \n  plot.ts(y, main = paste(\"y[t] = y[t - 1] + epsilon\"))\n  y\n}\n\ny1 <- UR(500)"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"ma-moving-average","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.6 MA (Moving-average)","text":"MA(q): \\(y_t=\\alpha+\\epsilon_t+\\sum_{=1}^q\\theta_i\\epsilon_{t-}\\)Generate MA(1) model:MA(q) model, \\(\\forall k>q\\) ACF \\(\\gamma(k)=0\\). ’s easy ACF determine q.can observe plot q 1.","code":"\n# generate MA(1) model y[t] = a + e[t] + r * e[t-1] with data length n\nMA1 <- function(a, r, n) {\n  set.seed(100)\n  e <- rnorm(n, 0, 0.2)\n  y <- 0\n  y[1] <- a + e[1]\n  for (t in 2 : n) {\n    y[t] <- a + e[t] + r * e[t - 1]\n  }\n  \n  plot.ts(y, main = paste(\"y[t] =\", a, \"+ e[t] +\", r, \"* e[t-1]\"))\n  y\n}\n\ny2 <- MA1(1.2, 0.3, 500)\nacf(y2)"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"arima-autoregressive-integrated-moving-average","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.2.7 ARIMA (Autoregressive-integrated-moving-average)","text":"Define lag operator \\(Ly_t:=y_{t-1}\\), \\((1-L)^d:=(1-L)[(1-L)^{d-1}]\\) (.e. d order difference). \\((1-L)^{d-1}y_t\\) weakly stationary \\((1-L)^dy_t\\) weakly stationary \\(d>0\\), \\((1-L)^dy_t\\) ARMA(p, q) process, \\(y_t\\) called ARIMA(p, d, q) process.can use arima() R estimate ARIMA model:","code":"\narima(y2, order = c(0, 0, 1), method = \"ML\") # MA(1)## \n## Call:\n## arima(x = y2, order = c(0, 0, 1), method = \"ML\")\n## \n## Coefficients:\n##          ma1  intercept\n##       0.2244     1.1903\n## s.e.  0.0469     0.0110\n## \n## sigma^2 estimated as 0.0401:  log likelihood = 94.63,  aic = -183.25"},{"path":"introduction-to-time-series-analysis-in-r.html","id":"garch-generalized-autoregressive-conditional-heteroskedasticity","chapter":"69 Introduction to Time Series Analysis in R","heading":"69.0.3 GARCH (Generalized Autoregressive Conditional Heteroskedasticity)","text":"AR(1) - GARCH(s, m):\\(y_t=\\phi_0+\\phi_{1}y_{t-1}+\\epsilon_t\\)\\(\\epsilon_t = \\sigma_t\\eta_t\\)\\(\\sigma_t^2=\\alpha_0+\\sum_{=1}^s\\beta_i\\sigma_{t-}^2+\\sum_{=1}^m\\alpha_i\\epsilon_{t-}^2\\)\\(var(\\epsilon_t|y_{t-1})=\\sigma^2(y_{t-1})\\) (conditional heteroskedasticity)GARCH models second moment information time series, ARIMA models level .","code":"\n# AR(1) - GARCH(1, 1) y[t] = a0 + a1 * y[t - 1] + v * epsilon, v[t]^2 = b0 + b1 + v[t-1]^2 + b2e^2, data length n\nGARCH <- function(a0, a1, b0, b1, b2, n) {\n  set.seed(1000)\n  n = n + 500\n  x <- rnorm(n, 0, 1) \n  y = 0\n  v = 0\n  a = 0\n  v[1] = b0 / (1 - b1 - b2)\n  a[1] = sqrt(v[1]) * x[1]\n  y[1] = a0 / (1 - a1) + a[1]\n  for (t in 2 : n) {\n    v[t] = b0 + b1 * v[t - 1] + b2 * a[t - 1] ^ 2\n    a[t] = sqrt(v[t]) * x[t]\n    y[t] = a0 + a1 * y[t - 1] + a[t]\n  }\n  y = y[-(1:500)]\n  v = v[-(1:500)]\n  plot.ts(y, main = paste(\"y[t] =\", a0, \"+\", a1, \"* y[t - 1] + epsilon\"))\n  z = cbind(v, v)\n  z\n}\n\ny3 <- GARCH(1, 0.5, 0.02, 0.6, 0.3, 500)"},{"path":"storytelling-with-data.html","id":"storytelling-with-data","chapter":"70 Storytelling with Data","heading":"70 Storytelling with Data","text":"Jessie WangBeing able tell story behind data important skill data scientists/ analysts. received lot training storytelling previous work. summarized key points put together deck. contains information build story, best practice data visualization slides, make successful presentation. link slides: https://github.com/jessiewyt/EDAV/blob/main/CC-Storytelling.pdfMy notes brilliant session taught Ravishankar Iyer. https://www.storyrules.com URL website anyone interested learning .","code":""},{"path":"tutorial-on-r-torch-package.html","id":"tutorial-on-r-torch-package","chapter":"71 Tutorial on R torch package","heading":"71 Tutorial on R torch package","text":"Wenbo Zhao","code":""},{"path":"tutorial-on-r-torch-package.html","id":"introduction-11","chapter":"71 Tutorial on R torch package","heading":"71.1 Introduction","text":"Nowadays, deep learning technique shown ability dealing data intensive tasks image recognition, neural language processing . python, two famous deep learning frameworks: tensorflow[1] pytorch[2]. tensorflow provides R accessibility since 2017, torch available end 2019[3]. use-friendliness pytorch strong visualization ability R, cool combine together. like talk use torch R.torch ecosystem includes several packages. Torch package basic one includes basic data structure tensor, N-d array numbers, nn_modules includes trainable weights nnf_modules static ones. Luz package provides compact implementation training procedure need write loops. Torchvision package especially designed vision tasks can crop transform images suitable size feed network.","code":""},{"path":"tutorial-on-r-torch-package.html","id":"background-2","chapter":"71 Tutorial on R torch package","heading":"71.2 Background","text":"","code":""},{"path":"tutorial-on-r-torch-package.html","id":"deep-learning","chapter":"71 Tutorial on R torch package","heading":"71.2.1 Deep Learning","text":"Like linear regression, support vector machine k-nearest neighbour, deep learning also kind machine learning methods can help us approximate output classify. methods need training dataset can finetune parameters give result new sample. One important benefit deep learning complexity models can abstract inner feature samples give precise prediction. said enough parameters, deep learning models can emulate function input output. course cause overfitting problem thus size model set compatible size input sample.However, present large deep learning model training process easy task traditional matrix multiplication. context, tensorflow pytorch appears higher level API. , can easily abstract deep learning block stacking blocks together gives complete network.","code":""},{"path":"tutorial-on-r-torch-package.html","id":"convolution-neural-network","chapter":"71 Tutorial on R torch package","heading":"71.2.2 Convolution Neural Network","text":"mentioned previous paragraph, deep learning network consisted multiple blocks. image classification task frequently used block convolution neural network (CNN). Suppose input size \\((N, C_{\\mbox{}}, H, W)\\) weight \\(W\\) size \\((C_{\\mbox{}}, C_{\\mbox{}}, k, k)\\) stride \\(s\\) padding \\(p\\), output size \\((N, C_{\\mbox{}}, H_{\\mbox{}}, W_{\\mbox{}})\\) \n\\[\n  H_{\\mbox{}} = \\left\\lfloor\\frac{H + 2p - k}{s}\\right\\rfloor + 1 \\\\\n  W_{\\mbox{}} = \\left\\lfloor\\frac{W + 2p - k}{s}\\right\\rfloor + 1\n\\]\n\n\\[\n  \\mbox{}[n, j, x, y] = \\mbox{bias}_j + \\mbox{inner_product}(W[j,:,:,:], \\mbox{}[n, :, 1+(x-1)s:1+(x-1)s+k, 1+(y-1)s:1+(y-1)s+k])\n\\]\n\\(.e.\\) output value inner product result two 4-d array size \\([1,C_\\mbox{}, k, k]\\) plus bias. block widely used image tasks achieved good result basic unit foundation networks Alexnet[4] Resnet[5].","code":""},{"path":"tutorial-on-r-torch-package.html","id":"implementation","chapter":"71 Tutorial on R torch package","heading":"71.3 Implementation","text":"","code":""},{"path":"tutorial-on-r-torch-package.html","id":"installation-3","chapter":"71 Tutorial on R torch package","heading":"71.3.1 Installation","text":"install torch Rstudio, first install torch package. example, also need luz torchvision, therefore runand run","code":"\ninstall.packages(\"torch\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"luz\")\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)"},{"path":"tutorial-on-r-torch-package.html","id":"fetching-dataset","chapter":"71 Tutorial on R torch package","heading":"71.3.2 Fetching Dataset","text":"example, like identify digits famous hand-written digit dataset MNIST using pure convolution neural network.batch_size parameter number inputs processed parallel value decided computation capacity processor. example input shown follow:","code":"\ndir <- \"./dataset/mnist\"\n\ntrain_ds <- mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = transform_to_tensor\n)\n\ntest_ds <- mnist_dataset(\n  dir,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\ntrain_dl <- dataloader(train_ds, batch_size = 128, shuffle = TRUE)\ntest_dl <- dataloader(test_ds, batch_size = 128)\nimage <- train_ds$data[1,1:28,1:28]\nimage_df <- melt(image)\nggplot(image_df, aes(x=Var2, y=Var1, fill=value))+\n  geom_tile(show.legend = FALSE) + \n  xlab(\"\") + ylab(\"\") +\n  scale_fill_gradient(low=\"white\", high=\"black\")"},{"path":"tutorial-on-r-torch-package.html","id":"building-up-the-network","chapter":"71 Tutorial on R torch package","heading":"71.3.3 Building up the network","text":"nn_module function requires 3 variables. name variable optional name module, initialization function includes accessaries needed network. example, convolution layers. parameter nn_conv2d input channel \\(C_\\mbox{}\\), output channel \\(C_\\mbox{}\\), kernel size \\(k\\) stride \\(s\\), described previous sections. padding \\(p\\) set 0 default.shown comment code, disregarding batch size \\(N\\), input size 28*28 image 1 input channel. conv1, using euqation calculating \\(H_\\mbox{}\\) \\(W_\\mbox{}\\), can see size 26*26 32 channels. conv2, shape 24*24 64 channels. max_pool2d(2) function selection largest one 2*2 region prevent overfit. shape 12*12 64 channels. flatten function aligns 9216 series two linear layers finally get 10 values network output index largest one among decides output prediction digit.","code":"\nnet <- nn_module(\n  \"Net\",\n  \n  initialize = function() {\n    self$conv1 <- nn_conv2d(1, 32, 3, 1)\n    self$conv2 <- nn_conv2d(32, 64, 3, 1)\n    self$dropout1 <- nn_dropout2d(0.25)\n    self$dropout2 <- nn_dropout2d(0.5)\n    self$fc1 <- nn_linear(9216, 128)\n    self$fc2 <- nn_linear(128, 10)\n  },\n  \n  forward = function(x) {\n    x %>%                                  # N * 1 * 28 * 28\n      self$conv1() %>%                     # N * 32 * 26 * 26\n      nnf_relu() %>%                       \n      self$conv2() %>%                     # N * 64 * 24 * 24\n      nnf_relu() %>% \n      nnf_max_pool2d(2) %>%                # N * 64 * 12 * 12\n      self$dropout1() %>% \n      torch_flatten(start_dim = 2) %>%     # N * 9216\n      self$fc1() %>%                       # N * 128\n      nnf_relu() %>% \n      self$dropout2() %>% \n      self$fc2()                           # N * 10\n  }\n)"},{"path":"tutorial-on-r-torch-package.html","id":"training","chapter":"71 Tutorial on R torch package","heading":"71.3.4 Training","text":"","code":"\nfitted <- net %>%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_accuracy()\n    )\n  ) %>%\n  fit(train_dl, epochs = 10, valid_data = test_dl)Epoch 1/10\nTrain metrics: Loss: 0.2813 - Acc: 0.9146                                                                    \nValid metrics: Loss: 0.0565 - Acc: 0.982\nEpoch 2/10\nTrain metrics: Loss: 0.1055 - Acc: 0.9687                                                                    \nValid metrics: Loss: 0.0424 - Acc: 0.985\nEpoch 3/10\nTrain metrics: Loss: 0.0782 - Acc: 0.9756                                                                    \nValid metrics: Loss: 0.0359 - Acc: 0.9872\nEpoch 4/10\nTrain metrics: Loss: 0.0626 - Acc: 0.9815                                                                    \nValid metrics: Loss: 0.0364 - Acc: 0.989\nEpoch 5/10\nTrain metrics: Loss: 0.0563 - Acc: 0.983                                                                     \nValid metrics: Loss: 0.0362 - Acc: 0.9889\nEpoch 6/10\nTrain metrics: Loss: 0.0522 - Acc: 0.9831                                                                    \nValid metrics: Loss: 0.0345 - Acc: 0.9892\nEpoch 7/10\nTrain metrics: Loss: 0.0448 - Acc: 0.9861                                                                    \nValid metrics: Loss: 0.029 - Acc: 0.9901\nEpoch 8/10\nTrain metrics: Loss: 0.0415 - Acc: 0.9863                                                                    \nValid metrics: Loss: 0.0307 - Acc: 0.9905\nEpoch 9/10\nTrain metrics: Loss: 0.0361 - Acc: 0.9883                                                                    \nValid metrics: Loss: 0.0289 - Acc: 0.9905\nEpoch 10/10\nTrain metrics: Loss: 0.0337 - Acc: 0.9892                                                                    \nValid metrics: Loss: 0.0286 - Acc: 0.9904"},{"path":"tutorial-on-r-torch-package.html","id":"testing","chapter":"71 Tutorial on R torch package","heading":"71.3.5 Testing","text":"","code":"\npreds <- predict(fitted, test_dl)\npreds$shape[1] 10000    10"},{"path":"tutorial-on-r-torch-package.html","id":"saving-the-model","chapter":"71 Tutorial on R torch package","heading":"71.3.6 Saving the model","text":"","code":"\nluz_save(fitted, \"mnist-cnn.pt\")"},{"path":"tutorial-on-r-torch-package.html","id":"conclusion-3","chapter":"71 Tutorial on R torch package","heading":"71.4 Conclusion","text":"Traning model R can gives us much merit since R can visualize data easier prettier python. example, can plot accuracy curve traning see whether model converged, plot distribution trained weights see distribution difference layer, etc. Although visualization method already implemented tensorboardX python, think still beneficial attempt train deep learning models R.","code":""},{"path":"tutorial-on-r-torch-package.html","id":"citation","chapter":"71 Tutorial on R torch package","heading":"71.5 Citation","text":"[1] https://www.tensorflow.org/\n[2] https://pytorch.org/\n[3] https://torch.mlverse.org/\n[4] Krizhevsky, Alex, Ilya Sutskever, Geoffrey E. Hinton. “Imagenet classification deep convolutional neural networks.” Advances neural information processing systems 25 (2012): 1097-1105.\n[5] , Kaiming, et al. “Deep residual learning image recognition.” Proceedings IEEE conference computer vision pattern recognition. 2016.","code":""},{"path":"edav-tutorials-correlogram-calendar-heatmap-and-slopegram.html","id":"edav-tutorials-correlogram-calendar-heatmap-and-slopegram","chapter":"72 EDAV Tutorials: Correlogram, Calendar Heatmap and Slopegram","heading":"72 EDAV Tutorials: Correlogram, Calendar Heatmap and Slopegram","text":"Yajie Zhang (yz3876)community contribution project, wish make something educating also might helpful future students. picked 3 data visualizations haven’t covered lectures found useful practice. Moreover, also found aesthetically\nappealing. 3 visualizations introduced include correlograms, calendar heatmaps slopegrams. datasets used include Red Wine Quality dataset, S&P500 XOM stock prices dataset tidyquant library, newcancer dataset CGPFunctions library, reflecting common use case graph introduced tutorials.part tutorials, used ggplot package realize visualizations use dataset choice reflects commonly used. graphs, also explained adjust aesthetic elements make look clearer, dataset’s patterns best reflected graphs. type visualization, summarized kind data best used plots. given time project, make plots reflect cases graphs fail convoy pattern data drawing -ideal examples. think best way illustrate characteristics visualization showing pros cons. giving good bad examples, give future learners better sense use & use visualizations.project, spent time exploring online visualizations like learning \nimplement using ggplot learned class. think ’s rewarding\nexperience. learned draw 3 graphs also independently find\nimplement visualizations. personally find EDAV topic essential building machine learning models impressive techniques well. data scientist, need learn build models also present results, data insights using visualizations frequently. hope future learners benefited tutorials impressed data visualizations .Links deliverable:\nVideo Tutorial,\nSlide Deck","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"recursive-codes-and-self-organized-map-with-r","chapter":"73 Recursive codes and self-organized map with R","heading":"73 Recursive codes and self-organized map with R","text":"Yuki IkedaIn article, introduce () develop recursive codes R order use R every phase data analyses, (B) draw self-organized map R, well known interesting way 2D projection (dimension reduction) (relatively) high dimensional data, (C) three useful functions ggplot practice.","code":"\nlibrary(tidyverse)\nlibrary(kohonen)\nlibrary(openintro)\nlibrary(datasets)"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"to-conduct-all-analyses-with-r-write-recursive-code-with-r","chapter":"73 Recursive codes and self-organized map with R","heading":"73.1 To conduct all analyses with R – Write recursive code with R","text":"","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"motivation-5","chapter":"73 Recursive codes and self-organized map with R","heading":"73.1.1 Motivation","text":"practice, believe one annoying workflows go around among many pieces software using python data cleaning, developing statistical analysis using R, visualizing Excel, etc. typically makes folder messy bothers successor current stuff leaves role. Hence, employ datavis tools R, also want develop analyses R. However, order us , learn many types analyses R. fact, R advanced language can many things . , introduce recursive coding R illustration.sometimes use recursive functions (give one frequent example later). People may think R good python handle recursive function, might conduct recursive programming using python , finishing , move R visualization, etc, makes workflow messy.","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"an-illustration-of-recursive-code-levenshtein-distance","chapter":"73 Recursive codes and self-organized map with R","heading":"73.1.2 An illustration of recursive code: Levenshtein distance","text":", introduce define recursive function R using example Levenshtein distance.\npractice, often case want connect B supposed . example, commercial bank, often connect corporate debtor data external information companies using company name available key.\nHowever, also often case one data company name “XXX, plc” one just “xxx,” one “XXX, plc, YYY brunch” “XXX, plc.”\ncases, directly connect two data. Levenshtein distance comes .\nLevenshtein distance two words minimum number edits (1) insert one character either one word, (2) delete one character either one word, (3) substitute one character either one word transform one word another.\npast job, used distance quantify difference words two company name columns, enabled us find possible matches .Levenshtein distance useful reason , however, computing distance requires us define recursive functions . Please see [1] definition written https://en.wikipedia.org/wiki/Levenshtein_distance. Hence, define recursive function R.","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"naive-coding","chapter":"73 Recursive codes and self-organized map with R","heading":"73.1.3 Naive coding","text":"One naive way just using “recall” function orders compute recursively:However, takes much time length strings gets longer since computes value distance partial strings x y .","code":"\nLD_naive <- function(x,y) {\n  if (nchar(x)==0){\n    return( nchar(y) )\n  } else if (nchar(y)==0){\n    return( nchar(x) )\n  } else if (substr(x,nchar(x),nchar(x))==substr(y,nchar(y),nchar(y))){\n    return (\n      Recall(substr(x,1,nchar(x)-1),substr(y,1,nchar(y)-1))\n    )\n  } else {\n    return(\n      1+\n      min(\n        Recall(substr(x,1,nchar(x)-1),y),\n        Recall(x,substr(y,1,nchar(y)-1)),\n        Recall(substr(x,1,nchar(x)-1),substr(y,1,nchar(y)-1))\n      )\n    )\n  }\n}\n\n## Levenshtein distance between \"kitten\" and \"sitting\"\n\nLD_naive(\"kitten\",\"sitting\")## [1] 3"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"better-coding-using-dp-table","chapter":"73 Recursive codes and self-organized map with R","heading":"73.1.4 Better coding using DP table","text":"Therefore, better coding prepare Dynamic Programming table preserve distance partial strings beginning first ending point x y:However, even x y identical, code computes distance every pair partial strings beginning first ending point x y. Clearly case, need compute \\(LD(x[1:],y[1:])\\) \\(\\). , code little redundant.","code":"\nLD0 <- function(x,y) {\n  \n  n <- nchar(x)\n  m <- nchar(y)\n  \n  M <- matrix(NA,n+1,m+1)\n  \n  M[1,] <- rep(0,m+1)\n  M[,1] <- rep(0,n+1)\n\n  for(i in 1:n){\n    for(j in 1:m){\n     \n      if(substr(x,i,i)==substr(y,j,j)){\n        M[i+1,j+1] <- M[i,j]\n      } else {\n        M[i+1,j+1] <- 1+min(M[i,j],M[i+1,j],M[i,j+1])\n      }\n      \n    }\n  }\n  \n  return(M[n+1,m+1])\n  \n}\n\n## Levenshtein distance between \"kitten\" and \"sitting\"\n\nLD0(\"kitten\",\"sitting\")## [1] 3"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"recursive-coding-using-global-variable","chapter":"73 Recursive codes and self-organized map with R","heading":"73.1.5 Recursive coding using global variable","text":"next code efficient sense computes distances need.Specifically, two strings identical, computes \\(LD(x[1:],y[1:])\\) \\(\\).seen example, key point prepare global variable \\(M\\) preserve distance partial strings, call computing distance longer strings one character. Note need use <<- input value global variable.way coding can applied recursive programming R.","code":"\nLD <- function(x,y) {\n  \n  n <- nchar(x)\n  m <- nchar(y)\n  \n  M <- matrix(NA,n+1,m+1)   ##Global variable\n\n  LDn <- function(x,y,n,m) {\n  \n    if (n==0){         \n      M[1,m+1] <<- m   ##use <<- when input into global variable\n      return( m )\n    } else if (m==0){\n      M[n+1,1] <<- n\n      return( n )\n    } else if (substr(x,n,n)==substr(y,m,m)){\n    \n      ## first get values from global variable\n      itm1 <- M[n,m]\n    \n      ## if global variable does not have the value,\n      ## then get instead from the function which we very define,\n      ## passing the value to global variable\n      if(is.na(itm1)){\n        itm1 <- LDn(x,y,n-1,m-1)\n        M[n,m] <<- itm1   ##use <<- when input into global variable\n      }\n    \n      return ( itm1 )\n    \n    } else {\n    \n      itm2 <- M[n,m+1]\n      itm3 <- M[n+1,m]\n      itm4 <- M[n,m]\n    \n    \n      if(is.na(itm2)){\n        itm2 <- LDn(x,y,n-1,m)\n        M[n,m+1] <<- itm2   ##use <<- when input into global variable\n      }\n\n    \n      if(is.na(itm3)){\n        itm3 <- LDn(x,y,n,m-1)\n        M[n+1,m] <<- itm3   ##use <<- when input into global variable\n      }\n\n    \n      if(is.na(itm4)){\n        itm4 <- LDn(x,y,n-1,m-1)\n        M[n,m] <<- itm4   ##use <<- when input into global variable\n      }\n        \n      return( 1+min( itm2, itm3, itm4 ) )\n    }\n  \n  }\n\n  return( LDn(x,y,n,m) )\n\n}\n\n## Levenshtein distance between \"kitten\" and \"sitting\"\n\nLD(\"kitten\",\"sitting\")## [1] 3"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"self-organized-map-with-r","chapter":"73 Recursive codes and self-organized map with R","heading":"73.2 Self-organized map with R","text":"chapter, introduce self-organized map draw using R package kohonen, discussing merits possible limits.conduct multivariate exploratory analysis, first want briefly look typical patterns data .\nHowever, can easily visually interpret 2-dimensional scatter patterns.\nHence, useful somehow project data points 2-dimensional plane.popular way using principal component analysis (PCR), however, even relatively moderate dimension like three four, non-specialist might feel PCA biplots three four principal component vectors easy see.self-organized map comes . employs simple neural net one input layer (data points) one output layer. specify 2D(Dimensional)-shape output layer, searches optimal projection (fairly informally) minimize distance point nearest unit 2D-output layer iterating minimization respect current unit output layer updating units based data points belong unit. sense, similar k-means method. difference since k mean vectors located 2D-plane, still visualize result k-means classification directly, self-organized map, since units (corresponding k mean vectors) squeezed 2D plane, can visualize .","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"iris-data","chapter":"73 Recursive codes and self-organized map with R","heading":"73.2.1 Iris data","text":"code elementary tutorial draw self-organized map R using well-known iris data. can see (1) group large sepal width small three lengths (setosa), (2) group small medium four lengths (versicolor) (3) group large four lengths (virginica).plots clearly show simultaneously (1) kind patterns multiple variables appear data, kind clusters (output layer) seem data set, (2) characteristic cluster related third variable, iris spices.\nHence, helpful matter understand data visually proceed train classification model.can check convergence iteration minimization distance plotting path average distance closest output layer unit.can also plot counts data points belong output unit.","code":"\n  set.seed(5702)\n  \n  ##\"som\" function needs to have matrix data (not data.frame) as data.\n  iris.mx <- as.matrix(iris[,1:4])\n  \n  ## We first have to specify the shape of the output layer\n  ## \"topo\" specifies the shape of 2D-output layer\n  ## \"xdim\" and \"ydim\" specifies the dimension of the output layer\n  g1 <- somgrid(topo=\"hexagonal\",xdim=4,ydim=4)\n  \n  ## Then we train the self-organized map by our iris data\n  ## \"rlen\" specifies the number of iteration of the minimization of distances\n  iris.som <- som(iris.mx,g1,rlen=200)\n  \n  ## When we visualize the distribution of values in some other column in each output unit, we have to make the column as integer. \n  ## Here, we take iris species as label.code (1=setosa,2=versicolor, and 3=virginica)\n  label.code <- as.integer(iris[,5])\n  \n  par(mfrow=c(1,2)) \n  ## plot the each unit and their typical relative value of each variable\n  plot(iris.som, type='codes')  \n  ## plot the distribution of iris species in each output unit \n  plot(iris.som,type=\"mapping\",labels=label.code,col=label.code)\n  plot(iris.som, type='change')\n   plot(iris.som, type='counts', palette.name = cm.colors)"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"titanic-data","chapter":"73 Recursive codes and self-organized map with R","heading":"73.2.2 Titanic data","text":"method can applied non-numeric data convert discrete variables integers.\nillustration Titanic data. clearly shows effective variable predict survival rate sex. Note set sex 1:man, 2:woman, survival 1:survived 2:survived.","code":"\n  set.seed(5702)\n\n  d1 <- as.data.frame(Titanic)\n  \n  Tita <- d1[rep(seq_len(nrow(d1)), d1$Freq),1:4]\n  \n  row.names(Tita) <- NULL\n  \n  Tita_n <- Tita %>% \n    mutate(class_n=ifelse(Class==\"3rd\",3,ifelse(Class==\"2nd\",2,ifelse(Class==\"1st\",1,4)))) %>%\n    mutate(sex_n=ifelse(Sex==\"Male\",1,2)) %>%\n    mutate(Age_n=ifelse(Age==\"Adult\",2,1)) %>%\n    mutate(Survived_n=ifelse(Survived==\"Yes\",1,2))\n  \n  g2 <- somgrid(topo=\"hexagonal\",xdim=5,ydim=5)\n  \n  Tita.som <- som(as.matrix(Tita_n[,5:7]),g2,rlen=500)\n  \n  par(mfrow=c(1,2)) \n  plot(Tita.som, type='codes')\n  plot(Tita.som,type=\"mapping\",labels=Tita_n[,8],col=Tita_n[,8])"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"limitation-in-mapping-plot","chapter":"73 Recursive codes and self-organized map with R","heading":"73.2.3 Limitation in Mapping plot","text":"third example lending data openintro package. plots , one can see sample size increases, mapping plot becomes hard see, limit package kohonen. large sample data, randomly sample data first, plot self-organized map.","code":"\n  set.seed(5702)\n\n  Lend <- loans_full_schema %>%\n    select(annual_income,total_credit_limit,interest_rate,balance,num_historical_failed_to_pay)\n\n  g3 <- somgrid(topo=\"hexagonal\",xdim=5,ydim=7)\n  \n  Lend.som <- som(as.matrix(Lend[,1:4]),g3,rlen=2000)\n  \n  par(mfrow=c(1,2)) \n  plot(Lend.som, type='codes')\n  plot(Lend.som,type=\"mapping\",labels=as.matrix(Lend[,5]),col=as.matrix(Lend[,5]))"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"limitation-in-codes-plot","chapter":"73 Recursive codes and self-organized map with R","heading":"73.2.4 Limitation in Codes plot","text":"last example data fifty states USA datasets package.\nGenerally, number variables increases, codes plot also getting hard see.","code":"\n  set.seed(5702)\n\n  States <- state.x77\n\n  g4 <- somgrid(topo=\"hexagonal\",xdim=3,ydim=3)\n  \n  Lend.som <- som(States,g4,rlen=200)\n  \n  par(mfrow=c(1,2)) \n  plot(Lend.som, type='codes',palette.name = rainbow)\n  plot(Lend.som,type=\"mapping\",labels=as.matrix(rownames(States)))"},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"three-situations-that-i-think-ggplot-is-the-most-useful-in-practice","chapter":"73 Recursive codes and self-organized map with R","heading":"73.3 Three situations that I think “ggplot” is the most useful in practice:","text":"","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"motivation-6","chapter":"73 Recursive codes and self-organized map with R","heading":"73.3.1 Motivation","text":"chapter, based six-year experience credit risk manager commercial bank, introduce three situations think “ggplot” useful real work even non-specialists data science.practice, used Excel time, people use “ggplot” datavis tools requiring coding.\nimportant one worker moves another department quits /job therefore hands successor, successor necessarily specialist Data Science can stably handle tools, .\nSpecifically, datavis job done 2021 reported general managers, often case needs done next year month keep tracking relevant changes.\n, original contributor leaves /role next year, role accomplished successor.\nTherefore, believe normal companies, datavis job remain done Excel time .said, also believe blending uses “ggplot” suitable datavis tools Excel undoubtedly better often difficult time-consuming datavis manipulation Excel.\nHereafter, introduce datavis manipulations (already learned class) use “ggplot” partial supportive tool.\nmay ignite non-specialist workers’ interest “ggplot” spread use office.","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"situation-1.-attaching-labels-in-scatter-plotbubble-chart-not-to-overlap-with-each-other","chapter":"73 Recursive codes and self-organized map with R","heading":"73.3.2 Situation 1. Attaching labels in scatter plot/bubble chart not to overlap with each other","text":"practice, tons situations use scatter plots bubble charts.\nTypically, many data points scatter plots bubbles bubble charts.\nHence, attach labels points/bubbles plot/chart, often overlap , making difficult read.believe Excel automatically manipulate overlap.\nHence, “ggplot” comes .\nintroduced class, use geom_text_repel function ggrepel package.","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"situation-2.-cleveland-dot-plots","chapter":"73 Recursive codes and self-organized map with R","heading":"73.3.3 Situation 2. Cleveland dot plots","text":"practice, often want make comparisons multiple indices simultaneously, Cleveland dot plots good .\nthink situation, non-specialist workers typically use multiple bar charts multiple line graphs.\nHowever, pointed class, multiple bar charts difficult read general, multiple line graphs often become redundant hard read especially lines crossing .\nEven though can draw “Cleveland dot plots”-like plot using scatter plots function Excel, time-consuming set x/y-axis values multiple legends/colors properly.","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"situation-3.-multivariate-plots","chapter":"73 Recursive codes and self-organized map with R","heading":"73.3.4 Situation 3. Multivariate plots","text":"practice, often want observe relationship X Y affected third variable Z.\nAmong many datavis tools introduced class mosaic plots alluvial plots, believe can drawn Excel best knowledge.\nHence, point view, feel much possibilities “ggplot” spread daily office.","code":""},{"path":"recursive-codes-and-self-organized-map-with-r.html","id":"references-5","chapter":"73 Recursive codes and self-organized map with R","heading":"73.4 References","text":"[1] Bruno Woltzenlogel Paleo, Levenshtein Distance: Two Applications Database Record Linkage Natural Language Processing, LAP LAMBERT Academic Publishing[2] Package “kohonen”: https://cran.r-project.org/web/packages/kohonen/kohonen.pdf[3] Examples self-organized map R, Doshisha University: https://www1.doshisha.ac.jp/~mjin/R/Chap_30/30.html","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"tutorial-of-three-ggplot2-based-packages","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74 Tutorial of three ggplot2 based packages","text":"Xiaorui QinThe package ggplot2, founded Hadley Wickham 2005 updated significantly April 2012, one powerful graphics frameworks R. Based graphics grammar, ggplot2 underlying grammar, helps us compose graphs combining independent components.order simplify operations ggplot2, packages emerge times require, e.g., official extension mechanism ggplot2 based packages.However, many tools well documented people spend much time searching information want use . community contribution, work tutorial three ggplot2 based packages, .e., ggrepel, gganimate, ggpubr, integrate information give examples help researchers use three packages easier.Note tutorial, examples give Problem Set 1-3, familiar understand easier.","code":"\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(devtools)\n# remotes::install_github(\"dgrtwo/gganimate\", force = TRUE)\nlibrary(gganimate) # must be installed from source\nlibrary(ggpubr)\nlibrary(gapminder)\nlibrary(openintro)\nlibrary(tidyverse)"},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"ggrepel","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.1 ggrepel","text":"","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"overview-4","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.1.1 Overview","text":"First, introduce ggrepel, extension ggplot2. main function ggrepel repel overlapping text labels away away data points label. label overlaps labels, ggrepel repel overlapping labels . label overlaps data points, ggrepel repel overlapping labels data points.section, give examples Problem Set 2 continuously optimize label distribution make plots clearer beautiful.","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"installation-loading","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.1.2 Installation & Loading","text":"install ggrepel CRAN directly load mentioned top tutorial.","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"usage-and-contribution","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.1.3 Usage and Contribution","text":"Recall “3. House sizes prices” Problem Set 2, want draw scatterplot price vs. area 3.. want show PIDs (Parcel identification number - can used city web site parcel review) labels data points scatterplot, e.g., data points whose area larger 3200, implement function follows:However, noticed many text labels PIDs overlapping labels data points , make plot clear beautiful. even distinguish labels data points. solve problem, use “geom_text_repel” ggrepel repel overlapping text labels data points follows:plot, see text labels repelled data points. whole plot clear easy distinguish labels. addition, little line segments added help us match labels corresponding data points.Based , also try make plot clear beautiful. example, try using “geom_label_repel” (based geom_label) ggrepel. operation places labels small box makes labels clear.also place shadows (glow) underneath text label enhance readability text shown plot .plot, add curved line segments indicate match text labels corresponding data points clearly. design curvature angle line segments.plot, add arrows curved line segments indicate match text labels corresponding data points better. design length arrows.Finally, use different line types segment lines, .e., dotted lines, make plot beautiful. conclude now, modified plot ggrepel clear beautiful. easy us distinguish labels get information quickly.","code":"\nggplot(data = ames, aes(x = area, y = price)) +\n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2) +\n  geom_text(aes(x = area, y = price, label = ifelse(area > 3200, PID, \"\")))+\n  theme_classic()\nggplot(data = ames, aes(x = area, y = price)) +\n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2) +\n  geom_text_repel(aes(x = area, y = price, label = ifelse(area > 3200, PID, \"\")))+\n  theme_classic()\nggplot(data = ames, aes(x = area, y = price)) + \n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2)+\n  geom_label_repel(aes(x = area, y = price,\n                       label = ifelse(area > 3200, PID, \"\")), fontface = \"bold\",\n                   color = \"red\") +\n  theme_classic()\nggplot(data = ames, aes(x = area, y = price)) +\n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2) +\n  geom_text_repel(aes(x = area, y = price, label = ifelse(area > 3200, PID, \"\")),\n                  color = \"grey\",\n                  bg.color = \"black\",\n                  bg.r = 0.15)+\n  theme_classic()\nggplot(data = ames, aes(x = area, y = price)) +\n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2) +\n  geom_text_repel(aes(x = area, y = price, label = ifelse(area > 3200, PID, \"\")),\n                  box.padding = 0.5,\n                  segment.curvature = -0.1,\n                  segment.ncp = 4,\n                  segment.angle = 25) +\n  theme_classic()\nggplot(data = ames, aes(x = area, y = price)) +\n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2) +\n  geom_text_repel(aes(x = area, y = price, label = ifelse(area > 3200, PID, \"\")),\n                  box.padding = 0.5,\n                  segment.curvature = -0.1,\n                  segment.ncp = 4,\n                  segment.angle = 25,\n                  arrow = arrow(length = unit(0.02, \"npc\"))) +\n  theme_classic()\nggplot(data = ames, aes(x = area, y = price)) +\n  geom_point(color = \"blue\", alpha = 0.3, size = 2.2) +\n  geom_text_repel(aes(x = area, y = price, label = ifelse(area > 3200, PID, \"\")),\n                  box.padding = 0.5,\n                  segment.linetype = 6,\n                  segment.curvature = -0.1,\n                  segment.ncp = 4,\n                  segment.angle = 25,\n                  arrow = arrow(length = unit(0.02, \"npc\"))) +\n  theme_classic()"},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"gganimate","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.2 gganimate","text":"","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"overview-5","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.2.1 Overview","text":"Second, introduce gganimate, extension ggplot2. main function gganimate draw animation. basis ggplot2, adds aesthetic mapping frame.section, give examples dataset “gapminder” draw optimize animation data changes time. (discover proper dataset Problem Sets, use proper one.)","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"installation-loading-1","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.2.2 Installation & Loading","text":"use package, need install ImageMagick first. install load package mentioned top tutorial. Note requires installation package source (GitHub installation).","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"usage-and-contribution-1","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.2.3 Usage and Contribution","text":"use dataset named “gapminder” example. dataset includes data GDP growth per capita, life expectancy population growth major countries world 1952 2007. want show population change countries different continents 1952-2007. Thus show static plot first. put “pop” population y-axis “continent” x-axis. Different counties different colors.try changing static plot animation. Since focus change population time, choose “year” transition time. following animation, easily discover population change 1952-2007 countries five continents.optimize animation, add “little tails” data points reflect direction changing following animation “shadow_wake(wake_length = 0.1, alpha = FALSE)”.","code":"\nggplot(data = gapminder, aes(x = continent, y = pop, colour = country)) +\n  geom_point(alpha = 0.7, size = 3, show.legend = FALSE) +\n  theme_classic()\nggplot(data = gapminder, aes(x = continent, y = pop, colour = country)) +\n  geom_point(alpha = 0.7, size = 3, show.legend = FALSE) +\n  transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  theme_classic()\nggplot(data = gapminder, aes(x = continent, y = pop, colour = country)) +\n  geom_point(alpha = 0.7, size = 3, show.legend = FALSE) +\n  transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_wake(wake_length = 0.15, alpha = FALSE) +\n  theme_classic()"},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"ggpubr","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.3 ggpubr","text":"","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"overview-6","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.3.1 Overview","text":"Finally, introduce ggpubr. ggpubr actually package developed based ggplot2. purpose simplify operations ggplot2 draw diagram meets requirements.section, give examples “1. Fast Food” Problem Set 1 draw histograms, density plots, box plots, violin plots ggpubr.","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"installation-loading-2","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.3.2 Installation & Loading","text":"install ggpubr CRAN directly load mentioned top tutorial.","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"usage-and-contribution-2","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.3.3 Usage and Contribution","text":"Recall “1. Fast Food” Problem Set 1, want analyze whether crispy items calories grilled items 1.c. asked create new variable, cooktype “Crispy” item name contains “Crispy” “Grilled” contains “Grilled”. plot overlapping density curves calories, one curve Crispy one curve Grilled, single set axes. curve different color. , borrow problem study tools ggpubr.First, create cooktype, .e., Grilled Crispy, show corresponding overlapping histogram. also add mean lines show mean values cooktype.Second, show corresponding overlapping density plot, similar 1.c Problem Set 1. also add mean lines show mean values cooktype.Based data, show box plot . plot helps us observe distribution whole situation different cooktypes clearly.Finally, draw violin plot based data. violin plot much clearly box plot show distribution whole situation different cooktypes clearly.","code":"\nprocessed_data <- filter(fastfood, xor(str_detect(fastfood$item, \"Grilled\"), str_detect(fastfood$item, \"Crispy\")))\nfiltered_data <- processed_data %>% mutate(cooktype = case_when(\n  str_detect(processed_data$item, \"Grilled\") ~ \"Grilled\",\n  str_detect(processed_data$item, \"Crispy\") ~ \"Crispy\"\n))\ngghistogram(data = filtered_data,\n            x = \"calories\",\n            add = \"mean\",\n            color = \"cooktype\",\n            fill = \"cooktype\",\n            bins = 30)\nggdensity(data = filtered_data,\n          x = \"calories\",\n          add = \"mean\", \n          color = \"cooktype\")\nggboxplot(data = filtered_data,\n          x = \"cooktype\", \n          y = \"calories\",\n          color = \"cooktype\",  \n          add = \"jitter\", shape = \"cooktype\")\nggviolin(data = filtered_data,\n         x = \"cooktype\", \n         y = \"calories\",\n         fill = \"cooktype\",\n         add = \"boxplot\", \n         add.params = list(fill = \"grey\")) +\n  stat_compare_means(label.y = 50)"},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"conlusion","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.4 Conlusion","text":"community contribution, write tutorial three ggplot2 based packages, .e., ggrepel, gganimate, ggpubr, well documented .tutorial, people use three tools easily capture information quickly examples. try use examples problem sets, familiar understand easily. evaluation project.also discuss learned project might differently next time. project, learned memorized specific usage functions three packages, also strengthened understanding ggplot2 related packages writing examples . process selecting examples, also deeply analyzed “kind datasets suitable package” “kind datasets can package mainly helps”. process strengthens understanding Data Analysis Visualization. example, ggrepl example, chose scatterplot optimize. working Problem Set 2, found text labels overlap draw scatterplots. time, since asked show certain labels, avoided problem. However, still went search solve problem found can use ggrepl package. Therefore, learned several ggplot2 based packages found practical can easily help solve many little problems ggplot2. example gganimate, chose dataset changes time, since think necessary animate plot study type data, can see change data time clearly directly.future, want learn ggplot2 based packages, ggiraph, ggstance, ggalt, ggforce, ggpmisc, ggthemes, etc. packages can also regarded “skills” help solve tricky problems Data Analysis Visualization. time, learning packages also help strengthen understanding abilities use ggplot2.","code":""},{"path":"tutorial-of-three-ggplot2-based-packages.html","id":"sources-6","chapter":"74 Tutorial of three ggplot2 based packages","heading":"74.5 Sources","text":"https://ggplot2-book.org/introduction.htmlhttps://ggrepel.slowkow.com/articles/examples.htmlhttps://mran.microsoft.com/snapshot/2017-08-20/web/packages/ggrepel/vignettes/ggrepel.htmlhttps://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.htmlhttps://zhuanlan.zhihu.com/p/33391501https://zhuanlan.zhihu.com/p/95665626https://zhuanlan.zhihu.com/p/36543477https://zhuanlan.zhihu.com/p/102653888","code":""},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"a-step-by-step-tutorial-for-natural-language-processing-in-r","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75 A Step by Step Tutorial for Natural Language Processing in R","text":"Yuwen ZhangNatural language processing (NLP) intersection linguistics, computer science, artificial intelligence aiming tackle interactions computers human language, particularly program computers process analyze large amounts natural language data. goal train computer can “understand” contents documents, including contextual nuances language within .document contains short tutorial perform NLP. provides general idea steps taken dealing natural language data.use dataset reviews Rotten Tomatoes (https://www.rottentomatoes.com/) demo. goal predict whether review type “Fresh” “Rotten” based review content.","code":""},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"step-0-exploratory-analysis","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75.0.0.1 Step 0 Exploratory Analysis","text":"first step NLP analysis, ’s often good idea perform exploratory analysis dataset get general idea data working , distribution values feature, missingness, etc. example, review_type, rough 9:5 ratio Fresh:Rotten, 5.8% empty reviews review_content.** order speed process, randomly sampled 40,000 reviews analysis","code":"\nreviews = read.csv(unz(\"resources/short_guide_for_NLP/rotten_tomatoes_critic_reviews.csv.zip\",\n                       \"rotten_tomatoes_critic_reviews.csv\"),\n                   colClasses=c(\"NULL\", \"NULL\", \"NULL\", \"NULL\", NA, \"NULL\", \"NULL\", NA))\n\nreviews$review_val = ifelse(reviews$review_type == \"Fresh\",1,0)\n\nprint(reviews %>% count(review_type))##   review_type      n\n## 1       Fresh 720210\n## 2      Rotten 409807\nempty_count = sum(reviews$review_content == \"\")\nempty_ratio = empty_count / nrow(reviews)\nempty_table = data.frame(\"Empty_Count\" = c(empty_count), \"Empty_Ratio\" = c(empty_ratio))\nprint(empty_table)##   Empty_Count Empty_Ratio\n## 1       65806  0.05823452\nset.seed(10)\nreviews_sample = reviews %>%\n  filter(review_content != \"\") %>%\n  sample_n(40000)\n\nknitr::kable(head(reviews_sample))"},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"step-1-data-cleaning","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75.0.0.2 Step 1 Data Cleaning","text":"Usually data hand standardized enough directly perform training, therefore necessary clean data first, lowercase words (“Hello”, “hello”, “hELLo” “hello”), remove irrelevant characters (“- ” “”), lemmatize words (“”, “”, “” common form “”), etc. However, need smart approach, questions consider experience NLPLowercase words: need lowercase words? Especially case sentiment analysis, specific capitalization reveal information sentiment? example, “movie” may mean question, “MOVIE” may convey stronger emotion upset confused.Lowercase words: need lowercase words? Especially case sentiment analysis, specific capitalization reveal information sentiment? example, “movie” may mean question, “MOVIE” may convey stronger emotion upset confused.Remove irrelevant characters: characters considering. example, periods (.) commas (,) sentence maybe irrelevant analysis, question marks (?) exclamation marks (!)? “don’t like movie.” vs. “don’t like movie!” convey magnitude emotion?Remove irrelevant characters: characters considering. example, periods (.) commas (,) sentence maybe irrelevant analysis, question marks (?) exclamation marks (!)? “don’t like movie.” vs. “don’t like movie!” convey magnitude emotion?interaction words also something consider. example, “good” versus “good” “good” sentence completely opposite meanings, “” interacting “good” second sentence.interaction words also something consider. example, “good” versus “good” “good” sentence completely opposite meanings, “” interacting “good” second sentence.tutorial, replace contraction words original form (’m , Don’t ), remove characters except question marks exclamation marks, lowercase words except words completely capitalized length > 1 (exclude words like ), add interaction term ‘’ following word (example, “good”, formatted sentence “good notgood”), lemmatize words.","code":"\nreviews_sample$review_content = replace_contraction(reviews_sample$review_content)\nreviews_sample$review_content = gsub(\"[^A-Za-z0-9 !?]\",\"\", reviews_sample$review_content)\nreviews_sample$review_content = gsub(\"([!?])\", \" \\\\1 \\\\2\", reviews_sample$review_content)\n\nreviews_sample = reviews_sample %>%\n  filter(review_content != \"\")\n\nfor (i in 1:nrow(reviews_sample)) {\n  review_content = reviews_sample$review_content[i]\n  review_split =  strsplit(review_content, \" +\")[[1]]\n  len = length(review_split)\n  for (j in 1:len) {\n    if (review_split[j] != toupper(review_split[j]) || nchar(review_split[j]) == 1) {\n      review_split[j] = tolower(review_split[j])\n    }\n    if (len > 1 && j > 1 && tolower(review_split[j-1]) == \"not\") {\n        review_split = c(review_split, paste(review_split[j-1], review_split[j], sep=\"\"))\n    }\n  }\n  review_split = lemmatize_words(review_split)\n  reviews_sample$review_content[i] = paste(review_split, collapse=' ')\n}"},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"step-2-data-transformation","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75.0.0.3 Step 2 Data Transformation","text":"cleaning dataset, next thing process data can feed model. first thing split data training, testing dataset. need transform dataset. Currently dataset contains strings, ’s unhelpful training. Therefore, need represent data another format.\nOne way using One-hot Encoding (Bag Words), encode character individually number. example, two strings “correct ” (STRING 1) “better ” (STRING 2), bag words {“”, “”, “correct”, “better”, “”, “”} dataframe can converted called Document-Term Matrix (DTM):format convenient model building.step, enough proceed training. However, can transform dataset even .\nOne important thing consider frequency words within one category versus categories. goal basically find words can uniquely represent category, therefore, words like “”, “movie”, “” probably present categories, making bad indicators, words like “good”, “love”, “excellent” unique “Fresh” category “bad”, “terrible”, “hate” indicative “Rotten” category. better reflect , can perform TF-IDF calculation words. tf–idf, TF*IDF, TFIDF, short term frequency–inverse document frequency, numerical statistic intended reflect important word document collection corpus. Term frequency, tf(t,d), frequency term t, inverse document frequency, idf(t,D), measure much information word provides, .e., ’s common rare across documents.calculation:\\({\\displaystyle \\mathrm {tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\d}{f_{t',d}}}}}\\)\\({\\displaystyle \\mathrm{idf}(t, D) = \\log \\frac{N}{|\\{d \\D: t \\d\\}|}}\\)\n\ntf–idf calculated :\\({\\displaystyle \\mathrm {tfidf} (t,d,D)=\\mathrm {tf} (t,d)\\cdot \\mathrm {idf} (t,D)}\\)wrap , first create DTM datasets, compute tf–idf DTM. However, real life, don’t want perform steps entire dataset, logically know information validation testing data. prevent leakage, perform fit_transform training data transform validation testing data, validation testing data transformed based information, information training data.","code":"                 \"i\"       \"am\"       \"correct\"       \"better\"        \"than\"        \"her\"\n    STRING 1      2          2             1               0             0             0\n    STRING 2      1          1             0               1             1             1\ntrain_test = initial_split(reviews_sample, prop = 0.75)\ntest = train_test %>%\n  testing()\ntrain = train_test %>%\n  training()\n\nmodel_tfidf = TfIdf$new(smooth_idf = TRUE)\n\ntrain_tokens = space_tokenizer(train$review_content, \" \")\ntrain_dtm = create_dtm(itoken(train_tokens), hash_vectorizer())\ntrain_tfidf = model_tfidf$fit_transform(train_dtm)\n\ntest_tokens = space_tokenizer(test$review_content, \" \")\ntest_dtm = create_dtm(itoken(test_tokens), hash_vectorizer())\ntest_tfidf = model_tfidf$transform(test_dtm)"},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"step-3-hyperparameter-tuning","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75.0.0.4 Step 3 Hyperparameter Tuning","text":"fitting model, need find best parameters model can give optimal result. step, use glmnet example training method (glmnet stands Lasso Elastic-Net Regularized Generalized Linear Models, extremely efficient procedures fitting entire lasso elastic-net regularization path linear regression, logistic multinomial regression models, Poisson regression, Cox model, multiple-response Gaussian, grouped multinomial regression. glmnet also one models can deal sparse matrix R). parameters tunning lambdas, use cross validation method find model minimum mean error corresponding parameters.can see, best lambda value tuned around 0.3981","code":"\nlambda_vals = 10^seq(3, -2, by = -.1)\ncv_glm_fit = cv.glmnet(x = train_tfidf, y = train$review_type,\n                       family = 'binomial', alpha = 0, lambda = lambda_vals, nfolds = 5)\nopt_lambda = cv_glm_fit$lambda.min\n\n\nknitr::kable(data.frame(Best_lambda = c(opt_lambda)))"},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"step-4-model-training-evaluation","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75.0.0.5 Step 4 Model training & Evaluation","text":"getting optimum hyperparameters, use train model, predit using test data.can see, get around 73% correctness model, bad!","code":"\nglm_fit = cv_glm_fit$glmnet.fit\npred_class = predict(glm_fit, s = opt_lambda, newx = test_tfidf, type='class')\n\nknitr::kable(data.frame(accuracy = c(1-mean(pred_class != test$review_type))))"},{"path":"a-step-by-step-tutorial-for-natural-language-processing-in-r.html","id":"citation-httpswww.kaggle.comstefanoleone992rotten-tomatoes-movies-and-critic-reviews-dataset","chapter":"75 A Step by Step Tutorial for Natural Language Processing in R","heading":"75.0.0.6 Citation: https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset","text":"","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"tutorial-for-ggvis-and-its-comparison-with-ggplot2","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76 Tutorial for ggvis and its Comparison with ggplot2","text":"Anbang Wang Keyi Guo","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"introduction-12","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.1 Introduction","text":"\nR users, ggplot2 name can hardly ignored. data visualization package, ggplot2 can actually take care details long provide data tell map variables aesthetics. certain features ggvis can practical easy use. addition, ggvis special interactive plots enables create complex, aesthetically pleasing charts interactive features. Therefore, can generate similar graphs ggplot2 dataset, can also generate interactive graphs. tutorial, include ways generating static plots ggvis, comparison ggplot2, also video tutorial interactive part.","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"install-and-load-ggvis-and-ggplot2-packages","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.2 Install and load “ggvis” and “ggplot2” packages","text":"\nShiny R package makes easy build interactive web apps straight R.","code":"\n#remotes::install_github(\"rstudio/shiny\")\nlibrary(shiny)\nlibrary(ggvis)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"load-dataset","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.3 Load dataset","text":"","code":"\nlibrary(datasets)\ndata(iris)"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"comparison-between-ggvis-and-ggplot","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.4 Comparison Between ggvis and ggplot","text":"","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"histogram-1","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.5 Histogram","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotThe default histograms ggvis ggplot similar different bin_widthThe default method used drawing ggplot histogram contain strokes","code":"\n#View(iris)\n\niris%>%\n  ggvis(~Sepal.Length)%>%\n  layer_histograms()\nggplot(iris, aes(x=Sepal.Length)) + \n  geom_histogram()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"scatter-plots","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.6 Scatter Plots","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotDifferences default methods drawing scatter plot:\nPoint size\nTheme\nX-y scales labels\nPoint sizeThemeX-y scales labels","code":"\niris %>% \n  ggvis(~Petal.Length, ~Petal.Width) %>% \n  layer_points()\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) + \n  geom_point()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"scatter-plots-customization","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.7 Scatter Plots (Customization)","text":"changing argument’s value, can actually create plot style.","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"fill-change-the-color-of-the-dots","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.7.1 fill (change the color of the dots)","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplot","code":"\niris %>% \n  ggvis(~Petal.Length, ~Petal.Width, fill=~Sepal.Length) %>% \n  layer_points()\nggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour = Sepal.Length))+\n  geom_point()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"size-change-dots-size","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.7.2 size (change dots size)","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplot","code":"\niris %>% \n  ggvis(~Petal.Length, ~Petal.Width, size=~Sepal.Length) %>% \n  layer_points()\nggplot(iris, aes(x=Petal.Length, y=Petal.Width, size = Sepal.Length))+\n  geom_point()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"opacity-change-transparency-of-the-dots","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.7.3 Opacity ( change transparency of the dots)","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\niris %>% \n  ggvis(~Petal.Length, ~Petal.Width, size:=300, opacity = ~Sepal.Length) %>% \n  layer_points()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"line-plot","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.8 Line Plot","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotDifferences default methods drawing line plots:\nGgvis ggplot use different line color (black vs blue)\nDifferent theme\nGgplot draws smooth line confidence interval, whearas ggvis draws smooth line\nGgvis ggplot use different line color (black vs blue)Different themeGgplot draws smooth line confidence interval, whearas ggvis draws smooth line","code":"\niris %>%\n  ggvis(~Petal.Length, ~Sepal.Length) %>% \n  layer_smooths()\nggplot(iris, aes(x=Petal.Length, y=Sepal.Length))+\n  geom_smooth()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"line-plot-customization","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.9 Line Plot (Customization)","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\ngglotThere arguments can change make line plot looks pretty:\nStroke (color line)\nSE (/without confidence interval)\nSize/Strokewidth (line width)\nSpan (Controls amount smoothing default loess smoother)\nStroke (color line)SE (/without confidence interval)Size/Strokewidth (line width)Span (Controls amount smoothing default loess smoother)","code":"\niris %>%\n  ggvis(~Petal.Length, ~Sepal.Length) %>% \n  layer_smooths(stroke:='red', span = 0.3, se=TRUE, strokeWidth:=5)\nggplot(iris, aes(x=Petal.Length, y=Sepal.Length))+\n  geom_smooth(se=FALSE, colour = \"red\", size=3, span=0.3, position = \"identity\")"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"scatter-plots-1","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.10 Scatter plots","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotUse scatter plot draw relationship Petal_Length Petal_WidthDots clustered species irisAssign unique color shape dots clustered different group","code":"\niris %>% \n  ggvis(~Petal.Length, ~Petal.Width, shape = ~factor(Species)) %>% \n  layer_points(fill= ~Species)\nggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species, shape=Species)) + \n  geom_point(size=3)"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"scatter-plots-with-fit-line","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.11 Scatter plots with fit line","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotBy default, ggvis draws fit line entire plot, whereas ggplot draws fit line clustered dot group.","code":"\niris %>% \n  ggvis(~Petal.Length, ~Petal.Width) %>% \n  layer_points(fill= ~Species)%>%\n  layer_smooths(span = 0.5, stroke:= 'black')\nggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + \n  geom_point(size=3)+\n  geom_smooth()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"density-plots","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.12 Density Plots","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotggvis draws density plot certain amount transparency default, makes easier distinguish overlapped graphsggplot, hand, draws plot degree transparency. hard understand graphs cover one another","code":"\niris %>% \n  group_by(Species) %>% \n  ggvis(~Sepal.Length, fill = ~factor(Species)) %>% \n  layer_densities()\nggplot(data=iris, aes(x=Sepal.Length, group=Species, fill=Species)) +\n    geom_density()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"boxplot-1","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.13 Boxplot","text":"ggvis\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplotThe default method drawing box plot ggvis ggplot except use different themes","code":"\niris %>% \n  ggvis(~Species, ~Sepal.Length) %>% \n  layer_boxplots(fill := \"lightblue\")\nggplot(iris, aes(x=Species, y=Sepal.Length)) + \n  geom_boxplot(fill = \"lightblue\")"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"plot-for-time-series-data","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.14 Plot for Time Series Data","text":"ggvis ggplot2 actually similar methods deal time series data.ggvis’s layer_paths draw line whatever order appears data. , choose economics dataset visualize unemployment rate changes date.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nggplot2’s geom_path() connects observations order appear data. can generate similar output.can also plot two variables related time. , plotting unemployment personal savings rate time.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nBetter ggvis, ggplot supports coloring terms time. shows unemployment personal savings change time clearly.","code":"\neconomics %>% \n  ggvis(~date, ~unemploy) %>% \n  layer_paths()\nggplot(economics, aes(date, unemploy)) +\n  geom_path()\neconomics %>%\n  ggvis(~unemploy/pop, ~psavert) %>%\n  layer_paths()\nm <- ggplot(economics, aes(unemploy/pop, psavert))\nm + geom_path(aes(colour = as.numeric(date)))"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"heatmap","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.15 Heatmap","text":"\nggvis ggplot2 can create heatmap suitable dataset. , use UCBAdmissions dataset example.","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"ggvis","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.15.0.0.1 ggvis","text":"ggvis, can create heatmap using layer_rects().\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n\n* must set two x, x2, width, two y, y2 height. ordinal scale, better set width height prop_band() occupy complete band corresponding categorical value.","code":"\nadmit<- as.data.frame(xtabs(Freq ~ Dept + Admit, UCBAdmissions))\n\nadmit %>% \n  ggvis(~Dept, ~Admit, fill = ~Freq) %>% \n  layer_rects(width = band(), height = band()) %>%\n  scale_nominal(\"x\", padding = 0, points = FALSE) %>%\n  scale_nominal(\"y\", padding = 0, points = FALSE)"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"ggplot","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.15.0.0.2 ggplot","text":"ggplot2, use geom_tile() heatmap. basic heatmap.\nClearly ggplot2 much easier tool use building heatmaps. Note two plots, rows Admitted Rejected swapped positions.","code":"\nggplot(admit, aes(Dept, Admit,fill=Freq)) +\n  geom_tile()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"interactive-graphs","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16 Interactive Graphs","text":"\n’s special ggvis interactive controls present graphical information straightforward vivid. allows us see differences changing argument values passed . order better show visualization features ggvis, include brief video tutorial .","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"link-here-httpswww.youtube.comwatchvjtcjgadyx4a","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.1 link here : https://www.youtube.com/watch?v=jTcjgaDyx4A","text":"ggvis designs series functions produce interactive controls. start basic ones, produces similar results statis plots. replacing constant values functions.","code":""},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"input_slider","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.1.1 input_slider","text":"\ninput_slider provide interactive slider. , use control size opacity scatter plot.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n\ncan also adjust span value fit line.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\niris %>% \n  ggvis(~Sepal.Length, ~Petal.Length, \n    size := input_slider(10, 100),\n    opacity := input_slider(0, 1)\n  ) %>% \n  layer_points()\niris %>%\n  ggvis(~Sepal.Length, ~Petal.Length) %>%\n  layer_smooths(span = input_slider(0.5, 1, value = 1)) %>%\n  layer_points(size := input_slider(20, 200, value = 100),\n               opacity := input_slider(0, 1))"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"input_numeric","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.1.2 input_numeric","text":"\ninput_numeric another way can use adjust size. Unlike input_slider, allows numbers comes spin box control.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\nsize_num <- input_numeric(label = \"Point size\", value = 25)\niris %>% \n  ggvis(~Sepal.Length, ~Petal.Length, size := size_num) %>% \n  layer_points()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"input_text","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.1.3 input_text","text":"\ninput_text allows kind input values. Therefore, can also used adjust colors without specify series options.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\nfill_text <- input_text(label = \"Point color\", value = \"lightblue\")\niris %>% \n  ggvis(~Sepal.Length, fill := fill_text) %>% \n  layer_bars()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"input_select","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.1.4 input_select","text":"\ncan use input_select provide series options values like shape color. provide options using “choices” argument.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\niris %>%\n  ggvis(~Sepal.Length, ~Petal.Length, fillOpacity := 0.5,\n        shape := input_select(label = \"Choose shape:\",\n                              choices = c(\"circle\", \"square\", \"cross\", \"diamond\", \"triangle-up\", \"triangle-down\")),\n        fill := input_select(label = \"Choose color:\", \n                             choices = c(\"black\", \"red\", \"blue\", \"green\"))) %>%\n  layer_points()"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"input_checkbox","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.1.5 input_checkbox","text":"\ninput_checkbox controls different values using “map” function. creates interactive checkbox. map function become valid checkbox checked.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\n# Used with a map function, to convert the boolean to another type of value\nmodel_type <- input_checkbox(label = \"Use flexible curve\",\n  map = function(val) if(val) \"loess\" else \"lm\")\niris %>% \n  ggvis(~Sepal.Length, ~Petal.Length) %>%\n  layer_model_predictions(model = model_type)"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"combine-different-methods","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.2 Combine different methods","text":"\ncan use input_slider input_select different types interactive effects. density graph, use input_slider adjust bandwidth, input_select provide series kernel options.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n\nSimilarly, use input_select input_slider box plot color stroke width control.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\niris %>% \n  ggvis(x = ~Petal.Width) %>%\n    layer_densities(\n      adjust = input_slider(.1, 2, value = 1, step = .1, label = \"Bandwidth adjustment\"),\n      kernel = input_select(\n        c(\"Gaussian\" = \"gaussian\",\n          \"Epanechnikov\" = \"epanechnikov\",\n          \"Rectangular\" = \"rectangular\",\n          \"Triangular\" = \"triangular\",\n          \"Biweight\" = \"biweight\",\n          \"Cosine\" = \"cosine\",\n          \"Optcosine\" = \"optcosine\"),\n        label = \"Kernel\")\n    )\niris %>% \n  ggvis(~Species, ~Sepal.Length) %>% \n  layer_boxplots(fill := input_select(label = \"Choose color:\", \n                             choices = c(\"green\", \"red\", \"blue\", \"grey\")),\n                 strokeWidth := input_slider(0.1,5))"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"multiple-outputs","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.16.3 Multiple Outputs","text":"\ncan also use one slider multiple outputs control. slider pre-defined, can use different functions adjust values time.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\n","code":"\nslider <- input_slider(10, 1000)\niris %>% ggvis(~Sepal.Length, ~Petal.Length) %>%\n  layer_points(fill := \"red\", stroke:= \"black\", size := slider) %>%\n  layer_points(stroke := \"black\", fill := NA, size := slider)"},{"path":"tutorial-for-ggvis-and-its-comparison-with-ggplot2.html","id":"references-6","chapter":"76 Tutorial for ggvis and its Comparison with ggplot2","heading":"76.17 References","text":"Package ’ggvis: ’https://cran.r-project.org/web/packages/ggvis/ggvis.pdfA Short Introduction ggvis: ‘https://towardsdatascience.com/-short-introduction--ggvis-52a4c104df71’Quick ggvis examples: ‘https://ggvis.rstudio.com/0.1/quick-examples.html’Data Visualization R ggvis Package: ‘https://dk81.github.io/dkmathstats_site/rvisual-ggvis-guide.html’","code":""},{"path":"feature-selection-in-r.html","id":"feature-selection-in-r","chapter":"77 Feature selection in r","heading":"77 Feature selection in r","text":"Zehui Wu","code":"\nlibrary(ggcorrplot)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(gam)"},{"path":"feature-selection-in-r.html","id":"introduction-13","chapter":"77 Feature selection in r","heading":"77.1 Introduction","text":"Feature selection one important tasks boost performance machine learning models. benefits feature selections include:Better Accuracy: removing irrelevant features let models make decisions using important features. experience, classification models can usually get 5 10 percent improvement accuracy scores feature selection.Better Accuracy: removing irrelevant features let models make decisions using important features. experience, classification models can usually get 5 10 percent improvement accuracy scores feature selection.Avoid Overfitting: models put weights irrelevant features.Avoid Overfitting: models put weights irrelevant features.Improve running time: decreasing dimension data makes models run faster.Improve running time: decreasing dimension data makes models run faster.tutorial, introduce intuitive popular methods R feature selection.","code":""},{"path":"feature-selection-in-r.html","id":"correlation-matrix","chapter":"77 Feature selection in r","heading":"77.2 Correlation Matrix","text":"Correlation matrix popular method feature selection. using correlation matrix, can see correlation pair numerical variables. can filter variables low correlation dependent variable, also can remove redundant variables identifying highly correlated independent variables.illustrate use method, use Winequality regression dataset UCI.firstly use cor() function calculate correlation matrix.","code":"\n# load the data\nwinequality <- read.csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep = \";\")\nhead(winequality)##   fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n## 1           7.0             0.27        0.36           20.7     0.045\n## 2           6.3             0.30        0.34            1.6     0.049\n## 3           8.1             0.28        0.40            6.9     0.050\n## 4           7.2             0.23        0.32            8.5     0.058\n## 5           7.2             0.23        0.32            8.5     0.058\n## 6           8.1             0.28        0.40            6.9     0.050\n##   free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n## 1                  45                  170  1.0010 3.00      0.45     8.8\n## 2                  14                  132  0.9940 3.30      0.49     9.5\n## 3                  30                   97  0.9951 3.26      0.44    10.1\n## 4                  47                  186  0.9956 3.19      0.40     9.9\n## 5                  47                  186  0.9956 3.19      0.40     9.9\n## 6                  30                   97  0.9951 3.26      0.44    10.1\n##   quality\n## 1       6\n## 2       6\n## 3       6\n## 4       6\n## 5       6\n## 6       6\ncor_matrix <- data.frame(cor(winequality))\ncor_matrix##                      fixed.acidity volatile.acidity  citric.acid residual.sugar\n## fixed.acidity           1.00000000      -0.02269729  0.289180698     0.08902070\n## volatile.acidity       -0.02269729       1.00000000 -0.149471811     0.06428606\n## citric.acid             0.28918070      -0.14947181  1.000000000     0.09421162\n## residual.sugar          0.08902070       0.06428606  0.094211624     1.00000000\n## chlorides               0.02308564       0.07051157  0.114364448     0.08868454\n## free.sulfur.dioxide    -0.04939586      -0.09701194  0.094077221     0.29909835\n## total.sulfur.dioxide    0.09106976       0.08926050  0.121130798     0.40143931\n## density                 0.26533101       0.02711385  0.149502571     0.83896645\n## pH                     -0.42585829      -0.03191537 -0.163748211    -0.19413345\n## sulphates              -0.01714299      -0.03572815  0.062330940    -0.02666437\n## alcohol                -0.12088112       0.06771794 -0.075728730    -0.45063122\n## quality                -0.11366283      -0.19472297 -0.009209091    -0.09757683\n##                        chlorides free.sulfur.dioxide total.sulfur.dioxide\n## fixed.acidity         0.02308564       -0.0493958591          0.091069756\n## volatile.acidity      0.07051157       -0.0970119393          0.089260504\n## citric.acid           0.11436445        0.0940772210          0.121130798\n## residual.sugar        0.08868454        0.2990983537          0.401439311\n## chlorides             1.00000000        0.1013923521          0.198910300\n## free.sulfur.dioxide   0.10139235        1.0000000000          0.615500965\n## total.sulfur.dioxide  0.19891030        0.6155009650          1.000000000\n## density               0.25721132        0.2942104109          0.529881324\n## pH                   -0.09043946       -0.0006177961          0.002320972\n## sulphates             0.01676288        0.0592172458          0.134562367\n## alcohol              -0.36018871       -0.2501039415         -0.448892102\n## quality              -0.20993441        0.0081580671         -0.174737218\n##                          density            pH   sulphates     alcohol\n## fixed.acidity         0.26533101 -0.4258582910 -0.01714299 -0.12088112\n## volatile.acidity      0.02711385 -0.0319153683 -0.03572815  0.06771794\n## citric.acid           0.14950257 -0.1637482114  0.06233094 -0.07572873\n## residual.sugar        0.83896645 -0.1941334540 -0.02666437 -0.45063122\n## chlorides             0.25721132 -0.0904394560  0.01676288 -0.36018871\n## free.sulfur.dioxide   0.29421041 -0.0006177961  0.05921725 -0.25010394\n## total.sulfur.dioxide  0.52988132  0.0023209718  0.13456237 -0.44889210\n## density               1.00000000 -0.0935914935  0.07449315 -0.78013762\n## pH                   -0.09359149  1.0000000000  0.15595150  0.12143210\n## sulphates             0.07449315  0.1559514973  1.00000000 -0.01743277\n## alcohol              -0.78013762  0.1214320987 -0.01743277  1.00000000\n## quality              -0.30712331  0.0994272457  0.05367788  0.43557472\n##                           quality\n## fixed.acidity        -0.113662831\n## volatile.acidity     -0.194722969\n## citric.acid          -0.009209091\n## residual.sugar       -0.097576829\n## chlorides            -0.209934411\n## free.sulfur.dioxide   0.008158067\n## total.sulfur.dioxide -0.174737218\n## density              -0.307123313\n## pH                    0.099427246\n## sulphates             0.053677877\n## alcohol               0.435574715\n## quality               1.000000000"},{"path":"feature-selection-in-r.html","id":"heat-map-for-correlation-matrix","chapter":"77 Feature selection in r","heading":"77.2.1 heat map for correlation matrix","text":"can also draw quick heat map visualize correlation ggcorrplot library.can see free.Sulfur.dioxide citric.acid small correlations dependent variable quality. may consider try removing two features models.","code":"\nggcorrplot(cor_matrix)"},{"path":"feature-selection-in-r.html","id":"function-to-find-redundant-variables","chapter":"77 Feature selection in r","heading":"77.2.2 function to find redundant variables","text":"Besides using graph identify redundant variables, can also use findCorrelation function caret library. outputs index variables need delete. two variables higher correlation cutoff, function removes variable largest mean absolute correlation.case, function finds density highly correlated variables.","code":"\nfindCorrelation(cor(winequality), cutoff=0.75)## [1] 8"},{"path":"feature-selection-in-r.html","id":"variable-importance","chapter":"77 Feature selection in r","heading":"77.3 Variable Importance","text":"functions caret library evaluate variable importance. Typically, variable importance evaluation can separated two categories: ones use model information ones .","code":""},{"path":"feature-selection-in-r.html","id":"calculate-feature-importance-without-models","chapter":"77 Feature selection in r","heading":"77.3.1 calculate feature importance without models","text":"don’t want use specific model evaluate features, can use filterVarImp() function Caret library.classification, function uses ROC curve analysis predictor use area curve scores.regression, linear line fit pair dependent variable independent variables, absolute value t-statistic slope independent variable used.data set Winequality regression task, t-statistic slopes used. two variables citric.acid free.sulfur.dioxide low scores, consistent correlation analysis .","code":"\n#use roc_curve area as score\nroc_imp <- filterVarImp(x = winequality[,1:11], y = winequality$quality)\n\n#sort the score in decreasing order\nroc_imp <- data.frame(cbind(variable = rownames(roc_imp), score = roc_imp[,1]))\nroc_imp$score <- as.double(roc_imp$score)\nroc_imp[order(roc_imp$score,decreasing = TRUE),]##                variable      score\n## 11              alcohol 33.8584627\n## 8               density 22.5812112\n## 5             chlorides 15.0242170\n## 2      volatile.acidity 13.8909397\n## 7  total.sulfur.dioxide 12.4176561\n## 1         fixed.acidity  8.0050275\n## 9                    pH  6.9917109\n## 4        residual.sugar  6.8603271\n## 10            sulphates  3.7613401\n## 3           citric.acid  0.6444005\n## 6   free.sulfur.dioxide  0.5708506"},{"path":"feature-selection-in-r.html","id":"calculate-feature-importance-with-models","chapter":"77 Feature selection in r","heading":"77.3.2 calculate feature importance with models","text":"advantage using model-specific evaluation feature scores specific linked model performance using filter features may help improve performance specific model.models built-importance score. models, need use filterVarImp() function . check models available, see documentation varImp(): https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/varImpIn following example, show svm model importance score implemented varImp returns error.train random forest regression model data set use varImp() function calculate feature importance based tree model. case random forest model, varLmp() wrapper around importance functions randomForest library.many features want use 100 . can choose use top 100 based feature importance scores.can see ranking different previous one specifically linked performance random forest model.","code":"\n#library(e1071)\n#svmfit = svm(x= winequality[,1:11],y= winequality[,12], kernel = \"linear\", scale = TRUE)\n#roc_imp2 <- varImp(svmfit, scale = FALSE)\n#train random forest model and calculate feature importance\nrf = randomForest(x= winequality[,1:11],y= winequality[,12])\nvar_imp <- varImp(rf, scale = FALSE)\n#sort the score in decreasing order\nvar_imp_df <- data.frame(cbind(variable = rownames(var_imp), score = var_imp[,1]))\nvar_imp_df$score <- as.double(var_imp_df$score)\nvar_imp_df[order(var_imp_df$score,decreasing = TRUE),]##                variable    score\n## 11              alcohol 620.2923\n## 8               density 417.3228\n## 2      volatile.acidity 396.4858\n## 6   free.sulfur.dioxide 383.1622\n## 5             chlorides 304.2705\n## 7  total.sulfur.dioxide 293.9179\n## 4        residual.sugar 280.3937\n## 9                    pH 256.8479\n## 3           citric.acid 253.5606\n## 1         fixed.acidity 232.3765\n## 10            sulphates 222.2735\nggplot(var_imp_df, aes(x=reorder(variable, score), y=score)) + \n  geom_point() +\n  geom_segment(aes(x=variable,xend=variable,y=0,yend=score)) +\n  ylab(\"IncNodePurity\") +\n  xlab(\"Variable Name\") +\n  coord_flip()"},{"path":"feature-selection-in-r.html","id":"univariate-feature-selection","chapter":"77 Feature selection in r","heading":"77.4 Univariate Feature Selection","text":"Univariate tests tests involve one dependent variable, including chi-sqaure test, analysis variance, linear regressions t-tests means.Univariate feature selection utilizes univariate statistical tests feature-outcome pair selects features perform best tests.sbf() used univariate feature selection model fitting function specified function argument sbfControl() function. following example, uses rfSBF, use node purity calculate scores.avoid overfitting feature selection, sbf() function use several iterations resampling feature selection. resampling methods can boot, cv, LOOCV LGOCV can specified method argument sbfControl().take several minutes run.","code":"\nfilterCtrl <- sbfControl(functions = rfSBF, method = \"repeatedcv\", repeats = 3)\nrfWithFilter <- sbf(x= winequality[,1:11],y= winequality[,12], sbfControl = filterCtrl)\nrfWithFilter## \n## Selection By Filter\n## \n## Outer resampling method: Cross-Validated (10 fold, repeated 3 times) \n## \n## Resampling performance:\n## \n##    RMSE Rsquared    MAE  RMSESD RsquaredSD   MAESD\n##  0.5923   0.5625 0.4256 0.02478    0.02653 0.01593\n## \n## Using the training set, 11 variables were selected:\n##    fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides...\n## \n## During resampling, the top 5 selected variables (out of a possible 11):\n##    alcohol (100%), chlorides (100%), citric.acid (100%), density (100%), fixed.acidity (100%)\n## \n## On average, 11 variables were selected (min = 11, max = 11)"},{"path":"feature-selection-in-r.html","id":"recursive-feature-elimination","chapter":"77 Feature selection in r","heading":"77.5 Recursive Feature elimination","text":"Recursive Feature elimination(RFE) popular method choose subset features. starts features removes feature lowest score iteration. trains smaller smaller subset features find best set features.also take several minutes run.","code":"\nfilterCtrl <- rfeControl(functions=rfFuncs, method=\"cv\", number=3)\nresults <- rfe(x= winequality[,1:11],y= winequality[,12], sizes=c(1:11), rfeControl=filterCtrl)\nresults## \n## Recursive feature selection\n## \n## Outer resampling method: Cross-Validated (3 fold) \n## \n## Resampling performance over subset size:\n## \n##  Variables   RMSE Rsquared    MAE   RMSESD RsquaredSD     MAESD Selected\n##          1 0.8740  0.03509 0.6688 0.009920    0.00882 0.0026647         \n##          2 0.7385  0.30791 0.5675 0.009737    0.01618 0.0015214         \n##          3 0.6830  0.40620 0.5172 0.012572    0.02489 0.0058556         \n##          4 0.6592  0.45090 0.4971 0.012260    0.02900 0.0021947         \n##          5 0.6466  0.47656 0.4848 0.011205    0.02869 0.0006118         \n##          6 0.6379  0.48519 0.4703 0.015099    0.03165 0.0004565         \n##          7 0.6304  0.49880 0.4647 0.015114    0.03224 0.0010132         \n##          8 0.6256  0.50895 0.4605 0.014754    0.03159 0.0024944         \n##          9 0.6208  0.51534 0.4552 0.015758    0.03235 0.0030313         \n##         10 0.6185  0.52055 0.4526 0.017347    0.03474 0.0025632         \n##         11 0.6154  0.52655 0.4503 0.015931    0.03320 0.0020967        *\n## \n## The top 5 variables (out of 11):\n##    volatile.acidity, alcohol, free.sulfur.dioxide, pH, residual.sugar\n# plot the results\nplot(results, type=c(\"g\", \"o\"))"},{"path":"feature-selection-in-r.html","id":"section-1","chapter":"77 Feature selection in r","heading":"77.6 ","text":"","code":""},{"path":"feature-selection-in-r.html","id":"resources","chapter":"77 Feature selection in r","heading":"77.7 Resources:","text":"https://topepo.github.io/caret/variable-importance.html#model-independent-metricshttps://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/varImphttps://machinelearningmastery.com/feature-selection---caret-r-package/https://towardsdatascience.com/-art--finding--best-features--machine-learning-a9074e2ca60dhttps://rdrr.io/rforge/caret/man/sbfControl.html","code":""},{"path":"debugging-in-rstudio.html","id":"debugging-in-rstudio","chapter":"78 Debugging in Rstudio","heading":"78 Debugging in Rstudio","text":"Tianxiang Li","code":""},{"path":"debugging-in-rstudio.html","id":"introduction-14","chapter":"78 Debugging in Rstudio","heading":"78.1 Introduction","text":"first assignment asked find hist() baseR determines number bins. Many students including got confused return function described documentation. ’d like share solved problem.tutorial mainly consists three parts: use help(), find source code function using methods(), debug RStudio. Hopefully can help people like encounter similar problems future.","code":"\nlibrary('openintro')\nlibrary('tidyverse')\nlibrary(dplyr)"},{"path":"debugging-in-rstudio.html","id":"first-choice-help","chapter":"78 Debugging in Rstudio","heading":"78.2 First Choice: help()","text":"","code":""},{"path":"debugging-in-rstudio.html","id":"how-to-use-help","chapter":"78 Debugging in Rstudio","heading":"78.2.1 how to use help()","text":"want find new function works, trying help() always first step. returns official documentation function. Usually detailed documentation answer questions. Take hist() example:show documentation hist() function. summarization:Note: input help() name function, use help(hist) instead help(hist()).also function example(hist), return interactive result can run examples function.","code":"\nhelp(hist)"},{"path":"debugging-in-rstudio.html","id":"understand-the-documentation","chapter":"78 Debugging in Rstudio","heading":"78.2.2 understand the documentation","text":"R documentation typically consists several parts: Description, Usage, Arguments, Details, Value, Examples . Usage can find arguments default values. Arguments explanations arguments. Value return value function. example, hist() returns object 6 attributes:","code":"\nx <- hist(mtl$asubic, plot=F)\nstr(x)## List of 6\n##  $ breaks  : num [1:10] 1.7 1.8 1.9 2 2.1 2.2 2.3 2.4 2.5 2.6\n##  $ counts  : int [1:9] 2 4 7 6 6 4 2 2 2\n##  $ density : num [1:9] 0.571 1.143 2 1.714 1.714 ...\n##  $ mids    : num [1:9] 1.75 1.85 1.95 2.05 2.15 2.25 2.35 2.45 2.55\n##  $ xname   : chr \"mtl$asubic\"\n##  $ equidist: logi TRUE\n##  - attr(*, \"class\")= chr \"histogram\""},{"path":"debugging-in-rstudio.html","id":"not-enough","chapter":"78 Debugging in Rstudio","heading":"78.2.3 not enough","text":"However, R documentation answers questions. case, documentation hist() says uses Sturges’ algorithm, uses suggestion pretty().description, still understand works: use result Sturges’ algorithm suggestion pretty(). use pretty() directly, results clearly results hist():totally understand function works, best way read","code":"\npretty(mtl$asubic)## [1] 1.6 1.8 2.0 2.2 2.4 2.6"},{"path":"debugging-in-rstudio.html","id":"source-code-methods","chapter":"78 Debugging in Rstudio","heading":"78.3 Source code: methods()","text":"totally understand function works, best way read source code. Since include documentation, find ?Sometimes can just type name function. return source code:Unfortunately, try hist(), happens:generic function (specifically, ’s S3 generic function). can use methods(), list available methods S3 S4 generic function:hist.default looking . Now successfully get source code hist().However, still doesn’t solve problem: function long! 120 lines, including many -else branches exception handling. painful read contents function just understand basic usage .use debugging tools built RStudio.","code":"\nhist## function (x, ...) \n## UseMethod(\"hist\")\n## <bytecode: 0x561bb6b54b28>\n## <environment: namespace:graphics>\nmethods(hist)## [1] hist.Date*   hist.default hist.POSIXt*\n## see '?methods' for accessing help and source code"},{"path":"debugging-in-rstudio.html","id":"debugging-in-rstudio-1","chapter":"78 Debugging in Rstudio","heading":"78.4 Debugging in RStudio","text":"Debugging useful tool understand code behaves actually running, find code running expected. can set breakpoint line interested , code running, stop breakpoint. can run code step step walk .Now let’s see use debugging find hist() works.","code":""},{"path":"debugging-in-rstudio.html","id":"set-the-breakpoint","chapter":"78 Debugging in Rstudio","heading":"78.4.1 set the breakpoint","text":"Firstly need set breakpoint. , must create R script file (can’t R markdown file). set breakpoint clicking left area line number. looks like :means stop runs line 1 runs line 2.","code":""},{"path":"debugging-in-rstudio.html","id":"run-the-script","chapter":"78 Debugging in Rstudio","heading":"78.4.2 run the script","text":"Save script run Code -> Run Region -> Run :Since set breakpoint step 1, automatically stop point!","code":""},{"path":"debugging-in-rstudio.html","id":"debug","chapter":"78 Debugging in Rstudio","heading":"78.4.3 debug","text":"stops, go debug mode looks like :shown picture, line 2 highlighted, meaning execute line 2 next step (hasn’t executed yet).Note rightside buttons appear Console. Button 1 means Execute next line code, button 2 means Step current function call. click button 1 execute line 2 line 3 highlighted. Since want see details hist(), click button 2.find source code hist()! time, know line executed green arrow highlight, variables values current step shown Environment.Since already step hist() function, now click button 1:can clearly see value variable breaks “Sturges” code goes instead else . can see next two steps happen:Now get :value breaks 7 next step executeFinally get result breaks:","code":"\nbreaks <- nclass.Sturges(x)\nbreaks <- pretty(range(x), n = 7, min.n = 1)"},{"path":"debugging-in-rstudio.html","id":"summary-1","chapter":"78 Debugging in Rstudio","heading":"78.4.4 summary","text":"debug script, can understand hist() determines breaks:Now may want know pretty() works. Using debugging tools introduced , find :Unfortunately, .Internal(pretty(...)) compiled C. beyond class tutorial, want know , can visit : https://stackoverflow.com/questions/14035506/","code":"\nbreaks <- pretty(range(mtl$asubic), n = nclass.Sturges(mtl$asubic), min.n = 1)\nbreaks##  [1] 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6"},{"path":"r-tutorial-cheatsheet.html","id":"r-tutorial-cheatsheet","chapter":"79 R tutorial cheatsheet","heading":"79 R tutorial cheatsheet","text":"Zhining QiuThis tutorial cheatsheet pdf version include basic functions plot give desired graphs.Click following link check cheatsheet:\nhttps://github.com/zhining4/data_work/blob/master/Colourful_Cheatsheet_R_tutorial.pdf","code":""},{"path":"brief-introduction-of-ggpubr.html","id":"brief-introduction-of-ggpubr","chapter":"80 Brief Introduction of ggpubr","heading":"80 Brief Introduction of ggpubr","text":"Hanlin Yan","code":""},{"path":"brief-introduction-of-ggpubr.html","id":"introduction-15","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.1 Introduction","text":"ggplot2, Hadley Wickham, excellent flexible package elegant data visualization R. However default generated plots requires formatting can send publication. Furthermore, customize ggplot, syntax opaque raises level difficulty researchers advanced R programming skills.‘ggpubr’ package provides easy--use functions creating customizing ‘ggplot2’- based publication ready plots. features follows:\nWrapper around ggplot2 package less opaque syntax beginners R programming.\nHelps researchers, non-advanced R programming skills, create easily publication-ready plots.\nMakes possible automatically add p-values significance levels box plots, bar plots, line plots, .\nMakes easy arrange annotate multiple plots page.\nMakes easy change grahical parameters colors labels.","code":""},{"path":"brief-introduction-of-ggpubr.html","id":"installation-4","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.2 Installation","text":"CRAN:Github:Loading environment:","code":"\n#install.packages(\"ggpubr\")\n#if(!require(devtools)) install.packages(\"devtools\")\n#devtools::install_github(\"kassambara/ggpubr\")\nlibrary(ggplot2)\nlibrary(ggpubr)"},{"path":"brief-introduction-of-ggpubr.html","id":"histogram-2","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.3 Histogram","text":"Create dataset attributes sex height 500 people sex values height inches.Now use heightdata plot corresponding histogram. use function gghistogram add mean lines showing mean values sex marginal rug showing one-dimensional density plot axis.","code":"\nset.seed(2000)\nheightdata <- data.frame(\n          sex = factor(rep(c(\"Female\", \"Male\"), each=500)),\n   height = c(rnorm(500, 65), rnorm(500, 70)))\ngghistogram(heightdata, x = \"height\",\n   add = \"mean\", rug = TRUE,\n   color = \"sex\", fill = 'sex', bins= 15)"},{"path":"brief-introduction-of-ggpubr.html","id":"density-plot-2","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.4 Density plot","text":"still use data plot corresponding two-dimensional density plot. add mean lines marginal rug.Now combine density plots histogram.","code":"\nggdensity(heightdata, x = \"height\",\n   add = \"mean\", rug = TRUE,\n   color = \"sex\", fill = \"sex\")\ngghistogram(heightdata, x = \"height\",\n   add = \"mean\", rug = TRUE,\n   color = \"sex\", fill = 'sex', bins= 15,\n   add_density = TRUE)"},{"path":"brief-introduction-of-ggpubr.html","id":"qq-plot","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.5 Qq plot","text":"qqplot example.","code":"\nset.seed(1234)\nwdata = data.frame(\n   sex = factor(rep(c(\"F\", \"M\"), each=200)),\n   weight = c(rnorm(200, 55), rnorm(200, 58)))\n\nggqqplot(wdata, x = \"weight\")"},{"path":"brief-introduction-of-ggpubr.html","id":"line-plot-1","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.6 Line plot","text":"Use ggline create line plot.","code":"\ndf2 <- data.frame(supp=rep(c(\"VC\", \"OJ\"), each=3),\n   dose=rep(c(\"D0.5\", \"D1\", \"D2\"),2),\n   len=c(6.8, 15, 33, 4.2, 10, 29.5))\n\nggline(df2, \"dose\", \"len\",\n  linetype = \"supp\", shape = \"supp\", color =  \"supp\")"},{"path":"brief-introduction-of-ggpubr.html","id":"pie-chart","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.7 Pie chart","text":"ggpie tool make pie plot.","code":"\ndf <- data.frame(\n group = c(\"Male\", \"Female\", \"Child\"),\n  value = c(25, 25, 50))\nlabs <- paste0(df$group, \" (\", df$value, \"%)\")\n\nggpie(df, \"value\", label = labs,\n   fill = \"group\", color = \"white\",\n   palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))"},{"path":"brief-introduction-of-ggpubr.html","id":"box-plot-1","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.8 Box plot","text":"use ToothGrowth dataset show example Box Plot.boxplot different seen geom_boxplot(). boxplot, can see distribution points within different groups Sports. Also, can change color shape easily.\ncan set customized comparison mean groups. result p-value ANOVA can added graph well. p-value Kruskal-Wallis test can also shown graph.","code":"\ndata(\"ToothGrowth\")\ndf <- ToothGrowth\nhead(df, 10)##     len supp dose\n## 1   4.2   VC  0.5\n## 2  11.5   VC  0.5\n## 3   7.3   VC  0.5\n## 4   5.8   VC  0.5\n## 5   6.4   VC  0.5\n## 6  10.0   VC  0.5\n## 7  11.2   VC  0.5\n## 8  11.2   VC  0.5\n## 9   5.2   VC  0.5\n## 10  7.0   VC  0.5\npbox <- ggboxplot(df, x = \"dose\", y = \"len\",\n                color = \"dose\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n                add = \"jitter\", shape = \"dose\")\npbox\nmy_comparisons <- list( c(\"0.5\", \"1\"), c(\"1\", \"2\"), c(\"0.5\", \"2\") )\npbox + stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value\n  stat_compare_means(label.y = 50)                   # Add global p-value"},{"path":"brief-introduction-of-ggpubr.html","id":"violin-plot","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.9 Violin plot","text":"still use first 100 rows fastfood dataset. time, draw violin plot regarding calories variable boxplots inside. can also compare means plot. Just like boxplot , can p-value ANOVA violin plot well.","code":"\nggviolin(df, x = \"dose\", y = \"len\", fill = \"dose\",\n         add = \"boxplot\", add.params = list(fill = \"white\"))+\n   stat_compare_means(comparisons = my_comparisons)+\n   stat_compare_means(label.y = 50)"},{"path":"brief-introduction-of-ggpubr.html","id":"bar-plot","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.10 Bar plot","text":"use mtcars dataset convert gear variable factor. add name columns.Now draw bar plot hp variable change fill color grouping variable gear. Sorting done globally.sort bars inside group.can standarize hp compare model mean.rotate adding just one line code:","code":"\ndata(mtcars)\ncars <- mtcars\ncars$gear <- factor(cars$gear)\ncars$name <- rownames(cars)\nhead(cars[,c(\"name\", \"wt\", \"hp\",\"gear\")])##                                name    wt  hp gear\n## Mazda RX4                 Mazda RX4 2.620 110    4\n## Mazda RX4 Wag         Mazda RX4 Wag 2.875 110    4\n## Datsun 710               Datsun 710 2.320  93    4\n## Hornet 4 Drive       Hornet 4 Drive 3.215 110    3\n## Hornet Sportabout Hornet Sportabout 3.440 175    3\n## Valiant                     Valiant 3.460 105    3\nggbarplot(cars, x = \"name\", y = \"hp\",\n          fill = \"gear\",               \n          color = \"white\",\n          palette = \"jco\",\n          sort.val = \"desc\",  \n          sort.by.groups = FALSE,\n          x.text.angle = 60,\n          ylab = \"Horse Power\",\n          xlab = FALSE,\n          legend.title=\"Gear\"\n          )\nggbarplot(cars, x = \"name\", y = \"hp\",\n          fill = \"gear\",               \n          color = \"white\",  \n          palette = \"jco\",\n          sort.val = \"asc\",  \n          sort.by.groups = TRUE,\n          x.text.angle = 60,\n          ylab = \"Horsepower\",\n          xlab = FALSE,\n          legend.title=\"Gear\"\n          )\ncars$hp_z <- (cars$hp-mean(cars$hp))/sd(cars$hp)\ncars$hp_grp <- factor(ifelse(cars$hp_z<0, \"low\",\"high\"), levels=c(\"low\", \"high\"))\nhead(cars[,c(\"name\", \"wt\", \"hp\", \"hp_grp\", \"gear\")])##                                name    wt  hp hp_grp gear\n## Mazda RX4                 Mazda RX4 2.620 110    low    4\n## Mazda RX4 Wag         Mazda RX4 Wag 2.875 110    low    4\n## Datsun 710               Datsun 710 2.320  93    low    4\n## Hornet 4 Drive       Hornet 4 Drive 3.215 110    low    3\n## Hornet Sportabout Hornet Sportabout 3.440 175   high    3\n## Valiant                     Valiant 3.460 105    low    3\nggbarplot(cars, x=\"name\", y=\"hp_z\",\n          fill = \"hp_grp\",\n          color = \"white\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle=60,\n          ylab = \"Horsepower z-scores\",\n          xlab = FALSE, \n          legend.title=\"Horsepower Group\")\nrotate=TRUE\nggbarplot(cars, x=\"name\", y=\"hp_z\",\n          fill = \"hp_grp\",\n          color = \"white\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle=90,\n          ylab = \"Horsepower z-scores\",\n          xlab = FALSE, \n          legend.title=\"Horsepower Group\",\n          rotate=TRUE,\n          ggtheme = theme_minimal())"},{"path":"brief-introduction-of-ggpubr.html","id":"cleveland-dot-plot-1","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.11 Cleveland dot plot","text":"still use mtcars dataset draw Cleveland dot plot.interesting color y text groups. need add :","code":"\nggdotchart(cars, x = \"name\", y = \"hp\",\n           color = \"gear\", \n           palette = \"jco\",\n           sorting = \"descending\",                 \n           rotate = TRUE,                          \n           dot.size = 2,\n           ggtheme = theme_pubr(),\n           legend.title = \"Gear\")+\n  theme_cleveland()                                \ny.text.col=TRUE\nggdotchart(cars, x = \"name\", y = \"hp\",\n           color = \"gear\", \n           palette = \"jco\",\n           sorting = \"descending\",                 \n           rotate = TRUE,                          \n           dot.size = 2,                           \n           ggtheme = theme_pubr(),\n           y.text.col=TRUE,\n           legend.title = \"Gear\")+\n  theme_cleveland()      "},{"path":"brief-introduction-of-ggpubr.html","id":"mix-multiple-graphs","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.12 Mix Multiple Graphs","text":"arrange multiple graphs one single page, ’ll use function ggarrange()[ggpubr]. Compared standard function plot_grid(), ggarange() can arrange multiple ggplots multiple pages.\nCreate box plot dot plot:","code":"\n# Box plot (bp)\nbxp <- ggboxplot(ToothGrowth, x = \"dose\", y = \"len\",\n                 color = \"dose\", palette = \"jco\")\nbxp\n# Dot plot (dp)\ndp <- ggdotplot(ToothGrowth, x = \"dose\", y = \"len\",\n                 color = \"dose\", palette = \"jco\", binwidth = 1)\ndp\n# scatter plot (sp)\nx <- c(841.564927857936, 841.564927857936, 841.424690409534, 841.284499691372, \n420.782463928968, 420.88768585043, 420.432103842432, 420.572177840048, \n420.712345204767, 420.782463928968, 420.747401645497)\ny <- c(43692.05, 51561, 34637.8270288285, 36198.5838053982, 36909.925, \n30733.6584146036, 32350.3164029975, 31906.371814093, 30367.0638226962, \n32410.975, 31970.2108157654)\ndf <- data.frame(x,y)\nsp <- ggscatter(df, x = 'x', y = 'y', add = \"reg.line\") +\n  stat_cor(label.x = 3, label.y = 3000) +\n  stat_regline_equation(label.x = 3, label.y = 5000)\nsp\nggarrange(bxp, dp, sp,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"},{"path":"brief-introduction-of-ggpubr.html","id":"conclusion-4","chapter":"80 Brief Introduction of ggpubr","heading":"80.0.13 Conclusion","text":"can see, ’s much easier plot required graphs using ggpubr without learning much layers ggplot2, makes graphing less difficult people familiar R programming.source:\nhttps://cran.r-project.org/web/packages/ggpubr/index.html\nhttps://github.com/kassambara/ggpubr","code":""},{"path":"r-based-data-organization-and-visualization.html","id":"r-based-data-organization-and-visualization","chapter":"81 R based data organization and visualization","heading":"81 R based data organization and visualization","text":"Tengteng Tao","code":"\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(ggplot2)\nlibrary(scales)"},{"path":"r-based-data-organization-and-visualization.html","id":"introduction-16","chapter":"81 R based data organization and visualization","heading":"81.1 Introduction","text":"cheat sheet, conclude important functions used past two graded homeworks. whole cheat sheet two sections. first section, functions related plotting listed. second section, functions helpful organizing data mentioned.","code":""},{"path":"r-based-data-organization-and-visualization.html","id":"plots-and-their-corresponding-codes","chapter":"81 R based data organization and visualization","heading":"81.2 Plots and their corresponding codes","text":"","code":""},{"path":"r-based-data-organization-and-visualization.html","id":"box-plots","chapter":"81 R based data organization and visualization","heading":"81.2.1 Box plots","text":"create box plots, use function geom_boxplot() ggplot2. basic function isggplot(dataframe, aes(y = TheColumnForYaxis , x = TheColumnForXaxis ))+\ngeom_boxplot()example used data fastfood openintro package. used data calories restaurant make box plot. addition,make sure plot intuitive enough, plot ordered mean value high low.","code":"\ndf <- openintro::fastfood\nggplot(df, aes(x = reorder(restaurant, calories, median), y = calories))+\ngeom_boxplot()+\ncoord_flip()+\nxlab(\"\")+\ntheme_grey(14)"},{"path":"r-based-data-organization-and-visualization.html","id":"histograms","chapter":"81 R based data organization and visualization","heading":"81.2.2 Histograms","text":"section, introduce two ways make histograms. Examples used data mtl package openintro\nfirst way plotting base R. can simply use function hist() likehist(datafram$TheColumnYouWantToPlot)ggplot(datafram, aes(TheColumnYouWantToPlot))+\ngeom_hist()","code":"\ndf <- openintro::mtl\nhist(df$asubic, col = \"lightblue\",  xlab = \"asubic\", main = \"Histogram of asubic based on R\")\nggplot(df, aes(asubic))+\n  geom_histogram()+stat_bin(bins = 9)+\n  ggtitle(\"Histogram of asubic based on ggplot\")"},{"path":"r-based-data-organization-and-visualization.html","id":"density-curves","chapter":"81 R based data organization and visualization","heading":"81.2.3 Density curves","text":"create density curves, use geom_density ggplot2. Basic function :ggplot(dataframe, aes(x=TheColumnYouWantToPlot))+\ngeom_density()data used example astralia soybean agridat package.","code":"\ndf <- agridat::australia.soybean\nggplot(df, aes(x = yield))+\n  geom_density(alpha = .2, color = \"blue\")+  \n  theme_grey(14)"},{"path":"r-based-data-organization-and-visualization.html","id":"normal-curves","chapter":"81 R based data organization and visualization","heading":"81.2.4 Normal curves","text":"create normal curves, need use stat_function(fun = dnorm, args = list(mean, sd)). basic function :ggplot(dataframe, aes(x=TheColumnYouWantToPlot))+\nstat_function(fun = dnorm, args = list(mean = mean(TheColumnYouWantToPlot), sd = sd(TheColumnYouWantToPlot)))data used example astralia soybean agridat package.","code":"\ndf <- agridat::australia.soybean\nggplot(df, aes(x = yield))+\n  stat_function(fun = dnorm, args = list(mean = mean(df$yield), sd = sd(df$yield)), color = \"red\")"},{"path":"r-based-data-organization-and-visualization.html","id":"ridgeline-plot","chapter":"81 R based data organization and visualization","heading":"81.2.5 Ridgeline plot","text":"create ridgeline plot, need use geom_density_ridges() package ggridges. basic function :ggplot(df, aes(y = TheColumnForYaxis , x = TheColumnForXaxis ))+\ngeom_density_ridges()+example used data loans_full_schema package openintro. addition,make sure plot intuitive enough, plot ordered mean value high low.","code":"\ndf <- openintro::loans_full_schema\nggplot(df, aes(y = reorder(loan_purpose, loan_amount, median), \n               x = loan_amount))+\n  geom_density_ridges(fill = \"blue\", alpha = .5, scale = 1)+\n  theme_ridges()+\n  theme(legend.position = \"none\")"},{"path":"r-based-data-organization-and-visualization.html","id":"frequency-bar-chart","chapter":"81 R based data organization and visualization","heading":"81.2.6 Frequency Bar Chart","text":"create frequency bar chart, need use geom_bar() ggplot2. basic function:ggplot(dataframe, aes(x = TheColumnYouWantToPlot))+\ngeom_bar(aes(y = ..count..))Data used Roof.Style ames package openintro. addition,make sure plot intuitive enough, plot ordered ascending order.","code":"\ndf <- openintro::ames\n\nggplot(df,  aes(x = fct_rev(fct_infreq(Roof.Style)))) + \n  geom_bar( aes(y = ..count..), color = \"blue\", fill = \"lightblue\") +\n  xlab(\"\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Frequency bar chart for the roof styles of the properties\")"},{"path":"r-based-data-organization-and-visualization.html","id":"cleveland-dot-plots","chapter":"81 R based data organization and visualization","heading":"81.2.7 Cleveland dot plots","text":"create cleveland dot plots, need geom_point ggplot2. basic function isggplot(dataframe, aes(y =reorder(TheColumnForYaxis, TheColumnForXaxis) , x = TheColumnForXaxis))+\ngeom_point()Data used seattlepets package openintro. plotted popular 30 names.","code":"\ndf <- openintro::seattlepets %>% dplyr::count(animal_name, sort = TRUE) %>% drop_na()\n\nggplot(df[1:30,], aes(x = n, y = reorder(animal_name, n)))+\n  geom_point()"},{"path":"r-based-data-organization-and-visualization.html","id":"scatter-plot-1","chapter":"81 R based data organization and visualization","heading":"81.2.8 Scatter plot","text":"create scatter plot, also use geom_point(). basic function isggplot(dataframe, aes(y = TheColumnForYaxis , x = TheColumnForXaxis))+\ngeom_point()Data used ames package openintro","code":"\ndf <- openintro::ames\nggplot(df, aes(area, price))+\n  geom_point( alpha = .15, stroke = 0,size = 1.5)+\n  ggtitle(\"Scatter plot of price vs. area\")"},{"path":"r-based-data-organization-and-visualization.html","id":"density-contour-lines","chapter":"81 R based data organization and visualization","heading":"81.2.9 Density contour lines","text":"create density contour lines, need use geom_density_2d(). basic function:ggplot(dataframe, aes(y = TheColumnForYaxis , x = TheColumnForXaxis))+\ngeom_density_2d()Data used ames package openintro.","code":"\nggplot(df,aes(area, price)) + \n  geom_density_2d(aes(colour=..level..)) + \n  scale_colour_gradient(low=\"green\",high=\"red\") +\n  ggtitle(\"Density contour lines of price vs. area\")"},{"path":"r-based-data-organization-and-visualization.html","id":"hexagonal-heatmap","chapter":"81 R based data organization and visualization","heading":"81.2.10 Hexagonal heatmap","text":"create hexagonal heatmap, need use geom_hex(). basic function:ggplot(dataframe, aes(y = TheColumnForYaxis , x = TheColumnForXaxis))+\ngeom_hex()Data used ames package openintro.","code":"\nggplot(df,aes(area, price)) + \n  geom_hex(bins = 30) +\n  scale_fill_gradient(low = \"#F2F0F7\", high = \"#08519C\" ) +\n  theme_bw() +\n  ggtitle(\"Hexagonal heatmap of price vs. area\")"},{"path":"r-based-data-organization-and-visualization.html","id":"square-heatmap","chapter":"81 R based data organization and visualization","heading":"81.2.11 Square heatmap","text":"create hexagonal heatmap, need use geom_bin_2d(). basic function:ggplot(dataframe, aes(y = TheColumnForYaxis , x = TheColumnForXaxis))+\ngeom_bin_2d()Data used ames package openintro.","code":"\nggplot(df, aes(area, price)) +\n geom_bin_2d(bins = 20) +\n  scale_fill_gradient(low = \"#F2F0F7\", high = \"#08519C\" ) +\n  theme_bw() +\n  ggtitle(\"Square heatmap of price vs. area\") "},{"path":"r-based-data-organization-and-visualization.html","id":"data-organization-functions","chapter":"81 R based data organization and visualization","heading":"81.3 Data organization functions","text":"","code":""},{"path":"r-based-data-organization-and-visualization.html","id":"pipe-operator","chapter":"81 R based data organization and visualization","heading":"81.3.1 Pipe(%>%) Operator","text":"Pipe operator can used simplified code. can simple interpreted “” example:\nfilter(data, variable == numeric_value) \ndata %>% filter(variable == numeric_value) yield result.using operator properly, can make code clean brief.\n### facet_warp()\nfacet_warp() allow us combine multiple plots, can give us directly view comparison type plots.data used example astralia soybean agridat package.","code":"\ndf <- agridat::australia.soybean\nggplot(df, aes(x = yield, color = loc, fill = loc))+\n  geom_histogram(aes(y = ..density..),  fill = NA) + \n  facet_wrap(~loc, nrow = 2, strip.position = \"right\")+\n  theme_grey(14)"},{"path":"r-based-data-organization-and-visualization.html","id":"filter-4","chapter":"81 R based data organization and visualization","heading":"81.3.2 filter()","text":"filter() function allows us select variable specific values. example, data seattlepets package openintro, can use filter find dogs’ name:","code":"\ndf <- openintro::seattlepets %>% filter(species == \"Dog\")\ndf## # A tibble: 35,181 × 7\n##    license_issue_date license_number animal_name species primary_breed          \n##    <date>             <chr>          <chr>       <chr>   <chr>                  \n##  1 2018-11-16         8002756        Wall-E      Dog     Mixed Breed, Medium (u…\n##  2 2018-11-11         S124529        Andre       Dog     Terrier, Jack Russell  \n##  3 2018-11-21         903793         Mac         Dog     Retriever, Labrador    \n##  4 2018-12-16         S138529        Cody        Dog     Retriever, Labrador    \n##  5 2017-10-04         580652         Millie      Dog     Terrier, Boston        \n##  6 2018-12-23         961052         Sabre       Dog     Terrier                \n##  7 2018-12-07         S125461        Thomas      Dog     Chihuahua, Short Coat  \n##  8 2018-11-07         8002543        Lulu        Dog     Vizsla, Smooth Haired  \n##  9 2018-12-15         S138838        Milo        Dog     Boxer                  \n## 10 2018-11-27         S123980        Anubis      Dog     Poodle, Standard       \n## # … with 35,171 more rows, and 2 more variables: secondary_breed <chr>,\n## #   zip_code <chr>"},{"path":"r-based-data-organization-and-visualization.html","id":"group_by-2","chapter":"81 R based data organization and visualization","heading":"81.3.3 group_by()","text":"group_by()function allow us group data variables.example, want group data crimes 2020 county region, can ","code":"\ndf <- read.csv(\"https://data.ny.gov/api/views/ca8h-8gjq/rows.csv\")\ndf %>% filter(Year == 2020) %>% group_by(County, Region)## # A tibble: 624 × 15\n## # Groups:   County, Region [62]\n##    County Agency     Year Months.Reported Index.Total Violent.Total Murder  Rape\n##    <chr>  <chr>     <int>           <int>       <int>         <int>  <int> <int>\n##  1 Albany Albany C…  2020              12        3547           875     18    61\n##  2 Albany Albany C…  2020              12           2             0      0     0\n##  3 Albany Albany C…  2020              12         127            12      0     4\n##  4 Albany Albany C…  2020              12         103            26      0    18\n##  5 Albany Altamont…  2020              12           6             2      0     0\n##  6 Albany Bethlehe…  2020              12         407            25      1     7\n##  7 Albany Coeymans…  2020              12          49             8      0     0\n##  8 Albany Cohoes C…  2020              12         134            23      0     3\n##  9 Albany Colonie …  2020              12        1936            83      0     4\n## 10 Albany County T…  2020              NA        7412          1115     19   109\n## # … with 614 more rows, and 7 more variables: Robbery <int>,\n## #   Aggravated.Assault <int>, Property.Total <int>, Burglary <int>,\n## #   Larceny <int>, Motor.Vehicle.Theft <int>, Region <chr>"},{"path":"r-based-data-organization-and-visualization.html","id":"summarise","chapter":"81 R based data organization and visualization","heading":"81.3.4 summarise()","text":"can use summarise() measure value certain group. example, data set ames openintro, want know average price area Neighborhood, can :","code":"\ndf <-openintro::ames %>%\n  group_by(Neighborhood) %>%\n  summarise(price = mean(price), area = mean(area))\n\ndf## # A tibble: 28 × 3\n##    Neighborhood   price  area\n##    <fct>          <dbl> <dbl>\n##  1 Blmngtn      196662. 1405.\n##  2 Blueste      143590  1160.\n##  3 BrDale       105608. 1115.\n##  4 BrkSide      124756. 1235.\n##  5 ClearCr      208662. 1744.\n##  6 CollgCr      201803. 1496.\n##  7 Crawfor      207551. 1723.\n##  8 Edwards      130843. 1338.\n##  9 Gilbert      190647. 1621.\n## 10 Greens       193531. 1157.\n## # … with 18 more rows"},{"path":"r-based-data-organization-and-visualization.html","id":"summarise_at","chapter":"81 R based data organization and visualization","heading":"81.3.5 summarise_at()","text":"specific situation, need summaries many variables. Writing one one time comsuming can using summarise_at().Example, want know year 2020, total number type crime happened every county. can write something like :","code":"\ndf2020 <- read.csv(\"https://data.ny.gov/api/views/ca8h-8gjq/rows.csv\") %>% \n  filter(Year == 2020)\ndf2020$Property.Total = NULL\ndf2020 %>%\n  group_by(County) %>%\n  summarise_at(.vars = names(.)[7:13], .funs = c(sum = \"sum\"))## # A tibble: 62 × 8\n##    County      Murder_sum Rape_sum Robbery_sum Aggravated.Assault_… Burglary_sum\n##    <chr>            <int>    <int>       <int>                <int>        <int>\n##  1 Albany              38      218         426                 1548         1416\n##  2 Allegany             6       76           6                   58          172\n##  3 Bronx              111      523        3519                 8976         2230\n##  4 Broome              10      252         156                  902         1336\n##  5 Cattaraugus          2       74          12                  162          274\n##  6 Cayuga               4      126          32                  236          290\n##  7 Chautauqua           2      164          66                  552         1080\n##  8 Chemung              4       64          56                  220          262\n##  9 Chenango             2      104          14                  106          292\n## 10 Clinton              2      118           6                  130          246\n## # … with 52 more rows, and 2 more variables: Larceny_sum <int>,\n## #   Motor.Vehicle.Theft_sum <int>"},{"path":"r-packages-for-map-visualization.html","id":"r-packages-for-map-visualization","chapter":"82 R packages for map Visualization","heading":"82 R packages for map Visualization","text":"Pablo Ulises Hernandez Garces","code":"\n# Required libraries\nlibrary(forcats) #Tools for Working with Categorical\nlibrary(ggmap) #Spatial Visualization with ggplot2\n\nlibrary(plotly) #Create Interactive Web Graphics via 'plotly.js'\n\n# Packages for tmap section\nlibrary(sf) #Support for simple features, a standardized way to encode spatial vector data\nlibrary(raster) #Reading, writing, manipulating, analyzing and modeling of spatial data.\nlibrary(tidyverse)\nlibrary(spData) # contains spatial datasets \nlibrary(USAboundaries) # US spacial datasets\nlibrary(maps) # Display of maps\n\nlibrary(tmap)    # for static and interactive maps\nlibrary(leaflet) # for interactive maps"},{"path":"r-packages-for-map-visualization.html","id":"introduction-17","chapter":"82 R packages for map Visualization","heading":"82.1 Introduction","text":"many different R packages dealing spatial data, popular tmap, ggmap Plotly.","code":""},{"path":"r-packages-for-map-visualization.html","id":"tmap","chapter":"82 R packages for map Visualization","heading":"82.2 tmap","text":"tmap package, thematic maps can generated great flexibility. syntax creating plots similar ggplot2, tailored maps.1The basic building block tm_shape(); defines input data, raster vector objects, followed one layer elements tm_fill() tm_dots(). layering demonstrated chunk belowThe instruction tm_fill() + tm_borders() equivalent tm_polygons().useful feature tmap ability store objects\nrepresenting maps, can plotted later, example adding additional layers.can add new shapes tm_shape(new_obj). case new_obj represents new spatial object plotted top preceding layers. new shape added way, subsequent aesthetic functions refer , another new shape added.useful feature tmap multiple map objects can arranged single ‘metaplot’ tmap_arrange()first plot previous figure demonstrates tmap’s default aesthetic settings. Gray shades used tm_fill() layers continuous black line used represent lines created tm_lines(). functions help change default aesthetic values tm_fill tm_borders, area lines respectively.tmap two main types map aesthetics: change data constant. Unlike ggplot2, uses helper function aes() represent variable aesthetics, tmap accepts aesthetic arguments either variable fields (based column names) constant values.useful tool visualization facet graphs. case maps tmap, done function tm_facets follows.function tm_facetsis also useful making animations based maps, combination tmap_animation. make animations change parameter = \"year\" along = \"year\". Additionally, parameter free.coords = FALSEmaintains map extent map iteration.\ndetail tmapfunctions, including interacting mapping using tmap, visit: Making maps R.","code":"\ndata(world)\n\n# Add fill layer to us_states shape\ntm_shape(world) + tm_fill(\"darkolivegreen3\") +\n  tm_borders() +\n  tm_format(\"World\", title=\"A green World\")\nmap_world = tm_shape(world) + tm_polygons()\nclass(map_world)## [1] \"tmap\"\ndata(land, metro)\n\npal8 <- c(\"#33A02C\", \"#B2DF8A\", \"#FDBF6F\", \"#1F78B4\", \"#999999\", \"#E31A1C\", \"#E6E6E6\", \"#A6CEE3\")\n\nmap_world2=map_world+\n  tm_shape(land, ylim = c(-88,88)) +\n    tm_raster(\"cover_cls\", palette = pal8, title = \"Global Land Cover\") +\n  tm_layout(scale = .8, \n    legend.position = c(\"left\",\"bottom\"),\n    legend.bg.color = \"white\", legend.bg.alpha = .2, \n    legend.frame = \"gray50\")\n\n\nmap_world3= map_world2+\n    tm_shape(metro) + \n    tm_dots(col = \"#E31A1C\") \ntmap_arrange(map_world, map_world2, map_world3)\nma1 = tm_shape(world) + tm_fill(col = \"red\")\nma2 = tm_shape(world) + tm_fill(col = \"red\", alpha = 0.3)\nma3 = tm_shape(world) + tm_borders(col = \"blue\")\nma4 = tm_shape(world) + tm_borders(lwd = 3)\nma5 = tm_shape(world) + tm_borders(lty = 2)\nma6 = tm_shape(world) + tm_fill(col = \"red\", alpha = 0.3) +\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\nurb_1970_2030 = spData::urban_agglomerations %>% \n  filter(year %in% c(1970, 1990, 2010, 2030))\n\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = FALSE)\nmap_anim = tm_shape(world) + tm_polygons() + \n  tm_shape(urban_agglomerations) + tm_dots(size = \"population_millions\") +\n  tm_facets(along = \"year\", free.coords = FALSE)\ntmap_animation(map_anim,filename = \"map_anim.gif\", delay = 1)"},{"path":"r-packages-for-map-visualization.html","id":"ggmap","chapter":"82 R packages for map Visualization","heading":"82.3 ggmap","text":"ggmap R package makes easy retrieve raster map tiles popular online mapping services like Google Maps Stamen Maps plot using ggplot2 framework. use library need online.Note: Google recently changed API requirements, ggmap users now required register Google. , part tutorial used get_googlemap() function. Instead, functions get_stamenmap()get_openstreetmap can used visualization purposes. two functions use map images.Example scatter plot map:ggplot2 geom’s available. example, possible make contour plot geom = \"density2d\":fact, since ggmap’s built top ggplot2, usual ggplot2 stuff (geoms, polishing, etc.) work, unique graphing perks ggmap brings table, .Faceting works, :examples make maps ggmap can found hereA useful tutorial includes maps Google using ggmap can found hereHere examples use package.","code":"\n# define helper\n`%notin%` <- function(lhs, rhs) !(lhs %in% rhs)\n\n# reduce crime to violent crimes in downtown houston\nviolent_crimes <- crime %>% \n  filter(\n    offense %notin% c(\"auto theft\", \"theft\", \"burglary\"),\n    -95.39681 <= lon & lon <= -95.34188,\n     29.73631 <= lat & lat <=  29.78400\n  ) %>% \n  mutate(\n    offense = fct_drop(offense),\n    offense = fct_relevel(offense, c(\"robbery\", \"aggravated assault\", \"rape\", \"murder\"))\n  )\n\nqmplot(lon, lat, data = violent_crimes, maptype = \"toner-lite\")\nqmplot(lon, lat, data = violent_crimes, maptype = \"toner-lite\", geom = \"density2d\", color = I(\"red\"))\nrobberies <- violent_crimes %>% filter(offense == \"robbery\")\n\nqmplot(lon, lat, data = violent_crimes, geom = \"blank\", \n  zoom = 14, maptype = \"toner-background\", darken = .7, legend = \"topleft\"\n) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\", alpha = .3, color = NA) +\n  scale_fill_gradient2(\"Robbery\\nPropensity\", low = \"white\", mid = \"yellow\", high = \"red\", midpoint = 650)\nqmplot(lon, lat, data = violent_crimes, maptype = \"toner-background\", color = offense) + \n  facet_wrap(~ offense)"},{"path":"r-packages-for-map-visualization.html","id":"plotly-1","chapter":"82 R packages for map Visualization","heading":"82.4 Plotly","text":"two main ways creating plotly object: either transforming ggplot2 object (via ggplotly()) plotly object directly initializing plotly object plot_ly()/plot_geo()/plot_mapbox().Plotly supports two different kinds maps:Mapbox maps tile-based maps. plot Mapbox maps Plotly may need Mapbox account public Mapbox Access Token. See Mapbox Map Layers documentation information.Mapbox maps tile-based maps. plot Mapbox maps Plotly may need Mapbox account public Mapbox Access Token. See Mapbox Map Layers documentation information.Geo maps outline-based maps. tutorial Geo outline-based maps explored.Geo maps outline-based maps. tutorial Geo outline-based maps explored.Plotly Geo maps built-base map layer composed “physical” “cultural” (.e. administrative border) data Natural Earth Dataset. Various lines area fills can shown hidden, color line-widths specified.map physical features enabled styled, \nlarger-scale 1:50m resolution:addition physical base map features, “cultural” base map included composed country borders selected sub-country borders states. map cultural features enabled styled, 1:50m resolution, includes country boundaries. See country sub-unit cultural base map features:Geo maps drawn according given map projection flattens Earth’s roughly-spherical surface 2-dimensional space.available projections ‘equirectangular’, ‘mercator’, ‘orthographic’, ‘natural earth’, ‘kavrayskiy7’, ‘miller’, ‘robinson’, ‘eckert4’, ‘azimuthal equal area’, ‘azimuthal equidistant’, ‘conic equal area’, ‘conic conformal’, ‘conic equidistant’, ‘gnomonic’, ‘stereographic’, ‘mollweide’, ‘hammer’, ‘transverse mercator’, ‘albers usa’, ‘winkel tripel’, ‘aitoff’ ‘sinusoidal’.Latitude Longitude Grid Lines can drawn using layout$geo$lataxis$showgrid layout$geo$lonaxis$showgrid options similar 2d cartesian ticks.Scatter plots maps highlight geographic areas can colored value.draw lines, great circles, contours maps. Lines maps can show distance geographic points contour lines (isolines, isopleths, isarithms).example use library simple features (sf) read shape files plotting features Plotly.detailed examples explanation plotting maps Mapbox Plotly can found chapter 4 - (2019), Sievert, Carson. Interactive web-based data visualization R, plotly, shiny.see general examples maps using plotly visit Plotly R Library Maps. , examples Choropleth maps, Scatter plots maps, Mapbox maps: density, layers, lines, areas, scatter.","code":"\ng <- list(\n  scope = 'world',\n  showland = TRUE,\n  landcolor = toRGB(\"LightGreen\"),\n  showocean = TRUE,\n  oceancolor = toRGB(\"LightBlue\"),\n  showlakes = TRUE,\n  lakecolor = toRGB(\"Blue\"),\n  showrivers = TRUE,\n  rivercolor = toRGB(\"Blue\"),\n  resolution = 50,\n  showland = TRUE,\n  landcolor = toRGB(\"#e5ecf6\")\n)\n\nfig <- plot_ly(type = 'scattergeo', mode = 'markers')\nfig <- fig %>% layout(geo = g)\nfig\ng <- list(\n  scope = 'world',\n  visible = F,\n  showcountries = T,\n  countrycolor = toRGB(\"Purple\"),\n  resolution = 50,\n  showland = TRUE,\n  landcolor = toRGB(\"#e5ecf6\")\n)\n\nfig <- plot_ly(type = 'scattergeo', mode = 'markers')\nfig <- fig %>% layout(geo = g)\nfig\ng <- list(\n  projection = list(\n    type = 'orthographic'\n  ),\n  showland = TRUE,\n  landcolor = toRGB(\"#e5ecf6\")\n)\n\nfig <- plot_ly(type = 'scattergeo', mode = 'markers')\nfig <- fig %>% layout(geo = g)\nfig\ng <- list(\n  lonaxis = list(showgrid = T),\n  lataxis = list(showgrid = T),\n  showland = TRUE,\n  landcolor = toRGB(\"#e5ecf6\")\n)\n\nfig <- plot_ly(type = 'scattergeo', mode = 'markers')\nfig <- fig %>% layout(geo = g)\nfig\ndf <- read.csv('https://raw.githubusercontent.com/plotly/datasets/master/2015_06_30_precipitation.csv')\n\n# change default color scale title\nm <- list(colorbar = list(title = \"Total Inches\"))\n\n# geo styling\ng <- list(\n  scope = 'north america',\n  showland = TRUE,\n  landcolor = toRGB(\"grey83\"),\n  subunitcolor = toRGB(\"white\"),\n  countrycolor = toRGB(\"white\"),\n  showlakes = TRUE,\n  lakecolor = toRGB(\"white\"),\n  showsubunits = TRUE,\n  showcountries = TRUE,\n  resolution = 50,\n  projection = list(\n    type = 'conic conformal',\n    rotation = list(lon = -100)\n  ),\n  lonaxis = list(\n    showgrid = TRUE,\n    gridwidth = 0.5,\n    range = c(-140, -55),\n    dtick = 5\n  ),\n  lataxis = list(\n    showgrid = TRUE,\n    gridwidth = 0.5,\n    range = c(20, 60),\n    dtick = 5\n  )\n)\n\nfig <- plot_geo(df, lat = ~Lat, lon = ~Lon, color = ~Globvalue)\nfig <- fig %>% add_markers(\n    text = ~paste(df$Globvalue, \"inches\"), hoverinfo = \"text\"\n  )\nfig <- fig %>% layout(title = 'US Precipitation 06-30-2015<br>Source: NOAA', geo = g)\n\nfig\nfig <- plot_geo(lat = c(40.7127, 51.5072), lon = c(-74.0059, 0.1275))\nfig <- fig %>% add_lines(color = I(\"blue\"), size = I(2))\nfig <- fig %>% layout(\n    title = 'London to NYC Great Circle',\n    showlegend = FALSE,\n    geo = list(\n      resolution = 50,\n      showland = TRUE,\n      showlakes = TRUE,\n      landcolor = toRGB(\"grey80\"),\n      countrycolor = toRGB(\"grey80\"),\n      lakecolor = toRGB(\"white\"),\n      projection = list(type = \"equirectangular\"),\n      coastlinewidth = 2,\n      lataxis = list(\n        range = c(20, 60),\n        showgrid = TRUE,\n        tickmode = \"linear\",\n        dtick = 10\n      ),\n      lonaxis = list(\n        range = c(-100, 20),\n        showgrid = TRUE,\n        tickmode = \"linear\",\n        dtick = 20\n      )\n    )\n  )\n\nfig\nnc <- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\n\nfig <- plot_ly(nc)\n\nfig"},{"path":"common-git-command-lines-tutorial-when-working-on-studio.html","id":"common-git-command-lines-tutorial-when-working-on-studio","chapter":"83 Common git command lines tutorial when working on studio","heading":"83 Common git command lines tutorial when working on studio","text":"Boquan SunWhen many people work project, efficient use github synchronize everyone’s process. git command lines can make us thing github conveniently. RStudio, also VisualStudio development tools can use command lines want. Therefore, significant familiar git common lines.Firstly set user’s informationThen clone github repositoryUpdate retrieve remote repositoryGit checkout get branch different work.writing new codes, commit remote repository.Sometimes want record current state working directory index, want go back clean working directoryWhen enter old branch, may forget done. commands help recall modified.Sometimes may see commit history.last, usually find edition bug, need revert .advantage using git revert doesn’t touch commit history. means can still see commits history, even reverted ones.Another safety measure everything happens local system unless push remote repo","code":"git config --global user.name \"[first_name last_name]\"#initialize a directory as git directory\ngit init \n\n#retrieve a git repository from github.com\ngit clone [url]# synchronize your local repository with remote\ngit pull \n\n#Download objects and refs from another repositorym\ngit fetch [alias or url] # start a new branch and then checked out.\ngit checkout -b [newbranch] \n\n# start a new branch and then checked out. If it already exists, initialize it as a new branch.\ngit checkout -B [newbranch] \n\n # switch to a existed branch\ngit checkout [branch]    \n\n# list all existed branches\ngit branch \n\n# delete a branch\ngit branch -d [branch]\n\n# create a new branch\ngit branch [branch] # add modified files to stage prepared to be commited\ngit add [file list] \n\n# add everying\ngit add -A \n\n#merges the specified branch’s history into the current branch\ngit merge [branch name] \n\n#commit your staged files\ngit commit -m '[describe your commit content]'\n\n# update remote repository \ngit push \n\n#set a new remote repo and update these commit to it\ngit push --set-upstream origin [repo name] # saves local modifications away and reverts the working directory to match the HEAD commit.\ngit stash \n\n#same as git stash\ngit stash push \n\n# list stash entries we have\ngit stash list \n\n# restore a recent stash modification\ngit stash apply # show modified files in working directory, staged for  next commit\ngit status \n\n# get difference of modified parts but not staged yet\ngit diff \n\n# get diiference of staged part but not committed yet\ngit diff --staged \n\n#unstage files to retain changes in working directory\ngit reset [file_list] # for current branch to see the commit history\ngit log \n\n# show commit in branchA but not in branchB\ngit log branchB..branchA\n\n# see the commits that changed file\ngit log --follow [file]\n\n#show all commit logs with indication of paths that moved\ngit log --stat -M#check commit history\ngit log -- oneline\n\n# specify the hash code next to our commit \ngit revert [hash code]"},{"path":"tutorial-of-radar-chart.html","id":"tutorial-of-radar-chart","chapter":"84 Tutorial of radar chart","heading":"84 Tutorial of radar chart","text":"Pengyu Zou Nuanyu Shou","code":""},{"path":"tutorial-of-radar-chart.html","id":"overview-7","chapter":"84 Tutorial of radar chart","heading":"84.1 Overview","text":"Radar chart useful chart display multivariate observations arbitrary number variables. consists sequence equi-angular spokes, called radii, spoke representing one variables. data length spoke proportional magnitude variable data point relative maximum magnitude variable across data points. Radar charts widely used sports chart players’ strengths weaknesses.document, introduce draw radar charts using package ggradar fmsb. method, give instructions installation, simple example parameter usage tutorial. Besides, explore use plotly make radar chart readable interactive.","code":""},{"path":"tutorial-of-radar-chart.html","id":"radar-chart-using-ggradar","chapter":"84 Tutorial of radar chart","heading":"84.2 Radar chart using ggradar","text":"","code":""},{"path":"tutorial-of-radar-chart.html","id":"installation-5","chapter":"84 Tutorial of radar chart","heading":"84.2.1 Installation","text":"","code":""},{"path":"tutorial-of-radar-chart.html","id":"install-by-devtools","chapter":"84 Tutorial of radar chart","heading":"84.2.1.1 Install by devtools","text":" ","code":""},{"path":"tutorial-of-radar-chart.html","id":"ggradar-is-a-github-package-and-requires-devtools-to-install-it.-run-following-script-to-install-ggradar.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.1.1 ggradar is a github package and requires devtools to install it. Run following script to install ggradar.","text":" ","code":"devtools::install_github(\"ricardo-bion/ggradar\",dependencies = TRUE,force = T\")"},{"path":"tutorial-of-radar-chart.html","id":"unzip-package-in-the-r-directory","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2 Unzip package in the R directory","text":" ","code":""},{"path":"tutorial-of-radar-chart.html","id":"step1-download-the-package-from-github.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.1 Step1: Download the package from github.","text":"\n ","code":""},{"path":"tutorial-of-radar-chart.html","id":"step2-move-the-zip-file-ggradar-master.zip-to-the-r-directory-..rbinx64-and-unzip-it.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.2 Step2: Move the zip file ggradar-master.zip to the R directory ../R/bin/x64 and unzip it.","text":"\n ","code":""},{"path":"tutorial-of-radar-chart.html","id":"step3-open-terminalmac-or-commandwindows-in-this-directory-and-run-the-following-script.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.3 Step3: Open terminal(Mac) or Command(Windows) in this directory and run the following script.","text":"\n ","code":"Rcmd build ggradar-master"},{"path":"tutorial-of-radar-chart.html","id":"this-will-create-a-ggradar_0.2.tar.gz-file-in-this-directory.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.4 This will create a ggradar_0.2.tar.gz file in this directory.","text":"\n ","code":""},{"path":"tutorial-of-radar-chart.html","id":"step4install-ggradar-in-rstudio","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.5 Step4:Install ggradar in RStudio","text":"Open RStudio, Tools -> Install Package choose install Package Archive File(.zip; .tar.gz).\n ","code":""},{"path":"tutorial-of-radar-chart.html","id":"browse-and-choose-the-ggradar_0.2.tar.gz-file-and-click-install.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.6 Browse and choose the ggradar_0.2.tar.gz file and click Install.","text":"\n ","code":""},{"path":"tutorial-of-radar-chart.html","id":"there-will-be-installation-information-in-the-console-showing-that-we-are-done.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.7 There will be installation information in the Console, showing that we are done.","text":"\n ","code":""},{"path":"tutorial-of-radar-chart.html","id":"now-we-can-check-in-the-packages-tab-that-if-ggradar-has-been-installed.","chapter":"84 Tutorial of radar chart","heading":"84.2.1.2.8 Now, we can check in the Packages tab that if ggradar has been installed.","text":"","code":""},{"path":"tutorial-of-radar-chart.html","id":"simple-example","chapter":"84 Tutorial of radar chart","heading":"84.2.2 Simple Example","text":"ggradar takes first string column group name rest numeric columns different axes plot. take columns 6 10 mtcars dataset, first column Group name.can find three circle lines radar chart, stand three bar values. 0%, 50% 100%. addition, five axes corresponding five columns data. axis connected inner layer solid line.","code":"\n#load data\nmtcars_radar <- mtcars %>% \n  as_tibble(rownames = \"group\") %>% \n  mutate_at(vars(-group), rescale) %>% \n  tail(2) %>% \n  select(1,6:10)\n\n#check data type with std() function\nstr(mtcars_radar)## tibble [2 × 6] (S3: tbl_df/tbl/data.frame)\n##  $ group: chr [1:2] \"Maserati Bora\" \"Volvo 142E\"\n##  $ drat : num [1:2] 0.359 0.622\n##  $ wt   : num [1:2] 0.526 0.324\n##  $ qsec : num [1:2] 0.0119 0.4881\n##  $ vs   : num [1:2] 0 1\n##  $ am   : num [1:2] 1 1\nggradar(mtcars_radar)"},{"path":"tutorial-of-radar-chart.html","id":"parameters-usage","chapter":"84 Tutorial of radar chart","heading":"84.2.3 Parameters Usage","text":"","code":""},{"path":"tutorial-of-radar-chart.html","id":"change-text-size","chapter":"84 Tutorial of radar chart","heading":"84.2.3.1 Change text size","text":"axis.label.size axis text.grid.label.size text three grids.","code":"\nggradar(mtcars_radar,axis.label.size = 10)\nggradar(mtcars_radar,grid.label.size = 10)"},{"path":"tutorial-of-radar-chart.html","id":"set-the-bar-value-for-each-grid","chapter":"84 Tutorial of radar chart","heading":"84.2.3.2 Set the bar value for each grid","text":"values.radar setting value grid. takes three values. provide three, first three taken.","code":"\nggradar(mtcars_radar,             \n            # \"125%\" will be ignore.\n            values.radar = c(\"33%\",\"66%\",\"100%\",\"125%\")\n        )"},{"path":"tutorial-of-radar-chart.html","id":"set-the-split-ratio-for-each-grid","chapter":"84 Tutorial of radar chart","heading":"84.2.3.3 Set the split ratio for each grid","text":"Three grids split data ratio according grid.max, grid.mid grid.min. correspond values.radar.","code":"\nggradar(mtcars_radar,\n            values.radar = c(\"0%\",\"55%\",\"100%\"),\n            grid.max = 1,\n            grid.mid = 0.6,\n            grid.min = 0\n        )"},{"path":"tutorial-of-radar-chart.html","id":"legend-2","chapter":"84 Tutorial of radar chart","heading":"84.2.3.4 Legend","text":"want get rid legend can set plot.legend = FALSE legend.position = \"none.","code":"\nggradar(mtcars_radar,\n                plot.legend = FALSE)\nggradar(mtcars_radar,\n                legend.position = \"none\")"},{"path":"tutorial-of-radar-chart.html","id":"background-and-styles-of-grid-lines.","chapter":"84 Tutorial of radar chart","heading":"84.2.3.5 Background and styles of grid lines.","text":"Use background.circle.colour set background color chart. gridline.max.colour, gridline.min.colour gridline.max.colour colors three grid lines. gridline.min.linetype can set style grid line solid longdash.","code":"\nggradar(mtcars_radar, \n             background.circle.colour = 'yellow',\n             gridline.min.colour = 'red',\n             gridline.mid.colour = 'blue',\n             gridline.max.colour = 'green',\n             gridline.min.linetype = 'solid')"},{"path":"tutorial-of-radar-chart.html","id":"set-the-colors-of-axes.","chapter":"84 Tutorial of radar chart","heading":"84.2.3.6 Set the colors of axes.","text":"group.colours take c() colors group.","code":"\nggradar(mtcars_radar,\n                  group.colours = c('red','blue'))"},{"path":"tutorial-of-radar-chart.html","id":"set-the-size-of-point","chapter":"84 Tutorial of radar chart","heading":"84.2.3.7 Set the size of point","text":"group.point.size take one value values data size.","code":"\nggradar(mtcars_radar,\n              group.point.size = 8\n        )\nggradar(mtcars_radar,\n              group.point.size = c(rep(2,6),rep(12, 6))\n        )"},{"path":"tutorial-of-radar-chart.html","id":"set-the-width-of-line","chapter":"84 Tutorial of radar chart","heading":"84.2.3.8 Set the width of line","text":"group.line.width take one value values data size.","code":"\nggradar(mtcars_radar,\n              group.line.width = 3\n        )\nggradar(mtcars_radar,\n              group.line.width = c(rep(1,6),rep(3,6))\n        )"},{"path":"tutorial-of-radar-chart.html","id":"radar-chart-using-fmsb","chapter":"84 Tutorial of radar chart","heading":"84.3 Radar chart using fmsb","text":"","code":""},{"path":"tutorial-of-radar-chart.html","id":"installation-6","chapter":"84 Tutorial of radar chart","heading":"84.3.1 Installation","text":"can install fmsb package using command : ","code":"install.packages(\"fmsb\")"},{"path":"tutorial-of-radar-chart.html","id":"simple-example-1","chapter":"84 Tutorial of radar chart","heading":"84.3.2 Simple Example","text":"default, data frame must include maximum values row 1 minimum values row 2 variables create axis, actual data given row 3 lower rows. first modify data use fmsb::radarchart draw plot:","code":"\ndf_maxmin <- data.frame(\n    drat = c(1, 0),\n    wt = c(1, 0),\n    qsec = c(1, 0),\n    vs = c(1, 0),\n    am = c(1, 0))\n\nmtcars_radar <- mtcars_radar[,c('drat','wt','qsec','vs','am')]\nmtcars_radar <- rbind(df_maxmin, mtcars_radar)\nfmsb::radarchart(mtcars_radar)"},{"path":"tutorial-of-radar-chart.html","id":"parameters-usage-1","chapter":"84 Tutorial of radar chart","heading":"84.3.3 Parameters Usage","text":"","code":""},{"path":"tutorial-of-radar-chart.html","id":"change-the-axistype","chapter":"84 Tutorial of radar chart","heading":"84.3.3.1 Change the axistype","text":"can change different axistype0 – axis label(Default)0 – axis label(Default)1 – center axis label only1 – center axis label only2 – around--chart label only2 – around--chart label only3 – center around--chart (peripheral) labels3 – center around--chart (peripheral) labels4 – *.** format axistype14 – *.** format axistype15 – *.** format axistype35 – *.** format axistype3","code":"\npar(mar = c(1, 2, 2, 1),mfrow = c(2, 2))\nradarchart(mtcars_radar, axistype = 1)\nradarchart(mtcars_radar, axistype = 2)\nradarchart(mtcars_radar, axistype = 3)\nradarchart(mtcars_radar, axistype = 4)"},{"path":"tutorial-of-radar-chart.html","id":"change-axis-segments","chapter":"84 Tutorial of radar chart","heading":"84.3.3.2 Change axis segments","text":"can modify seg change number segments axis.\nplot , can see set seg = 1 segments becomes less.","code":"\npar(mar = c(1, 1, 2, 1),mfrow = c(1,2))\nradarchart(mtcars_radar, axistype = 2)\nradarchart(mtcars_radar, axistype = 2, seg = 1)"},{"path":"tutorial-of-radar-chart.html","id":"set-points-symbol","chapter":"84 Tutorial of radar chart","heading":"84.3.3.3 Set Points Symbol","text":"can modify pty change point symbol. don’t plot data points, 32. default value 16. Besides, symbol can try.","code":"\npar(mar = c(1, 2, 2, 1),mfrow = c(2,2))\nradarchart(mtcars_radar, axistype = 2) # default\nradarchart(mtcars_radar, axistype = 2, pty = 32) # no points\nradarchart(mtcars_radar, axistype = 2, pty = 24) # triangle\nradarchart(mtcars_radar, axistype = 2, pty = 18) # square"},{"path":"tutorial-of-radar-chart.html","id":"modify-lines-apperance-colortypewidth","chapter":"84 Tutorial of radar chart","heading":"84.3.3.4 Modify Lines’ Apperance (color/type/width)","text":"pcol : vector color codes plot dataplty : vector line types plot dataplwd : vector line widths plot data","code":"\npar(mar = c(1, 2, 2, 1),mfrow = c(2,2))\nradarchart(mtcars_radar, axistype = 2)\nradarchart(mtcars_radar, axistype = 2,  pcol = c('#7FBF3F', '#FFB602'), )\nradarchart(mtcars_radar, axistype = 2, plty = c(1,1))\nradarchart(mtcars_radar, axistype = 2, plwd = c(0.3,5))"},{"path":"tutorial-of-radar-chart.html","id":"modify-the-axislines-colortypewidth","chapter":"84 Tutorial of radar chart","heading":"84.3.3.5 Modify the axislines (color/type/width)","text":"can change axislines appearance using parameters:cglcol:Line color radar gridscglty:Line type radar gridscglwd:Line width radar grids","code":"\npar(mar = c(1, 2, 2, 1),mfrow = c(2,2))\nradarchart(mtcars_radar, axistype = 2, cglcol = '#000000') \nradarchart(mtcars_radar, axistype = 2, cglty = 1, cglcol = '#7FBF3F')\nradarchart(mtcars_radar, axistype = 2, cglty = 1, cglcol = '#000000')\nradarchart(mtcars_radar, axistype = 2, cglty = 1, cglwd = 3, cglcol = '#000000')"},{"path":"tutorial-of-radar-chart.html","id":"filling-polygons","chapter":"84 Tutorial of radar chart","heading":"84.3.3.6 Filling Polygons","text":"can add filling polygons appearance using parameters:pfcol: vector color codes filling polygonspdensity: vector filling density polygonspangle: vector angles lines used filling polygons","code":"\npar(mar = c(1, 2, 2, 1),mfrow = c(2,2))\nradarchart(mtcars_radar, axistype = 2,  pfcol = c('#FFFF00','#7FBF3F'))\nradarchart(mtcars_radar, axistype = 2,  pfcol = c('#FFFF00','#7FBF3F'), pdensity= c(25,50))\nradarchart(mtcars_radar, axistype = 2,  pfcol = c('#FFFF00','#7FBF3F'), pdensity= c(10,10), pangle = c(0,0), plty = c(1,6))\nradarchart(mtcars_radar, axistype = 2,  pfcol = c(NA,'#99999980'), cglty=1)"},{"path":"interactive-radar.html","id":"interactive-radar","chapter":"85 Interactive Radar","heading":"85 Interactive Radar","text":"can use plotly draw interactive radar plot. example:\ndefine type add multible trace . plot get, can zoom , see detailed value information move mouse corresponding position.","code":"\nmtcars_theta = c('drat','wt','qsec','vs','am','drat')\n\n\np <- plot_ly(\n  type = 'scatterpolar',\n  fill = 'toself'\n) %>%\n  add_trace(\n    r = c(0.359447,0.5259524,0.0119047, 0, 1, 0.359447),\n    theta = mtcars_theta,\n    name = 'Maserati Bora'\n  ) %>%\n  add_trace(\n    r = c(0.6221198, 0.3239581, 0.4880952, 1, 1, 0.6221198),\n    theta = mtcars_theta,\n    name = 'Volvo 142E'\n  ) %>%\n  layout(\n    polar = list(\n      radialaxis = list(\n        visible = TRUE,\n        range = c(0,1)\n      )\n    )\n  )\n \np"},{"path":"r-shiny-tutorial.html","id":"r-shiny-tutorial","chapter":"86 R shiny tutorial","heading":"86 R shiny tutorial","text":"Hanqin Zhou","code":""},{"path":"r-shiny-tutorial.html","id":"what-is-shiny","chapter":"86 R shiny tutorial","heading":"86.1 What is shiny?","text":"Shiny R package makes easy build interactive web apps straight R. can host standalone apps webpage embed R Markdown documents build dashboards. can also extend Shiny apps CSS themes, html widgets, JavaScript actions. useful data visualization allows users interact data help analyzer tell story. tutorial help understand shiny works, start shiny app embed visualization plots learned class! Sounds excited? Let’s begin.","code":""},{"path":"r-shiny-tutorial.html","id":"the-architecture-of-a-shiny-app","chapter":"86 R shiny tutorial","heading":"86.2 The architecture of a shiny app","text":"shiny app basically web page maintained server running R script. Hence two parts. Part one UI theme defines styles interactions part two Server defines displayed data changes action triggered. use must R package shiny installed loaded code.","code":""},{"path":"r-shiny-tutorial.html","id":"a-simple-shiny-app","chapter":"86 R shiny tutorial","heading":"86.3 A simple shiny App","text":"following simple shiny app templateTo get rendered just run code get:Showing webpage now running localhost. can visit URL browser. get something like :terminate server just stop running code.’s indicates environment set ! let’s go inputs outputs see can play shiny.","code":"\nui <- fluidPage(\"Hi, it is a shiny app!\") # start a shiny page\n\nserver<- function(input,output){}\n\nshinyApp(ui = ui, server = server)"},{"path":"r-shiny-tutorial.html","id":"inputs","chapter":"86 R shiny tutorial","heading":"86.3.1 Inputs","text":"Input variable app developer ask users put .","code":""},{"path":"r-shiny-tutorial.html","id":"outputs","chapter":"86 R shiny tutorial","heading":"86.3.2 Outputs","text":"outputs component mainly tells webpage render add space ui R object. “hist” indicates name plot displayed space, doesn’t mean must histogram. assign space name output define render function server part.various types put shiny:","code":""},{"path":"r-shiny-tutorial.html","id":"server","chapter":"86 R shiny tutorial","heading":"86.3.3 Server","text":"server function plot table displayed defined.3 rules server function.Save object display output$Build object display render*(), eg. renderPlot()Access input values input$example:get:Try move slider get different histograms according choice.","code":"\nui <- fluidPage(\n  sliderInput(inputId = \"mean\",label = \"Choose the distribution mean\", value = 2, min = -10,max = 10),\n  sliderInput(inputId = \"std\",label = \"Choose the distribution standard deviation\", value = 1, min = -3, max = 3),\n  plotOutput(\"hist\")\n) \n\nserver<- function(input,output){\n  output$hist <- renderPlot({\n    hist(rnorm(100,mean = input$mean, sd = input$std))\n  })\n}\n\nshinyApp(ui = ui, server = server)"},{"path":"r-shiny-tutorial.html","id":"reactivity","chapter":"86 R shiny tutorial","heading":"86.3.4 Reactivity","text":"value shiny took input called reactive value. reactive can’t used outside reactive function sever function!Render*() functions basic reactive functions. also reactive functions well. commonly used reactive . reactive function one can basic data.frame transformation data processing procedures based input values. result can called inside render functions. example:example server parse input values list values, use list instead input arguments following functions.Reactive function widely used. case, can’t decide data use visualization input variables.","code":"\nui <- fluidPage(\n  sliderInput(inputId = \"num\",label = \"Choose sample number\", value = 20, min = 100,max = 0),\n  plotOutput(\"hist\")\n) \n\nserver<- function(input,output){\n  data <- reactive({\n    rnorm(input$num)\n  })\n  output$hist <- renderPlot({\n    hist(rnorm(data()))\n  })\n}\n\nshinyApp(ui = ui, server = server)"},{"path":"r-shiny-tutorial.html","id":"use-it-in-edav","chapter":"86 R shiny tutorial","heading":"86.4 Use it in EDAV","text":"far actually learned know make interactive report R shiny.Take parallel coordinates plot mentioned Problem Set 2 example. can create dynamic plot give us parallel coordinates plot year New York City Non-New York City using shiny app.exampleThe result:can select another RegionIt result another plot","code":"\ndf<-read_csv(\"https://data.ny.gov/api/views/ca8h-8gjq/rows.csv\")\nyear_c = c(df$Year)\nRegion_c = c(df$Region)\nui <- fluidPage(\n    selectInput(\"year\", \"Year:\",year_c,selected = \"2020\"),\n    selectInput(\"region\", \"Region:\",Region_c,selected = \"New York City\"),\n    plotOutput(\"cord\"),\n\n) \n\nserver<- function(input,output){\n  t <- reactive({\n    df %>% filter(Year == input$year, Region == input$region)\n  }) \n  output$cord <- renderPlot({ggparcoord(t(),columns = c(5:14), scale = \"globalminmax\",showPoints = TRUE) +geom_vline(xintercept = 1:10, color = \"darkgray\")+\n  ylab('Count') +\n  xlab('Categories') +\ntheme(legend.position = \"right\",axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1)) })\n\n}\n\nshinyApp(ui = ui, server = server)"},{"path":"r-shiny-tutorial.html","id":"fancier-layout","chapter":"86 R shiny tutorial","heading":"86.5 Fancier layout?","text":"plain shiny app, add things top bottom based written sequence. neat. Shiny also provide interface developer familiar html css customize appearance.two ways.First can use HTML function like:Second can use tag function.Shiny provide tag inn HTML tag function behaviors HTML. list:use , assign tags$","code":"\nfluidPage(\n  HTML(\"<h1>My shiny App<\/h1>\")\n)fluidPage(\n  tags$h1(\"My shiny app\"),\n  tags$p(style = \"font-family:Impact\", \"See other apps here\")\n  tags$a(\"shiny shoucase\", href = \"https://shiny.rstudio.com/gallery/#user-showcase\")\n)"},{"path":"r-shiny-tutorial.html","id":"share-your-app","chapter":"86 R shiny tutorial","heading":"86.6 Share your app","text":"building shiny app, share others? Since know shiny needs server running R maintain webpage. can’t spread form static document. R shiny provide ways share workUse shinyapps.io\n maintained Rstudio, free. ’s secure scalable.Use shinyapps.ioUse shiny server\n can customized, users can build server tailor need base Linux. can also monitor usage performance .Use shiny server","code":""},{"path":"tutorial-on-r-markdown.html","id":"tutorial-on-r-markdown","chapter":"87 Tutorial on R Markdown","heading":"87 Tutorial on R Markdown","text":"Anni Chen","code":""},{"path":"tutorial-on-r-markdown.html","id":"r-code-chunks-and-inline-r-code","chapter":"87 Tutorial on R Markdown","heading":"87.1 R code chunks and inline R code","text":"code chunk one main element R Markdown R codes can run .\nlot things can code chunk: can produce text output, tables, graphics. fine control output via chunk options, can provided inside curly braces ({r }). example, can choose hide text output via chunk option results = ‘hide’, set figure height 4 inches via fig.height = 4. Chunk options separated commas.large number chunk options knitr. listed commonly used options:eval: Whether evaluate code chunk.eval: Whether evaluate code chunk.echo: Whether echo source code output document.echo: Whether echo source code output document.collapse: Whether merge text output source code single code block output. mostly cosmetic: collapse = TRUE makes output compact, since R source code text output displayed single output block. default collapse = FALSE means R expressions text output separated different blocks.collapse: Whether merge text output source code single code block output. mostly cosmetic: collapse = TRUE makes output compact, since R source code text output displayed single output block. default collapse = FALSE means R expressions text output separated different blocks.warning, message, error: Whether show warnings, messages, errors output document. Note set error = FALSE, rmarkdown::render() halt error code chunk, error displayed R console. Similarly, warning = FALSE message = FALSE, messages shown R console.warning, message, error: Whether show warnings, messages, errors output document. Note set error = FALSE, rmarkdown::render() halt error code chunk, error displayed R console. Similarly, warning = FALSE message = FALSE, messages shown R console.include: Whether include anything code chunk output document. include = FALSE, whole code chunk excluded output, note still evaluated eval = TRUE. trying set echo = FALSE, results = 'hide', warning = FALSE, message = FALSE, chances simply mean single option include = FALSE instead suppressing different types text output individually.include: Whether include anything code chunk output document. include = FALSE, whole code chunk excluded output, note still evaluated eval = TRUE. trying set echo = FALSE, results = 'hide', warning = FALSE, message = FALSE, chances simply mean single option include = FALSE instead suppressing different types text output individually.cache: Whether enable caching. caching enabled, code chunk evaluated next time document compiled (code chunk modified), can save time. However, want honestly remind two hard problems computer science (via Phil Karlton): naming things, cache invalidation. Caching can handy also tricky sometimes.cache: Whether enable caching. caching enabled, code chunk evaluated next time document compiled (code chunk modified), can save time. However, want honestly remind two hard problems computer science (via Phil Karlton): naming things, cache invalidation. Caching can handy also tricky sometimes.fig.width fig.height: (graphical device) size R plots inches. R plots code chunks first recorded via graphical device knitr, written files. can also specify two options together single chunk option fig.dim, e.g., fig.dim = c(6, 4) means fig.width = 6 fig.height = 4.fig.width fig.height: (graphical device) size R plots inches. R plots code chunks first recorded via graphical device knitr, written files. can also specify two options together single chunk option fig.dim, e.g., fig.dim = c(6, 4) means fig.width = 6 fig.height = 4..width .height: output size R plots output document. options may scale images. can use percentages, e.g., .width = '80%' means 80% page width..width .height: output size R plots output document. options may scale images. can use percentages, e.g., .width = '80%' means 80% page width.taking Question 5 HW1 example:show result well ignore warning, can customize options:","code":"\nlibrary(openintro)\nlibrary(ggplot2)\nloans <- openintro::loans_full_schema\nggplot(loans,aes(loan_amount,fill=grade))+geom_histogram(binwidth = 2000,color='white')+ggtitle('Histogram of loan amount with different loan grade')warning=FALSE,error=FALSE,echo = FALSE, out.width = '50%'"},{"path":"tutorial-on-r-markdown.html","id":"markdown-syntax","chapter":"87 Tutorial on R Markdown","heading":"87.2 Markdown syntax","text":"text R Markdown document written Markdown syntax. Precisely speaking, Pandoc’s Markdown. many flavors Markdown invented different people, Pandoc’s flavor comprehensive one knowledge. can find full documentation Pandoc’s Markdown https://pandoc.org/MANUAL.html.","code":""},{"path":"tutorial-on-r-markdown.html","id":"inline-information","chapter":"87 Tutorial on R Markdown","heading":"87.2.1 Inline information","text":"Inline text italic surrounded underscores asterisks, e.g., _text_ *text*. Bold text produced using pair double asterisks (**text**). pair tildes (~) turn text subscript (e.g., H~3~PO~4~ renders H3PO4). pair carets (^) produce superscript (e.g., Cu^2+^ renders Cu2+).Inline text italic surrounded underscores asterisks, e.g., _text_ *text*. Bold text produced using pair double asterisks (**text**). pair tildes (~) turn text subscript (e.g., H~3~PO~4~ renders H3PO4). pair carets (^) produce superscript (e.g., Cu^2+^ renders Cu2+).mark text inline code, use pair backticks, e.g., code. include n literal backticks, use least n+1 backticks outside, e.g., can use four backticks preserve three backtick inside: ```code```, rendered code.mark text inline code, use pair backticks, e.g., code. include n literal backticks, use least n+1 backticks outside, e.g., can use four backticks preserve three backtick inside: ```code```, rendered code.Using HTML syntax change Font: <span style=\"font-family:Broadway;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change Font: <span style=\"font-family:Broadway;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change font size: <span style=\"font-size:35px;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change font size: <span style=\"font-size:35px;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change font color: <span style=\"color:#33C0FF;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change font color: <span style=\"color:#33C0FF;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change background color: <span style=\"background-color:#33FF8B;\">Data Visulization<\/span> gives Data VisulizationUsing HTML syntax change background color: <span style=\"background-color:#33FF8B;\">Data Visulization<\/span> gives Data VisulizationHyperlinks created using syntax [text](link), e.g., [RStudio](https://www.rstudio.com) like: RStudio.Hyperlinks created using syntax [text](link), e.g., [RStudio](https://www.rstudio.com) like: RStudio.syntax images similar: just add exclamation mark, e.g., ![alt text image title](path//image).\nexample:![cute kitten](resources/Introduction_to_RMarkdown/cute_kitten.jpg)\nsyntax images similar: just add exclamation mark, e.g., ![alt text image title](path//image).\nexample:![cute kitten](resources/Introduction_to_RMarkdown/cute_kitten.jpg)\nFootnotes put inside square brackets caret ^[], e.g., ^[footnote.]Footnotes put inside square brackets caret ^[], e.g., ^[footnote.]","code":""},{"path":"tutorial-on-r-markdown.html","id":"block-level-elements","chapter":"87 Tutorial on R Markdown","heading":"87.2.2 Block-level elements","text":"Section headers can written number pound signs, e.g.,","code":"\n# First-level header\n\n## Second-level header\n\n### Third-level header"},{"path":"tutorial-on-r-markdown.html","id":"second-level-header","chapter":"87 Tutorial on R Markdown","heading":"87.3 Second-level header","text":"","code":""},{"path":"tutorial-on-r-markdown.html","id":"third-level-header","chapter":"87 Tutorial on R Markdown","heading":"87.3.1 Third-level header","text":"want certain heading numbered, can add {-} {.unnumbered} heading, e.g.,# Preface {-}Unordered list items start *,-, +, can nest one list within another list indenting sub-list, e.g.output like:one itemone itemone itemone itemone item\none item\none item\none item\none itemone itemone itemone itemone itemone itemone itemPlain code blocks can written three backticks, can also indent blocks four spaces, e.g.,general, ’d better leave least one empty line adjacent different elements, e.g., header paragraph.","code":"- one item\n- one item\n- one item\n    - one more item\n    - one more item\n    - one more item```\nThis text is displayed verbatim / preformatted\n```\n\nOr indent by four spaces:\n\n    This text is displayed verbatim / preformatted"},{"path":"tutorial-on-r-markdown.html","id":"math-expressions","chapter":"87 Tutorial on R Markdown","heading":"87.3.2 Math expressions","text":"Inline LaTeX equations can written pair dollar signs using LaTeX syntax, e.g., $f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$ (actual output: \\(f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\); math expressions display style can written pair double dollar signs, e.g., $$f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$$, output looks like :\\[f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\]can also use math environments inside $ $ \\[ \\], e.g.,\\[\\begin{array}{ccc}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{array}\\]\\[X = \\begin{bmatrix}1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\n\\end{bmatrix}\\]\\[\\begin{vmatrix}& b\\\\\nc & d\n\\end{vmatrix}=ad-bc\\]","code":"$$\\begin{array}{ccc}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{array}$$$$X = \\begin{bmatrix}1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\n\\end{bmatrix}$$$$\\begin{vmatrix}a & b\\\\\nc & d\n\\end{vmatrix}=ad-bc$$"},{"path":"tutorial-on-r-markdown.html","id":"tables","chapter":"87 Tutorial on R Markdown","heading":"87.3.3 Tables","text":"Formatting tables can complicated task, especially certain cells span one column row. even complicated consider different output formats.\nexample:gives output:Demonstration simple table syntax.looking advanced control styling tables, recommended use kableExtra package, provides functions customize appearance PDF HTML tables.","code":"  Right     Left     Center     Default\n-------     ------ ----------   -------\n     12     12        12            12\n    123     123       123          123\n      1     1          1             1\n      \nTable:  Demonstration of simple table syntax."},{"path":"tutorial-on-r-markdown.html","id":"citation-1","chapter":"87 Tutorial on R Markdown","heading":"87.4 Citation:","text":"R Markdown techniques please visit R Markdown: Definitive Guide.","code":""},{"path":"dlookr-package.html","id":"dlookr-package","chapter":"88 Dlookr package","heading":"88 Dlookr package","text":"Mridul Gupta","code":""},{"path":"dlookr-package.html","id":"introduction-18","chapter":"88 Dlookr package","heading":"88.1 Introduction","text":"tutorial, going learn basics `dlookr’ package, use dataset important relevant package data scientist, statistician .","code":""},{"path":"dlookr-package.html","id":"what-is-dlookr","chapter":"88 Dlookr package","heading":"88.2 What is dlookr?","text":"According Cran, dlookr collection tools support data diagnosis, exploration, transformation. Data diagnostics provides information visualization missing values outliers unique negative values help us understand distribution quality data. Data exploration provides information visualization descriptive statistics univariate variables, normality tests outliers, correlation two variables, relationship target variable predictor. Data transformation supports binning categorizing continuous variables, imputates missing values outliers, resolving skewness. creates automated reports support three tasks.","code":""},{"path":"dlookr-package.html","id":"why-is-it-important","chapter":"88 Dlookr package","heading":"88.3 Why is it important?","text":"Well description sufficient enough convience someone importance simpler words believe 3 reasons believe learning package worth putting time :One package functions help us diagnose data, explore transform even reporting findings. makes easier us remember important functions otherwise also remember packages allows us stuff.One package functions help us diagnose data, explore transform even reporting findings. makes easier us remember important functions otherwise also remember packages allows us stuff.can easily integrated used dplyr & tidyverse something now become ubiquitous indistry.can easily integrated used dplyr & tidyverse something now become ubiquitous indistry.Instead writing longer codes, package generally functions give lot information data without much transformation.Instead writing longer codes, package generally functions give lot information data without much transformation.","code":""},{"path":"dlookr-package.html","id":"usecase","chapter":"88 Dlookr package","heading":"88.4 Usecase","text":"help us understand use, let us use dataset. using Rolling sales data Manhattan.\nhttps://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page","code":"\n#Reading data\nmanhattan <- read_excel(\"rollingsales_manhattan.xlsx\", skip = 4)\ndim(manhattan)## [1] 19354    21"},{"path":"dlookr-package.html","id":"data-diagnosis","chapter":"88 Dlookr package","heading":"88.4.1 Data Diagnosis","text":"","code":""},{"path":"dlookr-package.html","id":"overall-diagnosis","chapter":"88 Dlookr package","heading":"88.4.1.1 Overall Diagnosis","text":"easily get columns data missing.Now lets look different features/columns data.","code":"\n# Get missing and unique count for each column\ndiagnose(manhattan)## # A tibble: 21 × 6\n##    variables       types  missing_count missing_percent unique_count unique_rate\n##    <chr>           <chr>          <int>           <dbl>        <int>       <dbl>\n##  1 BOROUGH         chara…             0          0                 1   0.0000517\n##  2 NEIGHBORHOOD    chara…             0          0                39   0.00202  \n##  3 BUILDING_CLASS… chara…             0          0                39   0.00202  \n##  4 TAX_CLASS_AT_P… chara…            18          0.0930            9   0.000465 \n##  5 BLOCK           numer…             0          0              1341   0.0693   \n##  6 LOT             numer…             0          0              1662   0.0859   \n##  7 EASEMENT        logic…         19354        100                 1   0.0000517\n##  8 BUILDING_CLASS… chara…            18          0.0930          112   0.00579  \n##  9 ADDRESS         chara…             0          0             19010   0.982    \n## 10 APARTMENT_NUMB… chara…         10204         52.7            2630   0.136    \n## # … with 11 more rows\n# Using with dplyr and finding columns with missing data\ndiagnose(manhattan)## # A tibble: 21 × 6\n##    variables       types  missing_count missing_percent unique_count unique_rate\n##    <chr>           <chr>          <int>           <dbl>        <int>       <dbl>\n##  1 BOROUGH         chara…             0          0                 1   0.0000517\n##  2 NEIGHBORHOOD    chara…             0          0                39   0.00202  \n##  3 BUILDING_CLASS… chara…             0          0                39   0.00202  \n##  4 TAX_CLASS_AT_P… chara…            18          0.0930            9   0.000465 \n##  5 BLOCK           numer…             0          0              1341   0.0693   \n##  6 LOT             numer…             0          0              1662   0.0859   \n##  7 EASEMENT        logic…         19354        100                 1   0.0000517\n##  8 BUILDING_CLASS… chara…            18          0.0930          112   0.00579  \n##  9 ADDRESS         chara…             0          0             19010   0.982    \n## 10 APARTMENT_NUMB… chara…         10204         52.7            2630   0.136    \n## # … with 11 more rows\nmanhattan %>%\n  diagnose() %>%\n  select(-unique_count, -unique_rate) %>% \n  filter(missing_count > 0) %>% \n  arrange(desc(missing_count))## # A tibble: 10 × 4\n##    variables                 types     missing_count missing_percent\n##    <chr>                     <chr>             <int>           <dbl>\n##  1 EASEMENT                  logical           19354        100     \n##  2 LAND_SQUARE_FEET          numeric           17885         92.4   \n##  3 GROSS_SQUARE_FEET         numeric           17885         92.4   \n##  4 COMMERCIAL_UNITS          numeric           17292         89.3   \n##  5 APARTMENT_NUMBER          character         10204         52.7   \n##  6 RESIDENTIAL_UNITS         numeric            9328         48.2   \n##  7 TOTAL_UNITS               numeric            8735         45.1   \n##  8 YEAR_BUILT                numeric            2057         10.6   \n##  9 TAX_CLASS_AT_PRESENT      character            18          0.0930\n## 10 BUILDING_CLASS_AT_PRESENT character            18          0.0930"},{"path":"dlookr-package.html","id":"numerical-data-diagnosis","chapter":"88 Dlookr package","heading":"88.4.1.2 Numerical data diagnosis","text":"One function directly gives quantiles,mean, zeros, negative values outliers numeric values.","code":"\n# Looking at numerical data\ndiagnose_numeric(manhattan)## # A tibble: 10 × 10\n##    variables       min     Q1     mean median     Q3     max  zero minus outlier\n##    <chr>         <dbl>  <dbl>    <dbl>  <dbl>  <dbl>   <dbl> <int> <int>   <int>\n##  1 BLOCK            10    738   1.10e3   1162 1.45e3  2.25e3     0     0       0\n##  2 LOT               1     29   7.52e2   1003 1.20e3  9.11e3     0     0     451\n##  3 ZIP_CODE      10001  10013   1.00e4  10022 1.00e4  1.05e4     0     0    2601\n##  4 RESIDENTIAL_…     0      1   2.68e0      1 1   e0  4.9 e2   300     0    1257\n##  5 COMMERCIAL_U…     0      0   2.09e0      1 1   e0  2.39e2   870     0     192\n##  6 TOTAL_UNITS       0      1   2.93e0      1 1   e0  4.92e2    88     0    1228\n##  7 LAND_SQUARE_…     0   1749   6.02e3   2313 4.25e3  6.59e5     8     0     191\n##  8 GROSS_SQUARE…     0   3756   3.35e4   6681 1.38e4  2.4 e6    83     0     230\n##  9 YEAR_BUILT     1800   1922   1.95e3   1957 1.99e3  2.02e3     0     0       3\n## 10 SALE_PRICE        0 430000   2.42e6 925000 2.00e6  8.10e8  3136     0    1873\n# Using with dplyr and finding colums with zero values\ndiagnose_numeric(manhattan) %>% \n  filter(zero > 0) ## # A tibble: 6 × 10\n##   variables           min     Q1   mean median     Q3    max  zero minus outlier\n##   <chr>             <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <int> <int>   <int>\n## 1 RESIDENTIAL_UNITS     0      1 2.68e0      1 1   e0 4.9 e2   300     0    1257\n## 2 COMMERCIAL_UNITS      0      0 2.09e0      1 1   e0 2.39e2   870     0     192\n## 3 TOTAL_UNITS           0      1 2.93e0      1 1   e0 4.92e2    88     0    1228\n## 4 LAND_SQUARE_FEET      0   1749 6.02e3   2313 4.25e3 6.59e5     8     0     191\n## 5 GROSS_SQUARE_FEET     0   3756 3.35e4   6681 1.38e4 2.4 e6    83     0     230\n## 6 SALE_PRICE            0 430000 2.42e6 925000 2.00e6 8.10e8  3136     0    1873"},{"path":"dlookr-package.html","id":"categorical-data-diagnosis","chapter":"88 Dlookr package","heading":"88.4.1.3 Categorical data diagnosis","text":"directly gives us levels categorical columns frequency well.","code":"\n# Looking at categorical data\ndiagnose_category(manhattan)## # A tibble: 83 × 6\n##    variables    levels                        N  freq  ratio  rank\n##    <chr>        <chr>                     <int> <int>  <dbl> <int>\n##  1 BOROUGH      1                         19354 19354 100        1\n##  2 NEIGHBORHOOD UPPER EAST SIDE (59-79)   19354  1999  10.3      1\n##  3 NEIGHBORHOOD UPPER EAST SIDE (79-96)   19354  1886   9.74     2\n##  4 NEIGHBORHOOD UPPER WEST SIDE (59-79)   19354  1746   9.02     3\n##  5 NEIGHBORHOOD MIDTOWN EAST              19354  1160   5.99     4\n##  6 NEIGHBORHOOD UPPER WEST SIDE (79-96)   19354  1151   5.95     5\n##  7 NEIGHBORHOOD CHELSEA                   19354   912   4.71     6\n##  8 NEIGHBORHOOD MIDTOWN WEST              19354   881   4.55     7\n##  9 NEIGHBORHOOD GREENWICH VILLAGE-CENTRAL 19354   759   3.92     8\n## 10 NEIGHBORHOOD HARLEM-CENTRAL            19354   708   3.66     9\n## # … with 73 more rows\n# Filtering categories with NA levels\n\ndiagnose_category(manhattan) %>% \n  filter(is.na(levels))## # A tibble: 2 × 6\n##   variables            levels     N  freq   ratio  rank\n##   <chr>                <chr>  <int> <int>   <dbl> <int>\n## 1 TAX_CLASS_AT_PRESENT <NA>   19354    18  0.0930     8\n## 2 APARTMENT_NUMBER     <NA>   19354 10204 52.7        1"},{"path":"dlookr-package.html","id":"outlier-diagnosis","chapter":"88 Dlookr package","heading":"88.4.1.4 Outlier diagnosis","text":"tells us number outliers numerical column. look with_mean without_mean column also helps us analyse effect outlier data.can even plot outliers:use diagnose_outlier(), plot_outlier(), dplyr packages visualize numerical variables outlier ratio 0.5% higher.","code":"\n# Diagnose outlier for each numerical column/feature\n\ndiagnose_outlier(manhattan) ## # A tibble: 10 × 6\n##    variables    outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n##    <chr>               <int>          <dbl>         <dbl>     <dbl>        <dbl>\n##  1 BLOCK                   0         0              NaN      1.10e3     1097.   \n##  2 LOT                   451         2.33          4598.     7.52e2      660.   \n##  3 ZIP_CODE             2601        13.4          10103.     1.00e4    10019.   \n##  4 RESIDENTIAL…         1257         6.49            14.4    2.68e0        1    \n##  5 COMMERCIAL_…          192         0.992           16.5    2.09e0        0.613\n##  6 TOTAL_UNITS          1228         6.34            17.7    2.93e0        1    \n##  7 LAND_SQUARE…          191         0.987        28980.     6.02e3     2588.   \n##  8 GROSS_SQUAR…          230         1.19        175727.     3.35e4     7072.   \n##  9 YEAR_BUILT              3         0.0155        1800      1.95e3     1954.   \n## 10 SALE_PRICE           1873         9.68      14915667.     2.42e6  1081708.\n# Diagnose outlier for each numerical column/feature\nmanhattan %>%\n  plot_outlier(diagnose_outlier(manhattan) %>% \n                 filter(outliers_ratio >= 0.5) %>% \n                 select(variables) %>% \n                 unlist())"},{"path":"dlookr-package.html","id":"eda","chapter":"88 Dlookr package","heading":"88.4.2 EDA","text":"","code":""},{"path":"dlookr-package.html","id":"univariate-analysis","chapter":"88 Dlookr package","heading":"88.4.2.1 Univariate Analysis","text":"gives detailed metrics regarding distribution numerical variables. Along basic metrics like mean, standard deviation also gives metrics like skewness, kurtosis, percentiles, IQR etc.###Bivariate Analysis","code":"\n# Looking at the numerical data\ndescribe(manhattan)## # A tibble: 10 × 26\n##    variable       n    na    mean      sd se_mean    IQR skewness kurtosis   p00\n##    <chr>      <int> <int>   <dbl>   <dbl>   <dbl>  <dbl>    <dbl>    <dbl> <dbl>\n##  1 BLOCK      19354     0  1.10e3  5.20e2 3.74e+0 7.09e2   -0.149   -0.464    10\n##  2 LOT        19354     0  7.52e2  8.86e2 6.37e+0 1.18e3    2.77    16.2       1\n##  3 ZIP_CODE   19354     0  1.00e4  3.61e1 2.60e-1 1.5 e1    4.14    25.4   10001\n##  4 RESIDENTI… 10026  9328  2.68e0  1.16e1 1.16e-1 0        16.6    448.        0\n##  5 COMMERCIA…  2062 17292  2.09e0  1.04e1 2.30e-1 1   e0   13.7    234.        0\n##  6 TOTAL_UNI… 10619  8735  2.93e0  1.24e1 1.20e-1 0        15.0    361.        0\n##  7 LAND_SQUA…  1469 17885  6.02e3  2.73e4 7.12e+2 2.50e3   20.4    467.        0\n##  8 GROSS_SQU…  1469 17885  3.35e4  1.34e5 3.49e+3 1.00e4   11.4    164.        0\n##  9 YEAR_BUILT 17297  2057  1.95e3  3.81e1 2.89e-1 6.4 e1    0.227   -1.11   1800\n## 10 SALE_PRICE 19354     0  2.42e6  1.03e7 7.37e+4 1.57e6   35.7   2249.        0\n## # … with 16 more variables: p01 <dbl>, p05 <dbl>, p10 <dbl>, p20 <dbl>,\n## #   p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>, p60 <dbl>, p70 <dbl>,\n## #   p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>, p99 <dbl>, p100 <dbl>\n# Looking at the numerical data\nnormality(manhattan)## # A tibble: 10 × 4\n##    vars              statistic  p_value sample\n##    <chr>                 <dbl>    <dbl>  <dbl>\n##  1 BLOCK                 0.982 6.39e-25   5000\n##  2 LOT                   0.696 8.76e-70   5000\n##  3 ZIP_CODE              0.576 1.28e-76   5000\n##  4 RESIDENTIAL_UNITS     0.171 7.44e-75   5000\n##  5 COMMERCIAL_UNITS      0.141 1.89e-44   5000\n##  6 TOTAL_UNITS           0.171 2.92e-76   5000\n##  7 LAND_SQUARE_FEET      0.493 3.19e-32   5000\n##  8 GROSS_SQUARE_FEET     0.349 2.33e-35   5000\n##  9 YEAR_BUILT            0.937 6.87e-40   5000\n## 10 SALE_PRICE            0.223 5.76e-90   5000\n# Looking at the numerical data\nplot_normality(manhattan)\n# Looking at the numerical data\nmanhattan %>%\n  filter(NEIGHBORHOOD == \"MIDTOWN EAST\") %>%\n  group_by(`ZIP_CODE`) %>%\n  plot_normality(`SALE_PRICE`)\n# Looking at the numerical data\ncorrelate(manhattan)## # A tibble: 90 × 3\n##    var1              var2  coef_corr\n##    <fct>             <fct>     <dbl>\n##  1 LOT               BLOCK  -0.185  \n##  2 ZIP_CODE          BLOCK   0.239  \n##  3 RESIDENTIAL_UNITS BLOCK   0.0462 \n##  4 COMMERCIAL_UNITS  BLOCK  -0.0808 \n##  5 TOTAL_UNITS       BLOCK   0.0377 \n##  6 LAND_SQUARE_FEET  BLOCK  -0.00909\n##  7 GROSS_SQUARE_FEET BLOCK  -0.0860 \n##  8 YEAR_BUILT        BLOCK  -0.0974 \n##  9 SALE_PRICE        BLOCK  -0.0487 \n## 10 BLOCK             LOT    -0.185  \n## # … with 80 more rows\n# Looking at the numerical data\ncorrelate(manhattan, `SALE_PRICE`,`YEAR_BUILT`,`LAND_SQUARE_FEET`)## # A tibble: 27 × 3\n##    var1             var2              coef_corr\n##    <fct>            <fct>                 <dbl>\n##  1 LAND_SQUARE_FEET BLOCK              -0.00909\n##  2 YEAR_BUILT       BLOCK              -0.0974 \n##  3 SALE_PRICE       BLOCK              -0.0487 \n##  4 LAND_SQUARE_FEET LOT                 0.0194 \n##  5 YEAR_BUILT       LOT                 0.472  \n##  6 SALE_PRICE       LOT                 0.00774\n##  7 LAND_SQUARE_FEET ZIP_CODE           -0.0222 \n##  8 YEAR_BUILT       ZIP_CODE            0.0504 \n##  9 SALE_PRICE       ZIP_CODE           -0.0249 \n## 10 LAND_SQUARE_FEET RESIDENTIAL_UNITS   0.114  \n## # … with 17 more rows\n# Looking at the numerical data\nplot_correlate(manhattan)\n# Looking at the numerical data\nplot_correlate(manhattan, `SALE_PRICE`,`YEAR_BUILT`,`LAND_SQUARE_FEET`)"},{"path":"dlookr-package.html","id":"eda-on-target-variable","chapter":"88 Dlookr package","heading":"88.4.3 EDA on Target Variable","text":"","code":"\n# Imputing Tax class at time of sale column\nnum <- target_by(manhattan,`SALE_PRICE`)\nnum_num <- relate(num,`LAND_SQUARE_FEET`)\nnum_num## \n## Call:\n## lm(formula = formula_str, data = data)\n## \n## Coefficients:\n##      (Intercept)  LAND_SQUARE_FEET  \n##        7574327.3             102.1\n# Imputing Tax class at time of sale column\nsummary(num_num)## \n## Call:\n## lm(formula = formula_str, data = data)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -73368230  -7780279  -5859693   -894898 798878520 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      7574327.3   824042.4   9.192  < 2e-16 ***\n## LAND_SQUARE_FEET     102.1       29.5   3.460 0.000557 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 30840000 on 1467 degrees of freedom\n##   (17885 observations deleted due to missingness)\n## Multiple R-squared:  0.008092,   Adjusted R-squared:  0.007416 \n## F-statistic: 11.97 on 1 and 1467 DF,  p-value: 0.0005566\n# Imputing Tax class at time of sale column\nplot(num_num)"},{"path":"dlookr-package.html","id":"data-transformation","chapter":"88 Dlookr package","heading":"88.4.4 Data Transformation","text":"","code":""},{"path":"dlookr-package.html","id":"missing-value-imputation","chapter":"88 Dlookr package","heading":"88.4.4.1 Missing value Imputation","text":"\n#### Standardization Resolving Skewness","code":"\n# Imputing Tax class at time of sale column\nland_square_feet <- imputate_na(manhattan,`LAND_SQUARE_FEET`,SALE_PRICE, method = \"mice\")## \n##  iter imp variable\n##   1   1  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   1   2  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   1   3  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   1   4  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   1   5  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   2   1  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   2   2  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   2   3  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   2   4  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   2   5  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   3   1  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   3   2  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   3   3  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   3   4  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   3   5  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   4   1  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   4   2  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   4   3  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   4   4  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   4   5  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   5   1  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   5   2  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   5   3  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   5   4  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n##   5   5  RESIDENTIAL_UNITS  COMMERCIAL_UNITS  TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT\n# Imputing outliers in zip code\nsummary(land_square_feet)## * Impute missing values based on Multivariate Imputation by Chained Equations\n##  - method : mice\n##  - random seed : 61454\n## \n## * Information of Imputation (before vs after)\n##              Original   Imputation\n## n          1469.00000  19354.00000\n## na        17885.00000      0.00000\n## mean       6019.76378  14981.84693\n## sd        27286.48687  22630.55203\n## se_mean     711.92907    162.67087\n## IQR        2502.00000  16708.75000\n## skewness     20.44256      7.42966\n## kurtosis    467.37765    111.16625\n## p00           0.00000      0.00000\n## p01          46.36000   1100.63600\n## p05        1084.60000   1710.79000\n## p10        1386.80000   2013.00000\n## p20        1659.40000   2735.52000\n## p25        1749.00000   3242.85000\n## p30        1862.00000   3901.96000\n## p40        2013.00000   5684.12000\n## p50        2313.00000   9002.90000\n## p60        2523.00000  12836.80000\n## p70        3679.20000  16147.56000\n## p75        4251.00000  19951.60000\n## p80        5046.00000  23612.84000\n## p90       10294.00000  33393.42000\n## p95       16066.00000  38319.14000\n## p99       55862.00000 136548.44400\n## p100     659375.00000 659375.00000\n# Imputing outliers in zip code\nplot(land_square_feet)\n# Imputing outliers in year built\nyear_built <- imputate_outlier(manhattan, YEAR_BUILT, method = \"capping\")\n# Imputing outliers in year built\nsummary(year_built)## Impute outliers with capping\n## \n## * Information of Imputation (before vs after)\n##               Original    Imputation\n## n        17297.0000000 17297.0000000\n## na        2057.0000000  2057.0000000\n## mean      1954.4412326  1954.4585766\n## sd          38.0661067    38.0184836\n## se_mean      0.2894363     0.2890742\n## IQR         64.0000000    64.0000000\n## skewness     0.2269967     0.2376013\n## kurtosis    -1.1091423    -1.1466029\n## p00       1800.0000000  1840.0000000\n## p01       1899.0000000  1899.0000000\n## p05       1900.0000000  1900.0000000\n## p10       1910.0000000  1910.0000000\n## p20       1920.0000000  1920.0000000\n## p25       1922.0000000  1922.0000000\n## p30       1926.0000000  1926.0000000\n## p40       1931.0000000  1931.0000000\n## p50       1957.0000000  1957.0000000\n## p60       1963.0000000  1963.0000000\n## p70       1974.0000000  1974.0000000\n## p75       1986.0000000  1986.0000000\n## p80       2000.0000000  2000.0000000\n## p90       2014.0000000  2014.0000000\n## p95       2017.0000000  2017.0000000\n## p99       2019.0000000  2019.0000000\n## p100      2021.0000000  2021.0000000\n# Imputing outliers in year built\nplot(year_built)\n# Imputing outliers in zip code\n\nplot_outlier(manhattan)\nmanhattan %>% \n  mutate(SALE_PRICE_MINMAX = transform(manhattan$SALE_PRICE, method = \"minmax\")) %>% \n  select(SALE_PRICE_MINMAX) %>% \n  boxplot()\nfind_skewness(manhattan, value = TRUE, thres = 0.1)##             BLOCK               LOT          ZIP_CODE RESIDENTIAL_UNITS \n##            -0.149             2.775             4.143            16.631 \n##  COMMERCIAL_UNITS       TOTAL_UNITS  LAND_SQUARE_FEET GROSS_SQUARE_FEET \n##            13.699            14.993            20.422            11.368 \n##        YEAR_BUILT        SALE_PRICE \n##             0.227            35.721\n# Looking at the numerical data\nplot_normality(manhattan)\n# Looking at the numerical data\ngross_square_feet_log = transform(manhattan$GROSS_SQUARE_FEET, method = \"log\")\nsummary(gross_square_feet_log)## * Resolving Skewness with log\n## \n## * Information of Transformation (before vs after)\n##               Original Transformation\n## n           1469.00000    1469.000000\n## na         17885.00000   17885.000000\n## mean       33477.88836           -Inf\n## sd        133861.41500            NaN\n## se_mean     3492.56513            NaN\n## IQR        10036.00000       1.300734\n## skewness      11.37915            NaN\n## kurtosis     163.87090            NaN\n## p00            0.00000           -Inf\n## p01            0.00000           -Inf\n## p05            0.00000           -Inf\n## p10         2435.40000       7.797866\n## p20         3363.00000       8.120588\n## p25         3756.00000       8.231110\n## p30         4129.20000       8.325839\n## p40         5100.00000       8.536996\n## p50         6681.00000       8.807023\n## p60         8924.00000       9.096500\n## p70        11250.20000       9.328136\n## p75        13792.00000       9.531844\n## p80        19333.20000       9.869571\n## p90        68108.00000      11.128850\n## p95       130298.20000      11.777576\n## p99       458473.72000      13.034214\n## p100     2400000.00000      14.690979\n# Looking at the numerical data\nplot(gross_square_feet_log)"},{"path":"dlookr-package.html","id":"diagnosis-report","chapter":"88 Dlookr package","heading":"88.4.4.2 Diagnosis report","text":"","code":"\n# NOT RUN\nmanhattan %>%\n  diagnose_web_report(subtitle = \"manhattan\", output_dir = \"./\", \n                      output_file = \"Diagn.html\", theme = \"blue\")\n# NOT RUN\nmanhattan %>%\n  diagnose_paged_report(subtitle = \"manhattan\", output_dir = \"./\",\n                        output_file = \"Diagn.pdf\", theme = \"blue\")"},{"path":"dlookr-package.html","id":"references-7","chapter":"88 Dlookr package","heading":"88.4.5 References","text":"https://github.com/choonghyunryu/dlookrhttps://github.com/choonghyunryu/dlookrhttps://cran.r-project.org/web/packages/dlookr/index.htmlhttps://cran.r-project.org/web/packages/dlookr/index.html","code":""},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"urca-unit-root-test-and-cointegration-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89 Urca: Unit Root Test and Cointegration Test","text":"Zonghan Yue(zy2493), Alvin Pan (qp2134)","code":"\nlibrary(urca)\nlibrary(ggplot2)\nlibrary(RCurl)"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"introduction-19","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.1 Introduction:","text":"Rstudio package urca developed Bernhard Pfaff, Eric Zivot, Matthieu Stigler 2016. “urca” abbreviation Unit Root Cointegration Tests Time Series Data. package provides functions users cointegration Unit Root test.","code":""},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"dataset-description","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.1.0.1 Dataset Description:","text":"Data Set scraped OKEX exchange’s API. minute level historical candlesticks ETH price.Columns:\n  time: UTC time\n  open: first trade time period\n  high: highest price trade time period\n  low: lowest price trade time period\n  close: last trade time period\n  volume: quantity trades time periodExamples:","code":"\neth <- read.csv(url(\"https://raw.githubusercontent.com/yzh9810/edav_cc/main/eth-usdt.csv\"))\neth$time = as.POSIXlt(eth$time)"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"what-is-stationarity","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.1.0.2 What is stationarity ?","text":"stationary time series one whose properties depend time series observed. Thus, time series trends, seasonality, stationary — trend seasonality affect value time series different times. hand, white noise series stationary — matter observe , look much point time.","code":"\nplot(eth$time, eth$close, type=\"l\", xlab = \"Time\", ylab = \"price\", main = \"Unstationary: Eth Price\")\ntime = eth$time[2:100000]\npre_close = eth$close[1:99999]\nnext_close = eth$close[2:100000]\nclose = next_close - pre_close\nplot(time, close, type=\"l\", xlab = \"Time\", ylab = \"price\", main = \"stationary: Eth Return\")"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"what-is-unit-root","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.1.0.3 What is Unit Root?","text":"Unit Root: unit root (also called unit root process different stationary process) stochastic trend time series, sometimes called “random walk drift”; time series unit root, shows systematic pattern unpredictable.typical simplest time series model autoregressive model,\n\\[y_{t}=ay_{t-1}+\\epsilon_{t}\\]\nequation, predict variable \\(y_{t}\\) predicted previous time period \\(y_{t-1}\\) errors. preceding model first-order autoregression model. \\(< 1\\), model stationary. \\(= 1\\), model nonstationary, \\(\\) stationary test.Unit Root Tests:\nUnit root tests tests stationarity time series. shape stationarity shift time doesn’t cause change shape distribution. Unit roots one cause non-stationarity.","code":""},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"what-is-cointegration-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.1.0.4 What is Cointegration test?","text":"Cointegration test used establish correlation several time series long term.","code":""},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"below-we-have-included-decumentations-and-experiments-of-several-popular-test-methods","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.1.0.5 Below we have included decumentations and experiments of several popular test methods","text":"","code":""},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"unit-root-tests-methods","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.2 Unit Root Tests methods:","text":"","code":""},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"the-dickey-fuller-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.2.0.1 The Dickey Fuller Test:","text":"Dickey Fuller Test based linear regression.H0: null hypothes unit root present autoregressive time series model.H1: unit root present autoregressive time series model.formula test AR(1) \\(\\alpha = 0\\) \\(\\beta = 0\\)\\[y_{t} = (\\sigma y_{t-1}) + \\epsilon_{t}\\]\nthus,\n\\[\\Delta y_{t} = (\\sigma-1) y_{t-1} + \\epsilon_{t} = \\gamma y_{t-1} + \\epsilon_{t}\\]urca method: ur.df: Augmented-Dickey-Fuller Unit Root TestArguments:  y:  Vector tested unit root.  type:  Test type, either “none”, “drift” “trend”.AR(1) model:\n\\[y_{t} = \\alpha + \\beta t +  \\sigma y_{t-1} + \\epsilon_{t}\\]\n“none” means \\(\\alpha = 0\\) \\(\\beta = 0\\),“drift” means \\(\\beta = 0\\), \\(\\alpha\\) equal 0“trend” means \\(\\alpha\\) \\(\\beta\\) equal 0  lags:  Number lags endogenous variables included.Lags time series many previous time period used predict target variable. example, lags n means equation use \\[y_{t} = \\alpha + \\beta t +  (\\sigma_{t-1} y_{t-1} + \\sigma_{t-2} y_{t-2} + \\sigma_{t-3} y_{t-3} +...+ \\sigma_{t-n} y_{t-n}) + \\epsilon_{t}\\]  selectlags:  Lag selection can achieved according Akaike AIC\" Bayes “BIC” information criteria. maximum number lags considered set lags. default use “fixed” lag length set lags.Usage  ur.df(y, type = c(“none”, “drift”, “trend”), lags = 1, selectlags = c(“Fixed”, “AIC”, “BIC”))ExampleInterpretationValue test-statistic 0.2241 Critical values test statistics : tau1 -2.58 -1.95 -1.62 (1%, 5%, 10%). Thus, reject null, means unit root present. “z.lag1” \\(\\gamma\\) term, coefficient lag term (y(t-1)), p=0.823, implies gamma isn’t statistically significant model.However, R-square 0.00036, means time series satisfied non-draft non-trend model. Thus, model actualy fit data.","code":"\nurtest = ur.df(eth$close, type=\"none\", lags = 1)\nsummary(urtest)## \n## ############################################### \n## # Augmented Dickey-Fuller Test Unit Root Test # \n## ############################################### \n## \n## Test regression none \n## \n## \n## Call:\n## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -175.109   -2.053   -0.014    2.026  129.091 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## z.lag.1    7.198e-07  3.212e-06   0.224    0.823    \n## z.diff.lag 1.897e-02  1.985e-03   9.553   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.832 on 253595 degrees of freedom\n## Multiple R-squared:  0.00036,    Adjusted R-squared:  0.0003521 \n## F-statistic: 45.66 on 2 and 253595 DF,  p-value: < 2.2e-16\n## \n## \n## Value of test-statistic is: 0.2241 \n## \n## Critical values for test statistics: \n##       1pct  5pct 10pct\n## tau1 -2.58 -1.95 -1.62"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"the-elliottrothenbergstock-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.2.0.2 The Elliott–Rothenberg–Stock Test:","text":"Elliott–Rothenberg–Stock Test two subtest: P-test takes error term’s serial correlation account, DF-GLS test can applied detrended data without intercept.H0: null hypothes unit root present autoregressive time series model.H1: unit root present autoregressive time series model.urca method: ur.ers: Elliott, Rothenberg & Stock Unit Root TestArguments:  y:  Vector tested unit root.  type:  Test type, either “DF-GLS” (default), “P-test”.P-test?p-value approach hypothesis testing uses calculated probability determine whether evidence reject null hypothesis.DF-GLS?ADF-GLS test (DF-GLS test) test unit root economic time series sample. locally de-trends (de-means) data series efficiently estimate deterministic parameters series, use transformed data perform usual ADF unit root test.  model:  deterministic model used detrending.detrend involves removing effects trend data set show differences values trend; allows cyclical patterns identified. Detrending can done using regression analysis statistical techniques. Detrending shows different aspect time series data removing deterministic stochastic trends.  lag.max:  maximum numbers lags used testing decent lag truncation “P-test” (BIC used), maximum number lagged differences included test regression “DF-GLS”.Usage  ur.ers(y, type = c(“DF-GLS”, “P-test”), model = c(“constant”, “trend”), lag.max = 4)reject null hypothesis test statistics greater 5% confidence value.ExampleIntepretationTest-statistic : 6.3283, Critical values P-test 1.99, 3.26, 4.48, 1%, 5%, 10%, can reject null hypothes. means eth nonstationary.","code":"\nurtest = ur.ers(eth$close, type=\"P-test\", model=\"const\", lag.max=6)\nsummary(urtest)## \n## ############################################### \n## # Elliot, Rothenberg and Stock Unit Root Test # \n## ############################################### \n## \n## Test of type P-test \n## detrending of series with intercept \n## \n## Value of test-statistic is: 6.3283 \n## \n## Critical values of P-test are:\n##                 1pct 5pct 10pct\n## critical values 1.99 3.26  4.48"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"the-schmidtphillips-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.2.0.3 The Schmidt–Phillips Test:","text":"Schmidt–Phillips Test includes coefficients deterministic variables null alternate hypotheses. Subtypes rho-test tau-test.H0: null hypothes observable time series stationary around deterministic trend.H1: deterministic trend time series unit rootA process Y said trend-stationary \\[Y(t) = f(t) + e_{t}\\]\nf function R R. \\(e_{t}\\) stationary process.Adjusted R-squared: 0.9999 means eth time series fit trend model.p-value y.lagged < 0.01, can reject null hypothesis. means eth observable time series nonstationary around deterministic trend.urca method: ur.sp: Schmidt–Phillips TestArguments:  y:  Vector tested unit root.  type:  Test type, either ‘tau’ ‘rho’ test.tau test:Kendall’s Tau: usually smaller values Spearman’s rho correlation. Calculations based concordant discordant pairs. Insensitive error. P values accurate smaller sample sizesrho test:Spearman’s rho usually larger values Kendall’s Tau. Calculations based deviations. Much sensitive error discrepancies data.  pol.deg:  Degree polynomial test regression.  signif:  Significance level critical value test statistic.Usage  ur.sp(y, type = c(“tau”, “rho”), pol.deg = c(1, 2, 3, 4),\nsignif = c(0.01, 0.05, 0.1))ExampleInterpretation\nt-test, test-statistic : -1.562, Critical value significance level 0.05 : -3.02. Thus, reject null hypothesis.","code":"\nurtest = ur.sp(eth$close, type=\"tau\", pol.deg=1, signif=0.05)\nsummary(urtest)## \n## ################################### \n## # Schmidt-Phillips Unit Root Test # \n## ################################### \n## \n## \n## Call:\n## lm(formula = sp.data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -176.877   -2.054   -0.018    2.018  128.146 \n## \n## Coefficients:\n##              Estimate Std. Error   t value Pr(>|t|)    \n## (Intercept) 6.962e-02  4.456e-02     1.562    0.118    \n## y.lagged    1.000e+00  1.640e-05 60984.887   <2e-16 ***\n## trend.exp1  2.150e-07  1.442e-07     1.491    0.136    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.833 on 253595 degrees of freedom\n## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999 \n## F-statistic: 2.25e+09 on 2 and 253595 DF,  p-value: < 2.2e-16\n## \n## \n## Value of test-statistic is: -1.562 \n## Critical value for a significance level of 0.05 \n## is: -3.02"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"the-zivot-andrews-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.2.0.4 The Zivot-Andrews test:","text":"Zivot-Andrews test allows break unknown point intercept linear trend.H0: null hypothesis unit root process drift excludes exogenous structural changeH1: time series stationary excludes exogenous structural change\nurca method: ur.za: Zivot & Andrews Unit Root TestArguments:  y:  Vector tested unit root.  model:  Specification potential break occured either intercept, linear trend .  lag:  highest number lagged endogenous differenced variables included test regression.Usage  ur.za(y, model = c(“intercept”, “trend”, “”), lag=NULL)Examplex, sufficient evidence reject null unit root unit root single break.Intepretation:test said Potential break point 116 position. break position place series got structural breaks.Since p-values intercept y.l1 < 0.05 test-statistic -5.6 less Critical values P-test -4.8. can can reject null hypothes 95 percentage confidence. means eth stationary drift break.","code":"\nurtest = ur.za(eth$close[1:1000], model=\"intercept\", lag=2)\nsummary(urtest)## \n## ################################ \n## # Zivot-Andrews Unit Root Test # \n## ################################ \n## \n## \n## Call:\n## lm(formula = testmat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.9326  -1.4341  -0.0814   1.4239  26.2689 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  1.114e+02  1.990e+01   5.598 2.80e-08 ***\n## y.l1         9.597e-01  7.193e-03 133.431  < 2e-16 ***\n## trend        1.081e-03  4.467e-04   2.421   0.0156 *  \n## y.dl1       -2.652e-02  3.124e-02  -0.849   0.3961    \n## y.dl2       -1.065e-02  3.123e-02  -0.341   0.7331    \n## du           2.970e+00  5.658e-01   5.250 1.86e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.733 on 991 degrees of freedom\n##   (3 observations deleted due to missingness)\n## Multiple R-squared:  0.9918, Adjusted R-squared:  0.9918 \n## F-statistic: 2.397e+04 on 5 and 991 DF,  p-value: < 2.2e-16\n## \n## \n## Teststatistic: -5.6 \n## Critical values: 0.01= -5.34 0.05= -4.8 0.1= -4.58 \n## \n## Potential break point at position: 116"},{"path":"urca-unit-root-test-and-cointegration-test.html","id":"the-johansen-procedure-test","chapter":"89 Urca: Unit Root Test and Cointegration Test","heading":"89.2.0.5 The Johansen-Procedure test:","text":"Johansen-Procedure test procedure testing cointegration several time series.urca method: ca.jo: Unit Root Cointegration Test Time Series DataArguments:  x:  Data frame/matrix tested. Given general VAR model, \\[X_t = \\Pi_1 X_{t-1} + \\dots + \\Pi_k X_{t-k} + \\mu + {\\Phi D}_t + \\varepsilon_t , \\quad (t = 1, \\dots, T),\\]  type:  type test conducted, either “eigen”(eigenvalue) “trace”.  ecdet:  Character, ‘none’ intercept cointegration, ‘const’ constant term cointegration ‘trend’ trend variable cointegration.  K:  lag order series (levels) Vector autoregression(VAR).  spec:  Determines specification vector error correction model(VECM).   spec=“longrun”, VECM estimated :\n\\[\\Delta X_t = \\Gamma_1 \\Delta X_{t-1} + \\dots + \\Gamma_{p-1} \\Delta X_{t-p+1} + {\\Pi X}_{t-p} + \\mu + {\\Phi D}_t + \\varepsilon_t, \\quad (t = 1, \\dots , T),\\]\n   \\[\\Gamma_i = \\Pi_1 + \\dots + \\Pi_i - , \\quad (= 1, \\dots , p-1),\\]\n\\[\\Pi = \\Pi_1 + \\dots + \\Pi_p - \\]   \\(\\Gamma_i\\) matrices contain cumulative long-run impacts.   Else spec=“transitory”, VECM estimated :\n\\[\\Delta X_t = \\Gamma_1 \\Delta X_{t-1} + \\dots + \\Gamma_{p-1} \\Delta X_{t-p+1} + {\\Pi X}_{t-1} + \\mu + {\\Phi D}_t + \\varepsilon_t\\]\n   \\[\\Gamma_i = \\Pi_{+1} + \\dots + \\Pi_p - , \\quad(= 1, \\dots , p-1),\\]\\[\\Pi = \\Pi_1 + \\dots + \\Pi_p - .\\]\n   \\(\\Pi\\) matrix longrun VECM.   inferences drawn \\(\\Pi\\) explanatory power , \\(\\Gamma_i\\) matrices longer terms measuring transitory effects.  season:  seasonal dummies included, data frequency must set accordingly, .e ‘4’ quarterly data. “season” NULL, centered seasonal dummy variables included.  dumvar:  dummy variables included, matrix row dimension equal x can provided. “dumvar” NULL, matrix dummy variables included VECM. Furthermore, number rows matrix containing dummy variables must equal row number xNote: Critical values reported systems less 11 variables taken Osterwald-Lenum.Usage  ca.jo(x, type = c(“eigen”, “trace”), ecdet = c(“none”, “const”, “trend”), K = 2,\nspec=c(“longrun”, “transitory”), season = NULL, dumvar = NULL)  denote \\(r\\) rank matrix \\(\\Pi\\) ’s upper bounded number time series vectors test(Denoted \\(n\\)), \\(r = 0\\) represents test presence cointegration. sequentially tests whether \\(r \\leq k\\), \\(k \\[0, n-1]\\). normally reject null hypothesis test statistics larger 5% confidence level estimate, best estimate rank represents number time series linear combinations requires form stationary series. can use eigenvector corresponds largest eigenvalue compute linear combinations w.r.t \\(r\\).ExampleWe first extract target time series stock vectors 2 datasetsWe first visualize 2 time series data:Now modelling:InterpretationThere 3 generated eigenvalues largest one 1.03e-4. next section, 2 hypotheses \\(r\\leq 1\\) \\(r = 0\\), \\(r\\) rank matrix \\(\\Pi\\) ’s upper bounded number time series vectors test(case \\(r \\leq 2\\)). column 1,2,3,4 contain test statistics, 10%, 5% 1% confidence level values respectively.test statistics 31.34 greater 30.45, 1% confidence level value. Hence sufficient evidence reject null hypothesis cointegration. since \\(r \\leq 1\\) 10% confidence level value 10.49, higher test statistics, don’t sufficient evidence reject \\(r \\leq 1\\). Thus test tells us need one two time series vectors form stationary series. eigenvector largest eigenvalue column y.12 matrix section 3, need one y.12 x.12 form stationary series. plot 2 series weighted corresponding eigenvalues .References:Bernhard Pfaff. Package ‘urca’. September 6, 2016, https://cran.r-project.org/web/packages/urca/urca.pdfur.sp: Schmidt & Phillips Unit Root Test.(2020) https://rdrr.io/cran/urca/YUGESH VERMA. Dickey-Fuller Test Time-Series Analysis. Aug 18, 2021, https://analyticsindiamag.com/complete-guide--dickey-fuller-test--time-series-analysis/aptech, Guide Conducting Cointegration Tests, JANUARY 28, 2020 https://www.aptech.com/blog/-guide--conducting-cointegration-tests/Peter Schmidt, Peter C.B. Phillips, Cowles Foundation Research Economics Yale University October, 1989\nhttps://cowles.yale.edu/sites/default/files/files/pub/d09/d0933.pdfSchmidt, P. Phillips, P.C.B. (1992), LM Test Unit Root Presence Deterministic Trends, Oxford Bulletin Economics Statistics, 54(3), 257–287.Zivot, E. Andrews, Donald W.K. (1992), Evidence Great Crash, Oil-Price Shock, Unit-Root Hypothesis, Journal Business & Economic Statistics, 10(3), 251–270.Johansen test\nhttps://en.wikipedia.org/wiki/Johansen_test","code":"\nxrp <- read.csv(url(\"https://raw.githubusercontent.com/yzh9810/edav_cc/main/xrp-usdt.csv\"))\neth <- read.csv(url(\"https://raw.githubusercontent.com/yzh9810/edav_cc/main/eth-usdt.csv\"))\nxrp$time = as.POSIXlt(xrp$time)\neth$time = as.POSIXlt(eth$time)\nplot(xrp$time, xrp$close, type=\"l\", xlab = \"Time\", ylab = \"close\", col=\"red\", main = \"Stock timeseries\", ylim = c(0,10))\n#apply logorithm to eth data since the magnitudes are significantly higher.\nlines(eth$time, log(eth$close), col=\"blue\")\nstock = data.frame(y = xrp$close, x = log(eth$close))\ncoint = ca.jo(stock, type=\"trace\", ecdet=\"trend\", K=2, spec=\"longrun\")\nsummary(coint)## \n## ###################### \n## # Johansen-Procedure # \n## ###################### \n## \n## Test type: trace statistic , with linear trend in cointegration \n## \n## Eigenvalues (lambda):\n## [1]  1.033152e-04  2.027439e-05 -6.352747e-21\n## \n## Values of teststatistic and critical values of test:\n## \n##           test 10pct  5pct  1pct\n## r <= 1 |  5.14 10.49 12.25 16.26\n## r = 0  | 31.34 22.76 25.32 30.45\n## \n## Eigenvectors, normalised to first column:\n## (These are the cointegration relations)\n## \n##                   y.l2          x.l2     trend.l2\n## y.l2      1.000000e+00  1.000000e+00 1.000000e+00\n## x.l2     -1.210520e+00 -8.299038e+00 1.805231e-01\n## trend.l2  1.438052e-06  1.884477e-05 9.576652e-06\n## \n## Weights W:\n## (This is the loading matrix)\n## \n##              y.l2         x.l2      trend.l2\n## y.d -1.237713e-04 5.093904e-06 -1.371860e-20\n## x.d  3.424685e-05 4.744542e-06 -1.730493e-21\nplot(xrp$time, stock$y, type=\"l\", col = \"red\", xlab = \"Time\", ylab = \"close\", ylim = c(-10,2), main = \"Stationary series\")\nlines(xrp$time, -1.210520*stock$x, col = \"blue\")"},{"path":"comparing-excel-chart-making-with-rs.html","id":"comparing-excel-chart-making-with-rs","chapter":"90 Comparing Excel Chart Making with R’s","heading":"90 Comparing Excel Chart Making with R’s","text":"Charlie SturrFor community contribution project, revisited old homework assignment Seattle pets. working consulting 4 years, spent way many hours staring Excel spreadsheets, wanted see like try recreate homework assignment using Excel. can see went medium post : https://medium.com/@charlie.sturr/differences--r--excel--look--seattle-pets-data-5c8ed5fdeebf","code":""},{"path":"joining-and-reshaping-in-r.html","id":"joining-and-reshaping-in-r","chapter":"91 Joining and reshaping in R","heading":"91 Joining and reshaping in R","text":"Yisi Liu","code":"\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(forcats)"},{"path":"joining-and-reshaping-in-r.html","id":"motivation-7","chapter":"91 Joining and reshaping in R","heading":"91.1 Motivation","text":"majority analyses done assignments lectures far based single data set. real world, however, raw data may saved different files tables, may always meta data set containing information. example, want summary table showing many students department, department information student information kept separate. begin analysis, need combine two data sets single one , necessary, change preferred format.previous intern experience analytics engineer, merging transforming source data desired table crucial step. performed data wrangling ETL (extract-transform-loading) pipelines SQL back . working assignments class, surprisingly discovered comparable functions within dplyr tidyr, explain tutorial.","code":""},{"path":"joining-and-reshaping-in-r.html","id":"content","chapter":"91 Joining and reshaping in R","heading":"91.2 Content","text":"Joining / Merging data sets\nbind_rows, bind_cols\ninner_join, full_join, left_join, right_join\nsemi_join, anti_join\nbind_rows, bind_colsinner_join, full_join, left_join, right_joinsemi_join, anti_joinReshaping data\npivot_longer\npivot_wider\npivot_longerpivot_wider","code":""},{"path":"joining-and-reshaping-in-r.html","id":"combining-tables","chapter":"91 Joining and reshaping in R","heading":"91.3 Combining Tables","text":"","code":""},{"path":"joining-and-reshaping-in-r.html","id":"setup","chapter":"91 Joining and reshaping in R","heading":"91.3.1 Setup","text":"use student table walk tutorial today. student includes information student_id, student_name, department_id far.","code":"\nset.seed(111)\n\nstudent <- data.frame('student_id' = sample(100000:999999, 4, replace=FALSE),\n                      'student_name' = c('James','Alice','Amy','Roger'),\n                      'department_id' =  c(1,1,3,4))\ngrid.arrange(tableGrob(student), top = 'student table')"},{"path":"joining-and-reshaping-in-r.html","id":"types-of-bind","chapter":"91 Joining and reshaping in R","heading":"91.3.2 Types of Bind","text":"bind_rows()beginning semester, two new students Troy Wendy transferred school. Table new_student records student id, name, department id (new students’ majors remain undeclared NA ).convenience future analyses, want add new students’ information existing student table. append rows new_students onto student, perform bind_rows() function dplyr.bind_cols()semester, suppose first half students student table assigned take STAT 5701 second half take STAT 5702. table course records course id course name student takes, student name.interested questions like class James takes Roger Amy take class year, want include two columns student table.add additional column, use bind_columns() . Now able tell James takes STAT 5701, Roger Amy taking class semester.Note: rbind() cbind() function base R perform similarly bind_rows() bind_cols(). can find information rbind() cbind() trying ?rbind ?cbind.","code":"\nset.seed(123)\nnew_student = data.frame('student_id' = sample(100000:999999, 2, replace=FALSE),\n                        'student_name' = c('Troy','Wendy'),\n                        'department_id' =  NA)\n\ngrid.arrange(tableGrob(new_student), top = 'new_student table')\nstudent <- bind_rows(student, new_student)\ngrid.arrange(tableGrob(student), top = 'bind_rows: student & new_student')\ncourse = data.frame(\n        'course_id' = c(rep('STAT 5701',nrow(student)/2),rep('STAT 5702',nrow(student)/2)),\n        'course_name' = c(rep('Probability',nrow(student)/2),rep('Data Visualization',nrow(student)/2)))\ngrid.arrange(tableGrob(course), top = 'course table')\nbind_c = bind_cols(student, course) \n\ngrid.arrange(tableGrob(bind_c) , top = 'bind_rows: student & course')"},{"path":"joining-and-reshaping-in-r.html","id":"mutating-joins","chapter":"91 Joining and reshaping in R","heading":"91.3.3 Mutating Joins","text":"shown bind_cols() append columns table. However, less likely table number rows primary table always exist. Consider case student table 10,000 rows. Another table 10,000 rows, indicating student’s major, take lot space. common way store relational data follows:way, department table store modest amount data. However, new difficulty link data student table, want know students belong departments.KeysWe require common variable (combination variables) uniquely identifies observation least one tables identifies way across tables appropriately align observations . variable commonly referred ‘key’. department id key shared student department case.accommodate many ways tables arranged, require multiple distinct forms joins. summary table. https://dplyr.tidyverse.org/reference/mutate-joins.htmlNow, let’s perform different joins merge tables.inner_join()full_join()left_join()right_join()Multiple keysMultiple keys may required circumstances uniquely identify observation. data sets , example, student two separate observations, therefore need connect student_id course_id.","code":"\ndepartment <- data.frame('department_id' = c(1,2,3,4,5),\n                         'department_name' = c('Math','Computer Science', \n                                               'Philosophy','Linguistics',\n                                               'Economics'))\n\ngrid.arrange(tableGrob(student),tableGrob(department), nrow=1, top = 'student table, department table')\njoin_inner <- inner_join(student, department, by = 'department_id')\ngrid.arrange(tableGrob(join_inner), top = 'inner join: student & department')\njoin_full <- full_join(student, department, by = 'department_id')\ngrid.arrange(tableGrob(join_full), top = 'full join: student & department')\njoin_left <- left_join(student, department, by = 'department_id')\ngrid.arrange(tableGrob(join_left), top = 'left join: student & department')\njoin_right <- right_join(student, department, by = 'department_id')\ngrid.arrange(tableGrob(join_right), top = 'right join: student & department')\nstudent2 <- data.frame('student_id' = rep(student$student_id, 2),\n                       'student_name' = rep(student$student_name, 2),\n                       'course_id' = c(rep('STAT 5701',nrow(student)),rep('STAT 5702',nrow(student)))) %>%\n            arrange(student_id)\ngrades <- data.frame('course_id' = c(rep('STAT 5701',nrow(student)),rep('STAT 5702',nrow(student))),\n                     'student_id' = rep(student$student_id, 2),\n                     'final_grade' = c('A','A-','B','B+','A','B+','B','A-','A','A','A','A'))%>%\n            arrange(course_id, student_id)\n\ngrid.arrange(tableGrob(student2),tableGrob(grades), nrow=1, top = 'student2 table, grades table')\nstudent_with_grades = inner_join(student2, grades, by = c('student_id','course_id'))\ngrid.arrange(tableGrob(student_with_grades), top = 'inner join: student2 & grades')"},{"path":"joining-and-reshaping-in-r.html","id":"filtering-joins","chapter":"91 Joining and reshaping in R","heading":"91.3.4 Filtering joins","text":"addition mutating joins add columns x y, two filtering joins functions within dplyr. Filtering joins filter rows x based presence absence matches y. https://dplyr.tidyverse.org/reference/filter-joins.htmlRecall department_id key student table department table, department_id= {1,3,4} occurs tables.semi_join()anti_join()","code":"\n#only keep rows in student table\njoin_semi1 <- semi_join(student, department, by = 'department_id')\ngrid.arrange(tableGrob(join_semi1), top = 'semi join: student & department')\n#only keep rows in department table\njoin_semi2 <- semi_join(department,student, by = 'department_id')\ngrid.arrange(tableGrob(join_semi2), top = 'semi join: department & student')\n#only keep rows in student table\njoin_anti1 <- anti_join(student, department, by = 'department_id')\ngrid.arrange(tableGrob(join_anti1), top = 'anti join: student & department')\n#only keep rows in department table\njoin_anti2 <- anti_join(department, student, by = 'department_id')\ngrid.arrange(tableGrob(join_anti2), top = 'anti join: department & student')"},{"path":"joining-and-reshaping-in-r.html","id":"r-vs.-sql","chapter":"91 Joining and reshaping in R","heading":"91.3.5 R vs. SQL","text":"commands joins R SQL included table . can see languages mutating/filtering joins.","code":""},{"path":"joining-and-reshaping-in-r.html","id":"reshaping","chapter":"91 Joining and reshaping in R","heading":"91.4 Reshaping","text":"pivot_wider()Recall table student_with_grades.\ndata makes difficult read compare students’ grades classes. Alternatively, can extract courses’ names divide two columns, containing final grade value. make table pivot wider() function tidyr.pivot_longer()Conversely, given ‘wider’ table student_with_grades_2, want convert format student_with_grades order compare distribution final grades two classes. can using pivor_longer() function reshape table plot side--side bar chart ‘longer’ table.","code":"\nstudent_with_grades_2 = student_with_grades %>% pivot_wider(names_from = course_id, values_from = final_grade)\ngrid.arrange(tableGrob(student_with_grades_2), top = 'pivot_wider: student_with_grades_2')\nstudent_with_grades_3 <- student_with_grades_2 %>% \n                        pivot_longer(cols = c('STAT 5701','STAT 5702'), \n                                     names_to = 'course_id', \n                                     values_to = 'final_grade')\ngrid.arrange(tableGrob(student_with_grades_3), top = 'pivot_longer: student_with_grades_3')\nstudent_with_grades_3 %>% \n  ggplot(aes(fct_relevel(final_grade,'B', after = 3))) + \n  geom_bar() + \n  facet_grid(~course_id) +\n  ggtitle('Distribution of Final Grades in STAT 5701 & STAT 5702') +\n  xlab('Final grades')"},{"path":"joining-and-reshaping-in-r.html","id":"self-evaluation","chapter":"91 Joining and reshaping in R","heading":"91.5 Self-evaluation","text":"starting project, knew mutating joins inner_join() left_join(). discovered filtering joins like semi_join() anti_join() working . filtering joins extremely useful need data x based standards y.SQL, normally use statement create constraint x, seen ‘R vs. SQL’ comparison table included . , believe, intriguing aspect knowing many data-handling tools: can always find approaches perform function.Due time constraints, tutorial covers many ‘merging’ ‘reshaping’ functions familiar . get another chance, go parameters functions, copy, keep, na_matches mutating joins, assist everyone understand better.","code":""},{"path":"joining-and-reshaping-in-r.html","id":"reference-6","chapter":"91 Joining and reshaping in R","heading":"91.6 Reference","text":"R4DS Chapter 13: https://r4ds..co.nz/relational-data.html","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"running-sql-queries-on-dataframes","chapter":"92 Running Sql Queries on Dataframes","heading":"92 Running Sql Queries on Dataframes","text":"Chao Pang Krishna Kalluri","code":"\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(sparklyr)"},{"path":"running-sql-queries-on-dataframes.html","id":"motivation-8","chapter":"92 Running Sql Queries on Dataframes","heading":"92.1 Motivation","text":"goal tutorial allow students capability write SQL queries dplyr dataframes directly rather use dplyr functions assignments. Despite convenience dplyr offers, actually prefer beloved classic SQL queries ’s using daily work. believe lot people aren’t familiar R (specifically tidyverse) feel way prefer SQL beginning adventure R “universe”.’s important point writing SQL complete replacement dplyr, rather alternative option table students just feel one limit particular data manipulation tool. want acknolwedge dplyr well-thought-library, uses principles SQL queries, however, Googling right syntax doesn’t seem straightforward times (unlike pandas Python). motivation project stemmed frustrations able find right functions/syntax manipulate data ease. Although ’ve learned use dplyr three assignments, nice provide alternative future generations students.","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"sql-use-cases","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2 SQL use cases","text":"","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"get-started","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.1 Get started","text":"knitr underlying engine knitting R markdown files document, provides support different programming languages. particular, knitr provides SQL engine allows write SQL queries code chunk, see official documentation knitr sql engine. However, engine requires connection database, can query tables materialized database. leverage engine, create temp database import dataframes interest tables. dbplyer library provides convenience function get -memory database engine (sqllite) additional functions importing dataframes tables. familiar SQL queries want learn , please visit funny tutorial learn---write-sql--r","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"load-the-table-into-the-dbplyr-in-mem-database","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.1.1 Load the table into the dbplyr in-mem database","text":"Wow, actually super easy , took two lines code import table. However, took us long time find function even existed, hopefully won’t go pain future 😊!","code":"\nconnection = dbplyr::src_memdb()$con\ndplyr::copy_to(connection, iris)"},{"path":"running-sql-queries-on-dataframes.html","id":"run-your-first-query","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.1.2 Run your first query","text":"’ve imported table, get database connection object dbplyr directly, now just need pass code chunk chunk options. three options specified code chunk,sql: key word indicates sql code chunksql: key word indicates sql code chunkconnection: sqllite database connection created earlier (database connection work though!)connection: sqllite database connection created earlier (database connection work though!)","code":"--chunk options: {sql, connection=connection}\nSELECT\n  *\nFROM iris"},{"path":"running-sql-queries-on-dataframes.html","id":"group-by","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.1.3 Group by","text":"Calculate average petal length, petal width species.","code":"--chunk options: {sql, connection=connection}\nSELECT\n  species,\n  AVG(`Petal.Length`) AS avg_petal_length,\n  AVG(`Petal.Width`) AS avg_petal_width\nFROM iris\nGROUP BY species "},{"path":"running-sql-queries-on-dataframes.html","id":"saving-results-as-dataframes","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.1.4 Saving results as dataframes","text":"","code":"--chunk options: {sql, connection=connection, output.var=\"result_df\"}\nSELECT\n  Species AS specs,\n  `Petal.Width` AS petal_width,\n  `Petal.Length` AS ptal_length\nFROM iris\nWHERE species = 'setosa'\nresult_df %>% rmarkdown::paged_table()"},{"path":"running-sql-queries-on-dataframes.html","id":"complex-use-cases","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.2 Complex Use Cases","text":"","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"popular-pet-names","chapter":"92 Running Sql Queries on Dataframes","heading":"92.2.2.0.1 Popular pet names","text":"tutorial, going use pet name dataset called seattlepets demonstration.question trying figure — 20 popular cat dog names?Let’s try answer question using dplyr way.Now let’s try write SQL query thing.Voila, tables generated two different approaches look exactly , cool ! can choose two depending feel!Let’s look slightly complicated example, want know 20 popular names dogs cats combined. , need structure data long form generating Cleveland plot. using dplyr way first.cool thing SQL engine returns result regular dplyr dataframe, allows us mix match SQL queries dplyr syntax. demonstration, calculate total count dog cat names combined SQL query, leverage pivot_longer function structure data plotting.","code":"\nseattlepets %>% \n  drop_na(animal_name) %>%\n  filter (species %in% c(\"Dog\", \"Cat\")) %>% \n  group_by(species, animal_name) %>%\n  summarise(count=n()) %>%\n  ungroup() %>%\n  group_by(species) %>%\n  arrange(desc(count)) %>%\n  mutate(row_number=row_number()) %>%\n  ungroup() %>%\n  filter(row_number <= 20) %>%\n  rmarkdown::paged_table()\nconnection = dbplyr::src_memdb()$con\ndplyr::copy_to(connection, seattlepets)--chunk options: {sql, connection=connection}\nWITH pets AS\n(\n  SELECT\n    species, \n    animal_name,\n    COUNT(*) AS count\n  FROM seattlepets\n  WHERE animal_name IS NOT NULL\n    AND species IN (\"Cat\", \"Dog\")\n  GROUP BY species, animal_name\n),\nanimal_name_row_number AS\n(\n  SELECT\n    species,\n    animal_name,\n    count,\n    ROW_NUMBER() OVER(PARTITION BY species ORDER BY count DESC) AS row_number\n  FROM pets\n)\n\nSELECT\n  *\nFROM animal_name_row_number\nWHERE row_number <= 20\nORDER BY species DESC, count DESC\nseattlepets %>%\n  drop_na(animal_name) %>%\n  filter (species %in% c(\"Dog\", \"Cat\")) %>%\n  group_by(animal_name, species) %>%\n  summarise(count=n()) %>%\n  ungroup() %>%\n  pivot_wider(names_from=species, values_from=count) %>%\n  mutate(total = ifelse(is.na(Dog), 0, Dog) + ifelse(is.na(Cat), 0, Cat)) %>%\n  arrange(desc(total), animal_name) %>%\n  mutate(row_number=row_number()) %>%\n  filter(row_number <= 20) %>%\n  mutate(total_count = total) %>%\n  pivot_longer(cols=c(Dog, Cat, total), names_to=\"group_type\", values_to=\"count\") %>%\n\n ggplot() + \n    geom_point(aes(fct_reorder(animal_name, total_count), count, group=group_type, color=group_type)) +\n    coord_flip() + \n    xlab(\"Animal Name\") + \n    ylab(\"Count\")--chunk options: {sql, connection=connection, output.var=\"total_pet_count\"}\nWITH pet_counts AS\n(\n  SELECT\n    animal_name, \n    species,\n    COUNT(*) AS count\n  FROM seattlepets\n  WHERE animal_name IS NOT NULL\n    AND species IN (\"Dog\", \"Cat\")\n  GROUP BY animal_name, species\n)\n\nSELECT\n  animal_name,\n  Cat,\n  Dog,\n  Cat + Dog AS total,\n  ROW_NUMBER() OVER(ORDER BY Cat + Dog DESC) AS row_number\nFROM\n(\n  SELECT\n    animal_name,\n    IFNULL(SUM(CASE WHEN species = \"Cat\" THEN count END), 0) AS Cat,\n    IFNULL(SUM(CASE WHEN species = \"Dog\" THEN count END), 0) AS Dog\n  FROM pet_counts\n  GROUP BY animal_name\n) p\nORDER BY Cat + Dog DESC\ntotal_pet_count %>%\n  filter(row_number <= 20) %>%\n  mutate(total_count = total) %>%\n  pivot_longer(cols=c(Dog, Cat, total), names_to=\"group_type\", values_to=\"count\") %>%\n\n ggplot() + \n    geom_point(aes(fct_reorder(animal_name, total_count), count, group=group_type, color=group_type)) +\n    coord_flip() + \n    xlab(\"Animal Name\") + \n    ylab(\"Count\")"},{"path":"running-sql-queries-on-dataframes.html","id":"lessons-learned","chapter":"92 Running Sql Queries on Dataframes","heading":"92.3 Lessons learned","text":"Writing SQL queries R markdown complicated thought. Although Knitr already provides support SQL engine, documentation provide enough information use case, want write SQL queries dataframes. took us time realize use -memory SqlLite database import dataframes tables, allows us use SQL engine directly. feel missing gap documentation, reason created tutorial help others get speed.general, dplyr functions can reproduced SQL queries, vice versa. One perform data transformation using either dplyr functions SQL queries combination . However, certain things definitely easier depending background preference (subjective opinions). example, feel joins aliasing columns much easier SQL queries, whereas pivot operations away easier straightforward dplyr.want re-iterate writing SQL replacement dplyr functions, rather alternative approach one take. addition, acknowledge certain drawbacks SQL approach.SqlLite doesn’t support functions including pivot, unpivot, full outer join, right join. (functions offered SQL dialects though e.g. MSSQL)intermediate results generated SQL chunk, import tables -mem database can use subsequent SQL code chunks.SQL chunk doesn’t produce paged table . One save result dataframe view R chunk.","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"future-work","chapter":"92 Running Sql Queries on Dataframes","heading":"92.4 Future work","text":"Given limited time, happy ’ve achieved, however, things like improve upon near future.","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"missing-pagination-for-sql-engine-query-output","chapter":"92 Running Sql Queries on Dataframes","heading":"92.4.1 Missing pagination for sql engine query output","text":"noticed table generated sql engine doesn’t come pager, unlike formatiting pagination offered dplyr default. looked options offered sql engine, doesn’t seem support feature. planning expand table formatter existing sql engine support pagination.","code":""},{"path":"running-sql-queries-on-dataframes.html","id":"sql-engine-for-dplyr-dataframes","chapter":"92 Running Sql Queries on Dataframes","heading":"92.4.2 sql engine for dplyr dataframes","text":"Despite success, still feel importing tables -mem database passing connection along tedious unncessary, idea world, just reference dataframes environment directly SQL queries. explored idea briefly creating new knitr engine named eng_sql_local based existing eng_sql implementation. idea summarized ,use library called queryparser extract table references querywe get corresponding dataframes name knit_global environmentwe import dataframes dbplyr -memory databasewe invoke existing eng_sql run queryHere code snippet, also code fork.tested successfully local environment (one fork clone repo, build knitr locally test new code block). However, library queryparser doesn’t handle complex queries Common Table Expression (CTE), implement queryparser using regular expression identify table names query. Another problem found custom sql engine doesn’t support syntax highlighting, existing sql engine , try fix issue future.","code":"\n# sql engine using dbplyr in-memory db\neng_sql_local = function(options) {\n\n  query = one_string(options$code)\n  table_references <- queryparser::parse_query(query)$from\n  options$connection <- dbplyr::src_memdb()$con\n  overwrite <-ifelse(is.null(options$overwrite), FALSE, options$overwrite)\n\n  output <- tryCatch({\n    for (i in range(1, length(table_references)))\n    {\n      table_name <- as.character(table_references[[i]])\n      dataframe = get(table_name, envir = knit_global())\n      if ((!DBI::dbExistsTable(options$connection, table_name)) | overwrite){\n        DBI::dbWriteTable(dbplyr::src_memdb()$con, table_name, dataframe)\n      }\n    }\n  },error = function(e) {\n    e$message <-\n      paste(e$message,\n            paste(\"The table \", table_name, \" doesn't exist in the global environment.\", sep=\"\"),\n            \"Using the following command to set the dataframe in the global environment:\",\n            \"env <- knitr::knit_global()\",\n            paste(\"env$`\", table_name, \"` <- `\", table_name, \"`\", sep=\"\"),\n            sep = \"\\n\")\n    e\n  })\n\n  if (inherits(output, \"error\"))\n    return(engine_output(options, query, one_string(output)))\n\n  return(eng_sql(options))\n}"},{"path":"tidyverse.html","id":"tidyverse","chapter":"93 TidyVerse","heading":"93 TidyVerse","text":"Zhisen CaiSometimes may forget way use Tidyverse. Therefore, create help. People can finish basic part tidyverse using . separate 4 parts, manipulation two datasets, manipulation rows, manipulation columns group_by. examples function, people can know use function function works.Magrittr use pipeline convey data. (%>%) use dot represent thing convey used first parameter.","code":"\nlibrary(tidyverse)\n# we can omit '.'\nc(1, 3, 4, 5, NA) %>% mean(., na.rm=TRUE)## [1] 3.25\nc(1, 3, 4, 5, NA) %>% mean(na.rm = TRUE)  ## [1] 3.25\n# we can not omit '.' in the second parameter\nc(1, 3, 4, 5) %>% plot(., main=paste(., collapse=\", \"))\nc(1, 3, 4, 5) %>% plot(main=paste(., collapse=\", \"))"},{"path":"tidyverse.html","id":"connection-bewtween-two-datasets","chapter":"93 TidyVerse","heading":"93.0.1 Connection Bewtween two datasets","text":"Efficiently bind multiple data frames row column\nbind_rows() bind_cols() return type first input, either data frame, tbl_df, grouped_df\ncode: bind_rows(…, .id = NULL) bind_cols(…)","code":"\nbind_rows(\n+   sample_n(iris, 10),\n+   sample_n(iris, 10),\n+   sample_n(iris, 10)) %>% \n  glimpse()## Rows: 30\n## Columns: 5\n## $ Sepal.Length <dbl> 5.1, 6.0, 4.4, 5.7, 6.3, 5.1, 4.4, 5.7, 4.8, 7.2, 4.9, 5.…\n## $ Sepal.Width  <dbl> 3.5, 2.7, 3.2, 3.8, 2.7, 3.8, 2.9, 2.5, 3.1, 3.6, 2.4, 2.…\n## $ Petal.Length <dbl> 1.4, 5.1, 1.3, 1.7, 4.9, 1.5, 1.4, 5.0, 1.6, 6.1, 3.3, 4.…\n## $ Petal.Width  <dbl> 0.3, 1.6, 0.2, 0.3, 1.8, 0.3, 0.2, 2.0, 0.2, 2.5, 1.0, 1.…\n## $ Species      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\nbind_cols(\n  sample_n(iris, 10),\n  sample_n(iris, 10),\n  sample_n(iris, 10)\n) %>% glimpse()## Rows: 10\n## Columns: 15\n## $ Sepal.Length...1  <dbl> 6.7, 4.4, 6.1, 5.2, 4.6, 4.8, 7.1, 5.7, 6.4, 5.2\n## $ Sepal.Width...2   <dbl> 2.5, 3.0, 2.9, 2.7, 3.1, 3.4, 3.0, 2.9, 3.1, 4.1\n## $ Petal.Length...3  <dbl> 5.8, 1.3, 4.7, 3.9, 1.5, 1.6, 5.9, 4.2, 5.5, 1.5\n## $ Petal.Width...4   <dbl> 1.8, 0.2, 1.4, 1.4, 0.2, 0.2, 2.1, 1.3, 1.8, 0.1\n## $ Species...5       <fct> virginica, setosa, versicolor, versicolor, setosa, s…\n## $ Sepal.Length...6  <dbl> 5.5, 6.3, 6.0, 5.0, 5.1, 6.1, 6.1, 6.4, 5.1, 5.7\n## $ Sepal.Width...7   <dbl> 2.6, 3.4, 2.7, 3.4, 3.4, 3.0, 2.8, 3.1, 3.8, 2.6\n## $ Petal.Length...8  <dbl> 4.4, 5.6, 5.1, 1.5, 1.5, 4.6, 4.0, 5.5, 1.9, 3.5\n## $ Petal.Width...9   <dbl> 1.2, 2.4, 1.6, 0.2, 0.2, 1.4, 1.3, 1.8, 0.4, 1.0\n## $ Species...10      <fct> versicolor, virginica, versicolor, setosa, setosa, v…\n## $ Sepal.Length...11 <dbl> 5.7, 6.9, 6.1, 5.8, 6.4, 5.4, 5.4, 5.0, 7.0, 6.7\n## $ Sepal.Width...12  <dbl> 3.0, 3.1, 2.8, 2.6, 2.7, 3.4, 3.0, 3.5, 3.2, 3.3\n## $ Petal.Length...13 <dbl> 4.2, 5.4, 4.0, 4.0, 5.3, 1.5, 4.5, 1.3, 4.7, 5.7\n## $ Petal.Width...14  <dbl> 1.2, 2.1, 1.3, 1.2, 1.9, 0.4, 1.5, 0.3, 1.4, 2.1\n## $ Species...15      <fct> versicolor, virginica, versicolor, versicolor, virgi…"},{"path":"tidyverse.html","id":"combime-two-datasets-by-value-join","chapter":"93 TidyVerse","heading":"93.0.2 combime two datasets by value: join","text":"left_join\nleft join R merge operation two data frames merge returns rows one table (left side) matching rows second table. left join R return values second table already exist first table.\nleft_join(a_tibble, another_tibble, = …)inner join:\ninner_join keyword selects records matching values tables.return unmatched rows. Therefore, see last row shown left_join.\ncode: inner_join(a_tibble, another_tibble, = …)anti join:\nanti join returns rows first table find match second table.semi join\nSemi joins opposite anti joins: anti-anti join, like.\nsemi join returns rows first table can find match second table.\ncode: semi_join(a_tibble, another_tibble, = …)","code":"\n# prepare data \nsuperheroes <- tribble(\n  ~name,      ~alignment,  ~gender,   ~publisher,\n  \"Magneto\",  \"bad\",       \"male\",    \"Marvel\",\n  \"Storm\",    \"good\",      \"female\",  \"Marvel\",\n  \"Mystique\", \"bad\",       \"female\",  \"Marvel\",\n  \"Batman\",   \"good\",      \"male\",    \"DC\",\n  \"Joker\",    \"bad\",       \"male\",    \"DC\",\n  \"Catwoman\", \"bad\",       \"female\",  \"DC\",\n  \"Hellboy\",  \"good\",      \"male\",    \"Dark Horse Comics\"\n)\npublishers <- tribble(\n  ~publisher, ~yr_founded,\n  \"DC\",       1934L,\n  \"Marvel\",   1939L,\n  \"Image\",    1992L\n)\nsuperheroes %>% \n  left_join(publishers, by=\"publisher\")## # A tibble: 7 × 5\n##   name     alignment gender publisher         yr_founded\n##   <chr>    <chr>     <chr>  <chr>                  <int>\n## 1 Magneto  bad       male   Marvel                  1939\n## 2 Storm    good      female Marvel                  1939\n## 3 Mystique bad       female Marvel                  1939\n## 4 Batman   good      male   DC                      1934\n## 5 Joker    bad       male   DC                      1934\n## 6 Catwoman bad       female DC                      1934\n## 7 Hellboy  good      male   Dark Horse Comics         NA\ninner_join(superheroes, publishers, by=\"publisher\")## # A tibble: 6 × 5\n##   name     alignment gender publisher yr_founded\n##   <chr>    <chr>     <chr>  <chr>          <int>\n## 1 Magneto  bad       male   Marvel          1939\n## 2 Storm    good      female Marvel          1939\n## 3 Mystique bad       female Marvel          1939\n## 4 Batman   good      male   DC              1934\n## 5 Joker    bad       male   DC              1934\n## 6 Catwoman bad       female DC              1934\nsuperheroes %>%\n  anti_join(publishers, by=\"publisher\")## # A tibble: 1 × 4\n##   name    alignment gender publisher        \n##   <chr>   <chr>     <chr>  <chr>            \n## 1 Hellboy good      male   Dark Horse Comics\nsuperheroes %>%\n  semi_join(publishers, by=\"publisher\")## # A tibble: 6 × 4\n##   name     alignment gender publisher\n##   <chr>    <chr>     <chr>  <chr>    \n## 1 Magneto  bad       male   Marvel   \n## 2 Storm    good      female Marvel   \n## 3 Mystique bad       female Marvel   \n## 4 Batman   good      male   DC       \n## 5 Joker    bad       male   DC       \n## 6 Catwoman bad       female DC"},{"path":"tidyverse.html","id":"manipulation-on-column","chapter":"93 TidyVerse","heading":"93.0.3 manipulation on column","text":"use select choose column.\ncan use helper function choose column\nstarts_with() : start literal string\nends_with() : end literal string\nmatches(): match regular expression\nnum_range(): part numercial range\neverything(): selects columnsselect_if\ncan use select_if choose column logical condition function return true false.!!!! can use ~ represent anonymous functionWe can select data deleting columns using minus sign.rename column","code":"\ncolnames(iris)## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\niris <- iris %>% as_tibble() \niris %>%\n  select(starts_with(\"Petal\")) %>%\n  head(10)## # A tibble: 10 × 2\n##    Petal.Length Petal.Width\n##           <dbl>       <dbl>\n##  1          1.4         0.2\n##  2          1.4         0.2\n##  3          1.3         0.2\n##  4          1.5         0.2\n##  5          1.4         0.2\n##  6          1.7         0.4\n##  7          1.4         0.3\n##  8          1.5         0.2\n##  9          1.4         0.2\n## 10          1.5         0.1\niris %>%\n  select(ends_with(\"Length\"))%>%\n  head(10)## # A tibble: 10 × 2\n##    Sepal.Length Petal.Length\n##           <dbl>        <dbl>\n##  1          5.1          1.4\n##  2          4.9          1.4\n##  3          4.7          1.3\n##  4          4.6          1.5\n##  5          5            1.4\n##  6          5.4          1.7\n##  7          4.6          1.4\n##  8          5            1.5\n##  9          4.4          1.4\n## 10          4.9          1.5\niris %>%\n  select(contains(\".\")) %>%\n  head(10)## # A tibble: 10 × 4\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1          5.1         3.5          1.4         0.2\n##  2          4.9         3            1.4         0.2\n##  3          4.7         3.2          1.3         0.2\n##  4          4.6         3.1          1.5         0.2\n##  5          5           3.6          1.4         0.2\n##  6          5.4         3.9          1.7         0.4\n##  7          4.6         3.4          1.4         0.3\n##  8          5           3.4          1.5         0.2\n##  9          4.4         2.9          1.4         0.2\n## 10          4.9         3.1          1.5         0.1\nstarwars %>%\n  select_if(is.numeric) %>%\n  head(10)## # A tibble: 10 × 3\n##    height  mass birth_year\n##     <int> <dbl>      <dbl>\n##  1    172    77       19  \n##  2    167    75      112  \n##  3     96    32       33  \n##  4    202   136       41.9\n##  5    150    49       19  \n##  6    178   120       52  \n##  7    165    75       47  \n##  8     97    32       NA  \n##  9    183    84       24  \n## 10    182    77       57\nless_than_500 <- function(x) {\n  sum(x) < 500\n}\niris[1:4] %>%\n  select_if(less_than_500)%>%\n  head(10)## # A tibble: 10 × 2\n##    Sepal.Width Petal.Width\n##          <dbl>       <dbl>\n##  1         3.5         0.2\n##  2         3           0.2\n##  3         3.2         0.2\n##  4         3.1         0.2\n##  5         3.6         0.2\n##  6         3.9         0.4\n##  7         3.4         0.3\n##  8         3.4         0.2\n##  9         2.9         0.2\n## 10         3.1         0.1\niris[1:4] %>%\n  select_if(~ sum(.) < 500)%>%\n  head(10)## # A tibble: 10 × 2\n##    Sepal.Width Petal.Width\n##          <dbl>       <dbl>\n##  1         3.5         0.2\n##  2         3           0.2\n##  3         3.2         0.2\n##  4         3.1         0.2\n##  5         3.6         0.2\n##  6         3.9         0.4\n##  7         3.4         0.3\n##  8         3.4         0.2\n##  9         2.9         0.2\n## 10         3.1         0.1\niris %>%\n  select(-Species, -Petal.Length) %>%\n  head(10)## # A tibble: 10 × 3\n##    Sepal.Length Sepal.Width Petal.Width\n##           <dbl>       <dbl>       <dbl>\n##  1          5.1         3.5         0.2\n##  2          4.9         3           0.2\n##  3          4.7         3.2         0.2\n##  4          4.6         3.1         0.2\n##  5          5           3.6         0.2\n##  6          5.4         3.9         0.4\n##  7          4.6         3.4         0.3\n##  8          5           3.4         0.2\n##  9          4.4         2.9         0.2\n## 10          4.9         3.1         0.1\niris %>%   \n  select(Species, everything(), -ends_with(\"Length\")) %>%\n  head(10)## # A tibble: 10 × 3\n##    Species Sepal.Width Petal.Width\n##    <fct>         <dbl>       <dbl>\n##  1 setosa          3.5         0.2\n##  2 setosa          3           0.2\n##  3 setosa          3.2         0.2\n##  4 setosa          3.1         0.2\n##  5 setosa          3.6         0.2\n##  6 setosa          3.9         0.4\n##  7 setosa          3.4         0.3\n##  8 setosa          3.4         0.2\n##  9 setosa          2.9         0.2\n## 10 setosa          3.1         0.1\niris %>%\n  rename(sep_len=Sepal.Length, sep_wid=Sepal.Width) %>%\n  names()## [1] \"sep_len\"      \"sep_wid\"      \"Petal.Length\" \"Petal.Width\"  \"Species\""},{"path":"tidyverse.html","id":"manipulation-on-row","chapter":"93 TidyVerse","heading":"93.0.4 manipulation on row","text":"Use arrange() reorder rows.using desc() decreasing orderuse distinct() remove duplicated rowsuse drop_na() remove rows including NAor can drop na specific rowsuse filter select rowsuse filter_all() choose rows columnsuse filter_if() choose rows","code":"\n# prepare data\nset.seed(896)\nsw_dup <-\n  starwars %>%\n  select(-(films:starships)) %>%  \n  sample_n(100, replace=TRUE)\nhead(sw_dup,10) ## # A tibble: 10 × 11\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Sly Moo…    178    48 none       pale       white             NA <NA>  <NA>  \n##  2 Mon Mot…    150    NA auburn     fair       blue              48 fema… femin…\n##  3 Finn         NA    NA black      dark       dark              NA male  mascu…\n##  4 Roos Ta…    224    82 none       grey       orange            NA male  mascu…\n##  5 Arvel C…     NA    NA brown      fair       brown             NA male  mascu…\n##  6 BB8          NA    NA none       none       black             NA none  mascu…\n##  7 Saesee …    188    NA none       pale       orange            NA male  mascu…\n##  8 IG-88       200   140 none       metal      red               15 none  mascu…\n##  9 Yarael …    264    NA none       white      yellow            NA male  mascu…\n## 10 Jek Ton…    180   110 brown      fair       blue              NA male  mascu…\n## # … with 2 more variables: homeworld <chr>, species <chr>\nsw_dup %>%\n  arrange(name, gender) %>%\n  head(10)## # A tibble: 10 × 11\n##    name    height  mass hair_color skin_color  eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr> \n##  1 Adi Ga…    184    50 none       dark        blue              NA fema… femin…\n##  2 Arvel …     NA    NA brown      fair        brown             NA male  mascu…\n##  3 Bail P…    191    NA black      tan         brown             67 male  mascu…\n##  4 BB8         NA    NA none       none        black             NA none  mascu…\n##  5 BB8         NA    NA none       none        black             NA none  mascu…\n##  6 Ben Qu…    163    65 none       grey, gree… orange            NA male  mascu…\n##  7 Ben Qu…    163    65 none       grey, gree… orange            NA male  mascu…\n##  8 Beru W…    165    75 brown      light       blue              47 fema… femin…\n##  9 Beru W…    165    75 brown      light       blue              47 fema… femin…\n## 10 Bossk      190   113 none       green       red               53 male  mascu…\n## # … with 2 more variables: homeworld <chr>, species <chr>\nsw_dup %>%\n  arrange(desc(mass)) %>%\n  head(10)## # A tibble: 10 × 11\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n##  1 IG-88       200   140 none       metal      red               15 none  mascu…\n##  2 Tarfful     234   136 brown      brown      blue              NA male  mascu…\n##  3 Bossk       190   113 none       green      red               53 male  mascu…\n##  4 Jek Ton…    180   110 brown      fair       blue              NA male  mascu…\n##  5 Jek Ton…    180   110 brown      fair       blue              NA male  mascu…\n##  6 Jek Ton…    180   110 brown      fair       blue              NA male  mascu…\n##  7 Dexter …    198   102 none       brown      yellow            NA male  mascu…\n##  8 Dexter …    198   102 none       brown      yellow            NA male  mascu…\n##  9 Qui-Gon…    193    89 brown      fair       blue              92 male  mascu…\n## 10 Kit Fis…    196    87 none       green      black             NA male  mascu…\n## # … with 2 more variables: homeworld <chr>, species <chr>\nsw_dup %>%\n  distinct() %>%\n  glimpse() %>%\n  anyDuplicated()  ## Rows: 57\n## Columns: 11\n## $ name       <chr> \"Sly Moore\", \"Mon Mothma\", \"Finn\", \"Roos Tarpals\", \"Arvel C…\n## $ height     <int> 178, 150, NA, 224, NA, NA, 188, 200, 264, 180, 163, 183, 18…\n## $ mass       <dbl> 48, NA, NA, 82, NA, NA, NA, 140, NA, 110, 65, 80, 84, 75, 4…\n## $ hair_color <chr> \"none\", \"auburn\", \"black\", \"none\", \"brown\", \"none\", \"none\",…\n## $ skin_color <chr> \"pale\", \"fair\", \"dark\", \"grey\", \"fair\", \"none\", \"pale\", \"me…\n## $ eye_color  <chr> \"white\", \"blue\", \"dark\", \"orange\", \"brown\", \"black\", \"orang…\n## $ birth_year <dbl> NA, 48, NA, NA, NA, NA, NA, 15, NA, NA, NA, NA, 72, 47, 46,…\n## $ sex        <chr> NA, \"female\", \"male\", \"male\", \"male\", \"none\", \"male\", \"none…\n## $ gender     <chr> NA, \"feminine\", \"masculine\", \"masculine\", \"masculine\", \"mas…\n## $ homeworld  <chr> \"Umbara\", \"Chandrila\", NA, \"Naboo\", NA, NA, \"Iktotch\", NA, …\n## $ species    <chr> NA, \"Human\", \"Human\", \"Gungan\", \"Human\", \"Droid\", \"Iktotchi…## [1] 0\nsw_dup %>%\n  drop_na() %>%\n  glimpse() %>%\n  anyNA()## Rows: 30\n## Columns: 11\n## $ name       <chr> \"Mace Windu\", \"Beru Whitesun lars\", \"Padmé Amidala\", \"Mace …\n## $ height     <int> 188, 165, 165, 188, 175, 175, 165, 198, 196, 198, 188, 175,…\n## $ mass       <dbl> 84.0, 75.0, 45.0, 84.0, 79.0, 80.0, 45.0, 82.0, 66.0, 82.0,…\n## $ hair_color <chr> \"none\", \"brown\", \"brown\", \"none\", \"none\", \"none\", \"brown\", …\n## $ skin_color <chr> \"dark\", \"light\", \"light\", \"dark\", \"light\", \"red\", \"light\", …\n## $ eye_color  <chr> \"brown\", \"blue\", \"brown\", \"brown\", \"blue\", \"yellow\", \"brown…\n## $ birth_year <dbl> 72, 47, 46, 72, 37, 54, 46, 92, 52, 92, 22, 54, 92, 8, 72, …\n## $ sex        <chr> \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"female…\n## $ gender     <chr> \"masculine\", \"feminine\", \"feminine\", \"masculine\", \"masculin…\n## $ homeworld  <chr> \"Haruun Kal\", \"Tatooine\", \"Naboo\", \"Haruun Kal\", \"Bespin\", …\n## $ species    <chr> \"Human\", \"Human\", \"Human\", \"Human\", \"Human\", \"Zabrak\", \"Hum…## [1] FALSE\nsw_dup %>%\n  drop_na(gender:species) %>%\n  glimpse() %>%\n  anyNA()## Rows: 83\n## Columns: 11\n## $ name       <chr> \"Mon Mothma\", \"Roos Tarpals\", \"Saesee Tiin\", \"Yarael Poof\",…\n## $ height     <int> 150, 224, 188, 264, 180, 163, 183, 188, 165, 165, 188, 175,…\n## $ mass       <dbl> NA, 82, NA, NA, 110, 65, 80, 84, 75, 45, 84, 79, 48, NA, 80…\n## $ hair_color <chr> \"auburn\", \"none\", \"none\", \"none\", \"brown\", \"none\", \"none\", …\n## $ skin_color <chr> \"fair\", \"grey\", \"pale\", \"white\", \"fair\", \"grey, green, yell…\n## $ eye_color  <chr> \"blue\", \"orange\", \"orange\", \"yellow\", \"blue\", \"orange\", \"ye…\n## $ birth_year <dbl> 48, NA, NA, NA, NA, NA, NA, 72, 47, 46, 72, 37, NA, 82, 54,…\n## $ sex        <chr> \"female\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"…\n## $ gender     <chr> \"feminine\", \"masculine\", \"masculine\", \"masculine\", \"masculi…\n## $ homeworld  <chr> \"Chandrila\", \"Naboo\", \"Iktotch\", \"Quermia\", \"Bestine IV\", \"…\n## $ species    <chr> \"Human\", \"Gungan\", \"Iktotchi\", \"Quermian\", \"Human\", \"Toong\"…## [1] TRUE\nsw_dup %>%\n  filter(species == \"Human\", (is.na(mass) | height > 180)) %>%\n  head(10)## # A tibble: 10 × 11\n##    name    height  mass hair_color skin_color eye_color birth_year sex    gender\n##    <chr>    <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr>  <chr> \n##  1 Mon Mo…    150    NA auburn     fair       blue              48 female femin…\n##  2 Finn        NA    NA black      dark       dark              NA male   mascu…\n##  3 Arvel …     NA    NA brown      fair       brown             NA male   mascu…\n##  4 Mace W…    188    84 none       dark       brown             72 male   mascu…\n##  5 Mace W…    188    84 none       dark       brown             72 male   mascu…\n##  6 Qui-Go…    193    89 brown      fair       blue              92 male   mascu…\n##  7 Cliegg…    183    NA brown      fair       blue              82 male   mascu…\n##  8 Cliegg…    183    NA brown      fair       blue              82 male   mascu…\n##  9 Mon Mo…    150    NA auburn     fair       blue              48 female femin…\n## 10 Cliegg…    183    NA brown      fair       blue              82 male   mascu…\n## # … with 2 more variables: homeworld <chr>, species <chr>\niris[,1:4] %>%\n  as_tibble() %>%  \n  filter_all(any_vars(. > 7.5))## # A tibble: 6 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1          7.6         3            6.6         2.1\n## 2          7.7         3.8          6.7         2.2\n## 3          7.7         2.6          6.9         2.3\n## 4          7.7         2.8          6.7         2  \n## 5          7.9         3.8          6.4         2  \n## 6          7.7         3            6.1         2.3\nsw_dup %>% \n  filter_if(is.character, any_vars(is.na(.)))## # A tibble: 19 × 11\n##    name    height  mass hair_color skin_color  eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr> \n##  1 Sly Mo…    178    48 none       pale        white             NA <NA>  <NA>  \n##  2 Finn        NA    NA black      dark        dark              NA male  mascu…\n##  3 Arvel …     NA    NA brown      fair        brown             NA male  mascu…\n##  4 BB8         NA    NA none       none        black             NA none  mascu…\n##  5 IG-88      200   140 none       metal       red               15 none  mascu…\n##  6 Qui-Go…    193    89 brown      fair        blue              92 male  mascu…\n##  7 R2-D2       96    32 <NA>       white, blue red               33 none  mascu…\n##  8 R5-D4       97    32 <NA>       white, red  red               NA none  mascu…\n##  9 Captai…     NA    NA unknown    unknown     unknown           NA <NA>  <NA>  \n## 10 Ric Ol…    183    NA brown      fair        blue              NA <NA>  <NA>  \n## 11 Poe Da…     NA    NA brown      light       brown             NA male  mascu…\n## 12 BB8         NA    NA none       none        black             NA none  mascu…\n## 13 Yoda        66    17 white      green       brown            896 male  mascu…\n## 14 Yoda        66    17 white      green       brown            896 male  mascu…\n## 15 Ric Ol…    183    NA brown      fair        blue              NA <NA>  <NA>  \n## 16 Finn        NA    NA black      dark        dark              NA male  mascu…\n## 17 Rey         NA    NA brown      light       hazel             NA fema… femin…\n## 18 Finn        NA    NA black      dark        dark              NA male  mascu…\n## 19 Rey         NA    NA brown      light       hazel             NA fema… femin…\n## # … with 2 more variables: homeworld <chr>, species <chr>"},{"path":"tidyverse.html","id":"build-new-column-or-mutate-column","chapter":"93 TidyVerse","heading":"93.0.5 build new column or mutate column","text":"use mutate create rows columnsuse mutate_all apply function rows columnsuse mutate_if apply function specific row column satisfied conditionsuse mutate_at apply function specific row columnWe can use if_else() case_when() decide change value\nif_else(condition, true:—, false: —-)case_when (condition1 ~ class1,\ncondition2 ~ class2,\ncondition3 ~ class3,\n……\n)","code":"\niris %>%\n  as_tibble(iris) %>%\n  mutate(add_all = Sepal.Length + Sepal.Width + Petal.Length + Petal.Width) %>%\n  head(10)## # A tibble: 10 × 6\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species add_all\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>     <dbl>\n##  1          5.1         3.5          1.4         0.2 setosa     10.2\n##  2          4.9         3            1.4         0.2 setosa      9.5\n##  3          4.7         3.2          1.3         0.2 setosa      9.4\n##  4          4.6         3.1          1.5         0.2 setosa      9.4\n##  5          5           3.6          1.4         0.2 setosa     10.2\n##  6          5.4         3.9          1.7         0.4 setosa     11.4\n##  7          4.6         3.4          1.4         0.3 setosa      9.7\n##  8          5           3.4          1.5         0.2 setosa     10.1\n##  9          4.4         2.9          1.4         0.2 setosa      8.9\n## 10          4.9         3.1          1.5         0.1 setosa      9.6\niris %>%\n  as_tibble() %>%\n  mutate(median_petal_length = median(Petal.Length),\n         has_long_petals = Petal.Length > median_petal_length,\n         has_long_petals = as.numeric(has_long_petals)) %>%  \n  select(Petal.Length, median_petal_length, has_long_petals)## # A tibble: 150 × 3\n##    Petal.Length median_petal_length has_long_petals\n##           <dbl>               <dbl>           <dbl>\n##  1          1.4                4.35               0\n##  2          1.4                4.35               0\n##  3          1.3                4.35               0\n##  4          1.5                4.35               0\n##  5          1.4                4.35               0\n##  6          1.7                4.35               0\n##  7          1.4                4.35               0\n##  8          1.5                4.35               0\n##  9          1.4                4.35               0\n## 10          1.5                4.35               0\n## # … with 140 more rows\nmsleep %>%\n  mutate_all(tolower)## # A tibble: 83 × 11\n##    name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n##    <chr>  <chr> <chr> <chr> <chr>        <chr>       <chr>     <chr>       <chr>\n##  1 cheet… acin… carni carn… lc           12.1        <NA>      <NA>        11.9 \n##  2 owl m… aotus omni  prim… <NA>         17          1.8       <NA>        7    \n##  3 mount… aplo… herbi rode… nt           14.4        2.4       <NA>        9.6  \n##  4 great… blar… omni  sori… lc           14.9        2.3       0.133333333 9.1  \n##  5 cow    bos   herbi arti… domesticated 4           0.7       0.666666667 20   \n##  6 three… brad… herbi pilo… <NA>         14.4        2.2       0.766666667 9.6  \n##  7 north… call… carni carn… vu           8.7         1.4       0.383333333 15.3 \n##  8 vespe… calo… <NA>  rode… <NA>         7           <NA>      <NA>        17   \n##  9 dog    canis carni carn… domesticated 10.1        2.9       0.333333333 13.9 \n## 10 roe d… capr… herbi arti… lc           3           <NA>      <NA>        21   \n## # … with 73 more rows, and 2 more variables: brainwt <chr>, bodywt <chr>\niris %>%\n  mutate_if(is.double, as.integer) %>%\n  head(10)## # A tibble: 10 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <int>       <int>        <int>       <int> <fct>  \n##  1            5           3            1           0 setosa \n##  2            4           3            1           0 setosa \n##  3            4           3            1           0 setosa \n##  4            4           3            1           0 setosa \n##  5            5           3            1           0 setosa \n##  6            5           3            1           0 setosa \n##  7            4           3            1           0 setosa \n##  8            5           3            1           0 setosa \n##  9            4           2            1           0 setosa \n## 10            4           3            1           0 setosa\niris %>%\n  as_tibble() %>%\n  mutate_at(vars(contains(\"Length\"), contains(\"Width\")), ~ .*10) %>%\n  head(10)## # A tibble: 10 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1           51          35           14           2 setosa \n##  2           49          30           14           2 setosa \n##  3           47          32           13           2 setosa \n##  4           46          31           15           2 setosa \n##  5           50          36           14           2 setosa \n##  6           54          39           17           4 setosa \n##  7           46          34           14           3 setosa \n##  8           50          34           15           2 setosa \n##  9           44          29           14           2 setosa \n## 10           49          31           15           1 setosa\nwarpbreaks %>%\n  mutate(breed = if_else(wool == \"A\",\n                         true = \"Merino\",\n                         false = \"Corriedale\")) %>%\n  sample_frac(size = 0.15)##   breaks wool tension      breed\n## 1     41    B       L Corriedale\n## 2     54    A       L     Merino\n## 3     51    A       L     Merino\n## 4     15    B       H Corriedale\n## 5     13    B       H Corriedale\n## 6     30    A       M     Merino\n## 7     35    A       M     Merino\n## 8     21    A       H     Merino\nwarpbreaks %>%\n  mutate(tension=case_when(tension == \"H\" ~ \"High\",\n                           tension == \"M\" ~ \"Medium\",\n                           tension == \"L\" ~ \"Low\",\n                           TRUE ~ NA_character_)) %>%\n  sample_frac(size = 0.15)##   breaks wool tension\n## 1     29    B  Medium\n## 2     28    B    High\n## 3     30    A  Medium\n## 4     16    B    High\n## 5     25    A     Low\n## 6     29    B     Low\n## 7     70    A     Low\n## 8     15    B    High"},{"path":"tidyverse.html","id":"group","chapter":"93 TidyVerse","heading":"93.0.6 Group","text":"use group_by() create groups use ungroup separate group\nuse summarize() find properties group\nn() :count frequency\nsum(var),max(var),min(var),mean(var),median(var),sd(var)can also use summarize_if,summarize_all","code":"\niris %>%\n  group_by(Species) %>%\n  summarise(n = n(),\n            avg_len = mean(Petal.Length, na.rm=TRUE),\n            med_len = median(Petal.Length))## # A tibble: 3 × 4\n##   Species        n avg_len med_len\n##   <fct>      <int>   <dbl>   <dbl>\n## 1 setosa        50    1.46    1.5 \n## 2 versicolor    50    4.26    4.35\n## 3 virginica     50    5.55    5.55\ndat <- iris %>%\n  group_by(Species) %>%\n  summarise_if(is.numeric, list(~mean(.), ~median(.), ~min(.), ~max(.)))\ndat## # A tibble: 3 × 17\n##   Species    Sepal.Length_me… Sepal.Width_mean Petal.Length_me… Petal.Width_mean\n##   <fct>                 <dbl>            <dbl>            <dbl>            <dbl>\n## 1 setosa                 5.01             3.43             1.46            0.246\n## 2 versicolor             5.94             2.77             4.26            1.33 \n## 3 virginica              6.59             2.97             5.55            2.03 \n## # … with 12 more variables: Sepal.Length_median <dbl>,\n## #   Sepal.Width_median <dbl>, Petal.Length_median <dbl>,\n## #   Petal.Width_median <dbl>, Sepal.Length_min <dbl>, Sepal.Width_min <dbl>,\n## #   Petal.Length_min <dbl>, Petal.Width_min <dbl>, Sepal.Length_max <dbl>,\n## #   Sepal.Width_max <dbl>, Petal.Length_max <dbl>, Petal.Width_max <dbl>"},{"path":"how-to-animate-your-plots.html","id":"how-to-animate-your-plots","chapter":"94 How to animate your plots","heading":"94 How to animate your plots","text":"Thomas Holvoetggplot amazing create numerous visualizations data, ranging histograms scatter plots parallel coordinate plots, mosaic plots, . However, visualizing time dependencies static plot difficult. use one axes time, limits power visualization axes valuable resource. creating animated plot, gain additional axis. allows visualize change time relationship variables, gives real estate inspect dynamics additional variable. tutorial, show two ways animate plots R.","code":"\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(wpp2019)\nlibrary(maps)"},{"path":"how-to-animate-your-plots.html","id":"gganimate-1","chapter":"94 How to animate your plots","heading":"94.1 gganimate","text":"gganimate library provides intuitive way create animated plots. simply need specify time axis p + transition_time(time).start first example. secret world population rapidly rising. create simple time series plot total population time. However, another important insight age distribution population. dynamics distribution time quite difficult capture static plot. Animating distribution time yields interesting insights. world’s population shows steady increase age categories. can also see number newborns (0-4) increase constant rate, rather goes waves. Moreover, can see newborns age older age groups child mortality continues fall.total population, distribution stay less years. Japan contending aging population. can clearly observed dynamic age distribution plot . starts similar world population, younger population starts decreasing significantly weight shifts towards elderly.","code":"\n# Prepare the data: get the population per year, country, and age group\ndata(\"pop\")\n\npop_by_age <- rbind(popF, popM)\n\n# Some of the country names need to be changed (see later on)\npop_by_age$name <- recode(pop_by_age$name,\n                          \"United States of America\" = \"USA\",\n                          \"United Kingdom\" = \"UK\",\n                          \"Viet Nam\" = \"Vietnam\",\n                          \"Bolivia (Plurinational State of)\" = \"Bolivia\",\n                          \"Brunei Darussalam\" = \"Brunei\",\n                          \"Iran (Islamic Republic of)\" = \"Iran\",\n                          \"China, Hong Kong SAR\" = \"Hong Kong\",                  \n                          \"China, Macao SAR\" = \"China\",  \n                          \"China, Taiwan Province of China\" = \"Taiwan\",\n                          \"Dem. Republic of the Congo\" = \"Democratic Republic of the Congo\",\n                          \"Dem. People's Rep. of Korea\" = \"North Korea\",\n                          \"Lao People's Dem. Republic\" = \"Laos\",\n                          \"Venezuela (Bolivarian Republic of)\" = \"Venezuela\",\n                          \"United Republic of Tanzania\" = \"Tanzania\",\n                          \"Trinidad and Tobago\" = \"Trinidad\",\n                          \"Syrian Arab Republic\" = \"Syria\",\n                          \"State of Palestine\" = \"Palestine\",\n                          \"Russian Federation\" = \"Russia\",\n                          \"Republic of Moldova\" = \"Moldova\",\n                          \"Cote d'Ivoire\" = \"Ivory Coast\",\n                          \"Czechia\" = \"Czech Republic\",\n                          \"Congo\" = \"Republic of Congo\"\n                          )\n\n\npop_by_age <- pop_by_age %>%\n  pivot_longer(cols = 4:18, names_to = \"Year\") %>%\n  select(age, Year, name, value) %>%\n  group_by(age, Year, name) %>%\n  summarise(value = sum(value)) %>%\n  ungroup() %>%\n  mutate(age = fct_relevel(age, \"5-9\", after = 1)) %>%\n  mutate(age = fct_relevel(age, \"100+\", after = Inf)) %>%\n  mutate(Year = strtoi(Year))\n\n# This function will plot the distribution over time for a certain area\nplot_dynamic_distribution <- function(area) {\npop_by_age %>%\n  filter(name == area) %>%\n  ggplot(aes(x = age, y = 1000 * value)) +\n  geom_bar(stat = 'identity', fill = \"cornflowerblue\") + \n  transition_time(Year) +\n  labs(title = paste(area, \" Population in {frame_time}\"), y = \"Population\", x = \"Age\") + \n  theme(axis.text.x = element_text(angle = 90))\n}\nplot_dynamic_distribution(\"World\")\nplot_dynamic_distribution(\"Japan\")"},{"path":"how-to-animate-your-plots.html","id":"using-the-animate.hook-hook","chapter":"94 How to animate your plots","heading":"94.2 Using the animate.hook Hook","text":"Maps another great example additional time axis come handy. Generally, plots maps quite limiting. color-coding areas, one variable can plotted time. creating animation map evolution time variable can also visualized.gganimate package allow evolving color time. can take flexible approach gganimate create animated plot map. first create multiple plots corresponding different points time. plots printed output R-markdown code block (forget actually printing ). specify {r, animation.hook='gifski', interval=0.3} code block. combine outputted plots one GIF!example, look fraction 50+ population per country. already saw Japan, weight population shifted towards elderly. evolution look like world scale? can see population aging new phenomenon mainly affects developed countries.Creating animated plots valuable skill data visualization toolkit. Using gganimate one can easily animate ggplot plots. However, work plot types. using animation.hook approach can animate virtually plot create. let’s honest, animated plots cool :)Used ResourcesOn gganimate https://www.datanovia.com/en/blog/gganimate---create-plots--beautiful-animation--r/creating maps https://stackoverflow.com/questions/61838700/query----make-world-heat-map-using-ggplot--rOn animation.hook https://bookdown.org/yihui/rmarkdown-cookbook/animation.html","code":"\n# Prepare the data: calculate the fraction of 50+ population per country, per year\n\npop_by_old <- pop_by_age\n\npop_by_old$old = as.numeric(pop_by_old$age) >= 11\n\npop_by_old <- pop_by_old %>%\n  group_by(Year, name) %>%\n  mutate(total = sum(value)) %>%\n  ungroup()\n\npop_by_old <- pop_by_old %>%\n  group_by(Year, old, name) %>%\n  summarise(frac = sum(value) / total) %>% ungroup() %>%\n  unique %>%\n  filter(old)\nworld_map <- map_data(\"world\")\nworld_map <- subset(world_map, region != \"Antarctica\")\n\n# Iterate over the time axis to print multiple plots, these will be combined by the animation.hook\n\nfor (y in unique(pop_by_old$Year)) {\n  print(pop_by_old %>%\n    filter(Year == y) %>%\n    filter(name %in% unique(world_map$region)) %>%\n    select(Year, name, frac) %>%\n\n    ggplot() +\n    geom_map(\n      dat = world_map, map = world_map, aes(map_id = region),\n      fill = \"white\", color = \"#7f7f7f\"\n    ) +\n    geom_map(map = world_map, aes(map_id = name, fill = frac)) +\n    scale_fill_gradient(low = \"#fff7bc\", high = \"#cc4c02\", name = \"Fraction\", limits = c(0, max(pop_by_old$frac))) +\n    expand_limits(x = world_map$long, y = world_map$lat) + \n    labs(title = paste(\"Fraction of 50+ Population in \", y)))\n}"},{"path":"dataframe-operations-tutorial.html","id":"dataframe-operations-tutorial","chapter":"95 Dataframe operations tutorial","heading":"95 Dataframe operations tutorial","text":"Yunzhe Zhang","code":"\nlibrary(dplyr)"},{"path":"dataframe-operations-tutorial.html","id":"tutorial-for-being-familiar-with-dataframe-in-r","chapter":"95 Dataframe operations tutorial","heading":"95.0.1 Tutorial for being familiar with DataFrame in R","text":"tutorial introduce basic ways help beginners familiar dataframe R. Since usually plotting graph, need handle data well unavoidable process data stored dataframe. Thus, tutorial made people familiar function help user operation dataframe R.","code":""},{"path":"dataframe-operations-tutorial.html","id":"introduction-20","chapter":"95 Dataframe operations tutorial","heading":"95.0.1.1 Introduction","text":"\ndata frame table two-dimensional array-like structure column contains values one variable row contains one set values column. several characteristics data frame\n1. column names non-empty.\n2. row names unique.\n3. data stored data frame can numeric, factor character type.\n4. column contain number data items.Next, going introduce several basic operations dataframe R.can see, format create dataframe code. need explicitly states value row column name. want get structure data frame, use str() functionAs shown , information dataframe shown belowe like number observations column names. begin process data, use str() function help us familiar dataframe processing later. coudl also use summary function also get information dataframe.use following code iterate dataframe want operations dataframeHowever, deal dataframe, times want create empty dataframe. achieve following code.want append rows empty dataframe following codeWe also add whole columns dataframe following code.However, sometimes want append values another dataframe current dataframe. use rbind function append second dataframe first dataframe.also extract specific colunme following operation.Besides, important operation dataframe R index slicing. index slicing quite similar python totally . index slicing R represented [rows, columns]. example, df3[1:2, ] represents take first two rows columns [, 1:2] represent take rows first two columnsAt time, also use subset filter values get rows want. subset function also supports logical operators.basic operations dataframe, next going use dplyr pacakge advanced operations dataframe.","code":"\n# Create a data frame.\ntemp <- data.frame(\n   index = c (1:5), \n   names = c(\"Rick\",\"Dan\",\"Michelle\",\"Ryan\",\"Gary\"),\n   salary = c(623.3,515.2,611.0,729.0,843.25), \n   sex = c(\"male\", \"female\", \"male\", \"male\", \"male\"),\n   Date = as.Date(c(\"2012-01-01\", \"2013-09-23\", \"2014-11-15\", \"2014-05-11\",\n      \"2015-03-27\"))\n)\n# display the data frame.           \ntemp ##   index    names salary    sex       Date\n## 1     1     Rick 623.30   male 2012-01-01\n## 2     2      Dan 515.20 female 2013-09-23\n## 3     3 Michelle 611.00   male 2014-11-15\n## 4     4     Ryan 729.00   male 2014-05-11\n## 5     5     Gary 843.25   male 2015-03-27\nstr(temp)## 'data.frame':    5 obs. of  5 variables:\n##  $ index : int  1 2 3 4 5\n##  $ names : chr  \"Rick\" \"Dan\" \"Michelle\" \"Ryan\" ...\n##  $ salary: num  623 515 611 729 843\n##  $ sex   : chr  \"male\" \"female\" \"male\" \"male\" ...\n##  $ Date  : Date, format: \"2012-01-01\" \"2013-09-23\" ...\nsummary(temp)##      index      names               salary          sex           \n##  Min.   :1   Length:5           Min.   :515.2   Length:5          \n##  1st Qu.:2   Class :character   1st Qu.:611.0   Class :character  \n##  Median :3   Mode  :character   Median :623.3   Mode  :character  \n##  Mean   :3                      Mean   :664.4                     \n##  3rd Qu.:4                      3rd Qu.:729.0                     \n##  Max.   :5                      Max.   :843.2                     \n##       Date           \n##  Min.   :2012-01-01  \n##  1st Qu.:2013-09-23  \n##  Median :2014-05-11  \n##  Mean   :2014-01-14  \n##  3rd Qu.:2014-11-15  \n##  Max.   :2015-03-27\nfor(i in 1:nrow(temp)) {\n    # row represent each row in the dataframe\n    row <- temp[i,]\n    # we could get specific column value by $, for example print the names for each row\n    print(row$names)\n}## [1] \"Rick\"\n## [1] \"Dan\"\n## [1] \"Michelle\"\n## [1] \"Ryan\"\n## [1] \"Gary\"\n# A empty df with no column names and set the dimension of the dataframe to be 3*3\nemptydf =  data.frame(matrix(ncol = 3, nrow = 3))\n# setting the column names\nx <- c(\"name\", \"age\", \"gender\")\ncolnames(emptydf) <- x\nemptydf##   name age gender\n## 1   NA  NA     NA\n## 2   NA  NA     NA\n## 3   NA  NA     NA\n# append at the first row\nemptydf[1,] = c(\"alex\", 18, \"male\")\n# append at the second row\nemptydf[2,] = c(\"tony\", 21, \"male\")\n# append at the last row\nemptydf[nrow(emptydf),] = c(\"Ales\", 25, \"female\")\nemptydf##   name age gender\n## 1 alex  18   male\n## 2 tony  21   male\n## 3 Ales  25 female\nemptydf$race <- c(\"White\",\"Black\",\"White\")\nemptydf##   name age gender  race\n## 1 alex  18   male White\n## 2 tony  21   male Black\n## 3 Ales  25 female White\ndf2 <- data.frame(var1=c(\"Derick\", \"James\", \"Molly\"),\n                  var2=c(23, 12, 6),\n                  var3=c(\"male\", \"male\", \"female\"),\n                  var4=c(\"black\", \"black\", \"Asian\"))\nx <- c(\"name\", \"age\", \"gender\", \"race\")\ncolnames(df2) <- x\ndf3 <- rbind(emptydf, df2)\ndf3##     name age gender  race\n## 1   alex  18   male White\n## 2   tony  21   male Black\n## 3   Ales  25 female White\n## 4 Derick  23   male black\n## 5  James  12   male black\n## 6  Molly   6 female Asian\nname <- data.frame(df3$gender,df3$age)\nprint(name)##   df3.gender df3.age\n## 1       male      18\n## 2       male      21\n## 3     female      25\n## 4       male      23\n## 5       male      12\n## 6     female       6\n# first case\ntemp1 <- df3[1:2,]\ntemp1##   name age gender  race\n## 1 alex  18   male White\n## 2 tony  21   male Black\n# second case\ntemp2 <- df3[, 1:2]\ntemp2##     name age\n## 1   alex  18\n## 2   tony  21\n## 3   Ales  25\n## 4 Derick  23\n## 5  James  12\n## 6  Molly   6\n# first case\ntemp3 <- subset(temp, names == \"Dan\" | salary > 750)\ntemp3##   index names salary    sex       Date\n## 2     2   Dan 515.20 female 2013-09-23\n## 5     5  Gary 843.25   male 2015-03-27\n# second case\ntemp4 <- subset(temp, salary < 700 & sex == \"male\")\ntemp4##   index    names salary  sex       Date\n## 1     1     Rick  623.3 male 2012-01-01\n## 3     3 Michelle  611.0 male 2014-11-15"},{"path":"dataframe-operations-tutorial.html","id":"dplyr","chapter":"95 Dataframe operations tutorial","heading":"95.0.1.2 Dplyr","text":"Dplyr grammar data manipulation, providing consistent set verbs help solve common data manipulation challenges:\n1. mutate() adds new variables functions existing variables.\n2. select() picks variables based names.\n3. filter() picks cases based values.\n4. summarise() reduces multiple values single summary.\n5. arrange() changes ordering rows., first need run installation Dplyr package import itNext, going use stats_wl.csv example import using read.csv function. Note, csv file must stored directory. Otherwise, need revise path order import .select()(optionally rename) select variables data frame, using concise mini-language makes easy refer variables based name.also select consecutive columns following codeWe also select columns specifing column wantWe also use start_with() , end_with() , contains() match column names.filter() function used subset data frame, retaining rows satisfy conditions. retained, row must produce value TRUE conditions. Note condition evaluates NA row dropped, unlike base subsetting [.\nmany functions operators useful constructing expressions used filter data:\n1. ==,>,>= etc.\n2. &, |, !, xor().\n3. .na().\n4. (), near().see code, add various logical operations filtering dataframe even including xor.also use filter .na() delete rows whose credit_amount NA. Thus, use filter get rows want based need.mutate() adds new variables preserves existing ones.use mutate initialize new column called pro_age calculated age / mean(age)summarise() creates new data frame. one () rows combination grouping variables; grouping variables, output single row summarising observations input. contain one column grouping variable one column summary statistics specified.use grou_by() summarise find mean age, mean duration total counts class good class bad.arrange orders rows data frame values selected columns.use arrange() sort values based variable choose. code , sort dataframe variable mean_age asenting order. also choose desc order adding desc() front mean_age.","code":"\n#install.packages(\"dplyr\")\ndf <- read.csv(\"resources/Rayzyz_resources/data.csv\", stringsAsFactors = F)\nhead(df)##   duration credit_amount     savings_status employment  property_magnitude age\n## 1        6          1169 'no known savings'      '>=7'       'real estate'  67\n## 2       48          5951             '<100'   '1<=X<4'       'real estate'  22\n## 3       12          2096             '<100'   '4<=X<7'       'real estate'  49\n## 4       42          7882             '<100'   '4<=X<7'    'life insurance'  45\n## 5       24          4870             '<100'   '1<=X<4' 'no known property'  53\n## 6       36          9055 'no known savings'   '1<=X<4' 'no known property'  35\n##   own_telephone class\n## 1                good\n## 2                 bad\n## 3                good\n## 4          none  good\n## 5                 bad\n## 6                good\nhead(df,10) %>% select(duration, employment, age)##    duration employment age\n## 1         6      '>=7'  67\n## 2        48   '1<=X<4'  22\n## 3        12   '4<=X<7'  49\n## 4        42   '4<=X<7'  45\n## 5        24   '1<=X<4'  53\n## 6        36   '1<=X<4'  35\n## 7        24      '>=7'  53\n## 8        36   '1<=X<4'  35\n## 9        12   '4<=X<7'  61\n## 10       30 unemployed  28\nhead(df,10) %>% select(duration:age)##    duration credit_amount     savings_status employment  property_magnitude age\n## 1         6          1169 'no known savings'      '>=7'       'real estate'  67\n## 2        48          5951             '<100'   '1<=X<4'       'real estate'  22\n## 3        12          2096             '<100'   '4<=X<7'       'real estate'  49\n## 4        42          7882             '<100'   '4<=X<7'    'life insurance'  45\n## 5        24          4870             '<100'   '1<=X<4' 'no known property'  53\n## 6        36          9055 'no known savings'   '1<=X<4' 'no known property'  35\n## 7        24            NA      '500<=X<1000'      '>=7'    'life insurance'  53\n## 8        36            NA             '<100'   '1<=X<4'                 car  35\n## 9        12            NA           '>=1000'   '4<=X<7'       'real estate'  61\n## 10       30            NA             '<100' unemployed                 car  28\nhead(df,10) %>% select(-(age))##    duration credit_amount     savings_status employment  property_magnitude\n## 1         6          1169 'no known savings'      '>=7'       'real estate'\n## 2        48          5951             '<100'   '1<=X<4'       'real estate'\n## 3        12          2096             '<100'   '4<=X<7'       'real estate'\n## 4        42          7882             '<100'   '4<=X<7'    'life insurance'\n## 5        24          4870             '<100'   '1<=X<4' 'no known property'\n## 6        36          9055 'no known savings'   '1<=X<4' 'no known property'\n## 7        24            NA      '500<=X<1000'      '>=7'    'life insurance'\n## 8        36            NA             '<100'   '1<=X<4'                 car\n## 9        12            NA           '>=1000'   '4<=X<7'       'real estate'\n## 10       30            NA             '<100' unemployed                 car\n##    own_telephone class\n## 1                 good\n## 2                  bad\n## 3                 good\n## 4           none  good\n## 5                  bad\n## 6                 good\n## 7                 good\n## 8                 good\n## 9                 good\n## 10                 bad\n# starts_with\nhead(df,10) %>% select(starts_with(\"dura\"))##    duration\n## 1         6\n## 2        48\n## 3        12\n## 4        42\n## 5        24\n## 6        36\n## 7        24\n## 8        36\n## 9        12\n## 10       30\n# contains\nhead(df,10) %>% select(contains(\"_\"))##    credit_amount     savings_status  property_magnitude own_telephone\n## 1           1169 'no known savings'       'real estate'              \n## 2           5951             '<100'       'real estate'              \n## 3           2096             '<100'       'real estate'              \n## 4           7882             '<100'    'life insurance'          none\n## 5           4870             '<100' 'no known property'              \n## 6           9055 'no known savings' 'no known property'              \n## 7             NA      '500<=X<1000'    'life insurance'              \n## 8             NA             '<100'                 car              \n## 9             NA           '>=1000'       'real estate'              \n## 10            NA             '<100'                 car\n# get rows whose duration is > 20 and class is good.\nhead(df,10) %>% filter(duration > 20 & class == as.character(\"good\"))##   duration credit_amount     savings_status employment  property_magnitude age\n## 1       42          7882             '<100'   '4<=X<7'    'life insurance'  45\n## 2       36          9055 'no known savings'   '1<=X<4' 'no known property'  35\n## 3       24            NA      '500<=X<1000'      '>=7'    'life insurance'  53\n## 4       36            NA             '<100'   '1<=X<4'                 car  35\n##   own_telephone class\n## 1          none  good\n## 2                good\n## 3                good\n## 4                good\n# get rows whose duration is > 20 or class is good.\nhead(df,10) %>% filter(duration > 20 | class == as.character(\"good\"))##    duration credit_amount     savings_status employment  property_magnitude age\n## 1         6          1169 'no known savings'      '>=7'       'real estate'  67\n## 2        48          5951             '<100'   '1<=X<4'       'real estate'  22\n## 3        12          2096             '<100'   '4<=X<7'       'real estate'  49\n## 4        42          7882             '<100'   '4<=X<7'    'life insurance'  45\n## 5        24          4870             '<100'   '1<=X<4' 'no known property'  53\n## 6        36          9055 'no known savings'   '1<=X<4' 'no known property'  35\n## 7        24            NA      '500<=X<1000'      '>=7'    'life insurance'  53\n## 8        36            NA             '<100'   '1<=X<4'                 car  35\n## 9        12            NA           '>=1000'   '4<=X<7'       'real estate'  61\n## 10       30            NA             '<100' unemployed                 car  28\n##    own_telephone class\n## 1                 good\n## 2                  bad\n## 3                 good\n## 4           none  good\n## 5                  bad\n## 6                 good\n## 7                 good\n## 8                 good\n## 9                 good\n## 10                 bad\nhead(df,10) %>% filter(duration > 20 & class == as.character(\"good\") & is.na(credit_amount) == FALSE)##   duration credit_amount     savings_status employment  property_magnitude age\n## 1       42          7882             '<100'   '4<=X<7'    'life insurance'  45\n## 2       36          9055 'no known savings'   '1<=X<4' 'no known property'  35\n##   own_telephone class\n## 1          none  good\n## 2                good\nhead(df,10) %>% select(duration, class, age) %>% group_by(class) %>% mutate(por_age = age/ mean(age)) ## # A tibble: 10 × 4\n## # Groups:   class [2]\n##    duration class   age por_age\n##       <int> <chr> <int>   <dbl>\n##  1        6 good     67   1.36 \n##  2       48 bad      22   0.641\n##  3       12 good     49   0.994\n##  4       42 good     45   0.913\n##  5       24 bad      53   1.54 \n##  6       36 good     35   0.710\n##  7       24 good     53   1.08 \n##  8       36 good     35   0.710\n##  9       12 good     61   1.24 \n## 10       30 bad      28   0.816\nhead(df,10) %>%\n  group_by(class) %>%\n  summarise(mean_age = mean(age), mean_duration = mean(duration), count = n())## # A tibble: 2 × 4\n##   class mean_age mean_duration count\n##   <chr>    <dbl>         <dbl> <int>\n## 1 bad       34.3            34     3\n## 2 good      49.3            24     7\nhead(df,10) %>%\n  group_by(property_magnitude) %>%\n  summarise(mean_age = mean(age), mean_duration = mean(duration), count = n()) %>% arrange(mean_age)## # A tibble: 4 × 4\n##   property_magnitude  mean_age mean_duration count\n##   <chr>                  <dbl>         <dbl> <int>\n## 1 car                     31.5          33       2\n## 2 'no known property'     44            30       2\n## 3 'life insurance'        49            33       2\n## 4 'real estate'           49.8          19.5     4"},{"path":"dataframe-operations-tutorial.html","id":"join-two-tables","chapter":"95 Dataframe operations tutorial","heading":"95.0.1.3 Join two tables","text":"four different type join table left join, right join, inner join, full join. first create two dataframe df1 df2.Left Join join table based common values df1 df2 . values df1 can find df2, keep missing values corresponding column df2 initialized NARight Join join table based common values df1 df2 . values df2 can find df1, keep missing values corresponding column df1 initialized NAInner Join join two tables values shared df1 df2Full_join concate values df1 df2 leave NA values can find table.","code":"\ndf1 <- data.frame(\n   index = c (1:10), \n   State = c(\"GA\",\"GA\", \"GA\",\"FL\",\"FL\",\"FL\",\"AL\",\"AL\",\"AL\", \"CA\"), \n   Limit = c(\"50000\", \"50000\", \"40000\", \"30000\", \"75000\",\"75000\", \"85000\", \"90000\", \"45000\", \"125000\")\n)\ndf1##    index State  Limit\n## 1      1    GA  50000\n## 2      2    GA  50000\n## 3      3    GA  40000\n## 4      4    FL  30000\n## 5      5    FL  75000\n## 6      6    FL  75000\n## 7      7    AL  85000\n## 8      8    AL  90000\n## 9      9    AL  45000\n## 10    10    CA 125000\ndf2 <- data.frame(\n   index = c (1:4), \n   State = c(\"GA\",\"FL\", \"AL\", \"OH\"), \n   regulatory_limit = c(\"50000\", \"75000\", \"45000\", \"48000\")\n)\ndf2##   index State regulatory_limit\n## 1     1    GA            50000\n## 2     2    FL            75000\n## 3     3    AL            45000\n## 4     4    OH            48000\ndf1 %>% left_join(df2, by=\"State\")##    index.x State  Limit index.y regulatory_limit\n## 1        1    GA  50000       1            50000\n## 2        2    GA  50000       1            50000\n## 3        3    GA  40000       1            50000\n## 4        4    FL  30000       2            75000\n## 5        5    FL  75000       2            75000\n## 6        6    FL  75000       2            75000\n## 7        7    AL  85000       3            45000\n## 8        8    AL  90000       3            45000\n## 9        9    AL  45000       3            45000\n## 10      10    CA 125000      NA             <NA>\ndf1 %>% right_join(df2, by=\"State\")##    index.x State Limit index.y regulatory_limit\n## 1        1    GA 50000       1            50000\n## 2        2    GA 50000       1            50000\n## 3        3    GA 40000       1            50000\n## 4        4    FL 30000       2            75000\n## 5        5    FL 75000       2            75000\n## 6        6    FL 75000       2            75000\n## 7        7    AL 85000       3            45000\n## 8        8    AL 90000       3            45000\n## 9        9    AL 45000       3            45000\n## 10      NA    OH  <NA>       4            48000\ndf1 %>% inner_join(df2, by=\"State\")##   index.x State Limit index.y regulatory_limit\n## 1       1    GA 50000       1            50000\n## 2       2    GA 50000       1            50000\n## 3       3    GA 40000       1            50000\n## 4       4    FL 30000       2            75000\n## 5       5    FL 75000       2            75000\n## 6       6    FL 75000       2            75000\n## 7       7    AL 85000       3            45000\n## 8       8    AL 90000       3            45000\n## 9       9    AL 45000       3            45000\ndf1 %>% full_join(df2, by=\"State\")##    index.x State  Limit index.y regulatory_limit\n## 1        1    GA  50000       1            50000\n## 2        2    GA  50000       1            50000\n## 3        3    GA  40000       1            50000\n## 4        4    FL  30000       2            75000\n## 5        5    FL  75000       2            75000\n## 6        6    FL  75000       2            75000\n## 7        7    AL  85000       3            45000\n## 8        8    AL  90000       3            45000\n## 9        9    AL  45000       3            45000\n## 10      10    CA 125000      NA             <NA>\n## 11      NA    OH   <NA>       4            48000"},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"a-brief-instruction-for-the-half-semester-of-edav5702","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96 A brief instruction for the half semester of EDAV5702","text":"Siyuan Sang","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"introduction-21","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.1 Introduction","text":"\n","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"the-exploratory-data-analysis-and-visualizationedav-is-a-very-useful-course-for-r-users-to-get-familiar-with-the-techiques-and-rules-we-need-to-learn-in-terms-of-data-visualizationhonestly-anyone-can-figure-out-this-from-the-name.-in-this-instruction-instead-of-digging-into-some-r-vitualization-skills-ill-provide-you-with-some-useful-suggestions-and-resources-based-on-my-first-half-of-semester.-i-sincerely-hope-this-instruction-can-help-somebody-out-in-the-future.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.1.0.1 The Exploratory Data Analysis and Visualization(EDAV) is a very useful course for R users to get familiar with the techiques and rules we need to learn in terms of data visualization(honestly, anyone can figure out this from the name). In this instruction, instead of digging into some R vitualization skills, I’ll provide you with some useful suggestions and resources based on my first half of semester. I sincerely hope this instruction can help somebody out in the future.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"the-packages-you-should-get-familiar-with","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.2 The packages you should get familiar with","text":"","code":"\nlibrary(ggplot2)\nlibrary(tidyverse)"},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"these-two-lines-of-code-will-appear-on-the-top-of-your-every-homework-assignment.-get-familiar-with-them-ggplot2-is-the-base-of-graph-in-this-course-and-tidyverse-is-the-core-package-of-tidyverse.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.2.0.1 These two lines of code will appear on the top of your every homework assignment. Get familiar with them, ggplot2 is the base of graph in this course, and tidyverse is the core package of tidyverse.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"herere-two-websites-you-can-learn-in-advance","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.2.0.2 Here’re two websites you can learn in advance,","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"for-ggpolt2-httpsr4ds.had.co.nzdata-visualisation.html","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.2.0.3 For ggpolt2: https://r4ds.had.co.nz/data-visualisation.html","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"for-tidyverse-httpsr4ds.had.co.nzintroduction.htmlthe-tidyverse","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.2.0.4 For tidyverse: https://r4ds.had.co.nz/introduction.html#the-tidyverse","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"pay-attention-to-the-rules-learned-in-class","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.3 Pay attention to the rules learned in class","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"in-the-process-of-data-vatualization-there-are-some-rules-that-you-may-ignore.-however-many-studentsincluding-me-lost-points-because-of-this.-to-make-it-more-specific-ill-quote-my-work-and-the-correct-one-from-my-homework1-here-to-show-you-how-it-works.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.3.0.1 In the process of data vatualization, there are some rules that you may ignore. However, many students(including me) lost points because of this. To make it more specific, I’ll quote my work and the correct one from my homework1 here to show you how it works.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"my-code-goes-like-this","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.3.0.2 My code goes like this:","text":"","code":"\nlibrary(openintro)\nggplot(loans_full_schema, aes(x =loan_purpose, y = loan_amount)) + \n  geom_boxplot() +\n  coord_flip()"},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"while-the-correct-one-should-be","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.3.0.3 While the correct one should be:","text":"","code":"\nlibrary(openintro)\nggplot(loans_full_schema, aes(x = fct_reorder(loan_purpose, loan_amount), y = loan_amount)) + \n  geom_boxplot() +\n  coord_flip()"},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"see-the-difference-the-only-thing-i-forget-to-do-is-arrange-the-boxed-in-descending-order-of-median.-however-it-turns-out-that-this-little-difference-can-cause-huge-difference.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.3.0.4 See the difference? The only thing I forget to do is arrange the boxed in descending order of median. However, it turns out that this little difference can cause huge difference.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"try-your-best-to-imporve-the-graph","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.4 Try your best to imporve the graph","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"as-a-course-of-data-vitualization-in-my-view-drawing-a-graph-should-never-be-the-end-of-the-story.-therere-many-ways-to-make-your-graphs-look-more-clear-and-refined.-here-i-give-an-example-of-the-scatterplot-of-housing-price.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.4.0.1 As a course of data vitualization, in my view, drawing a graph should never be the end of the story. There’re many ways to make your graphs look more clear and refined. Here I give an example of the scatterplot of housing price.","text":"","code":"\ncor_df <- ames %>%\n  group_by(Neighborhood) %>%\n  summarize(cor = cor(area, price), beta = lm(price~area)$coefficients[2], meanprice = mean(price)) %>%\n  ungroup() %>%\n  arrange(beta)\nggplot(cor_df, aes(meanprice, beta)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se= FALSE)"},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"its-now-just-a-set-of-points-with-a-line-in-this-graph.-now-i-want-to-improve-it.-considering-our-goal-to-make-an-analysis.-it-can-be-a-good-idea-to-put-the-names-of-neighborhoods-inside.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.4.0.2 It’s now just a set of points with a line in this graph. Now I want to improve it. Considering our goal to make an analysis. It can be a good idea to put the names of Neighborhoods inside.","text":"","code":"\nggplot(cor_df, aes(meanprice, beta, label = Neighborhood)) +\n  geom_point() +\n  geom_text(nudge_y = 10, size = 3) +\n  geom_smooth(method=\"lm\", se= FALSE)"},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"here-is-a-blog-id-like-to-provide-which-is-a-wide-summary-of-many-codes-to-improve-your-graph","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.4.0.3 Here is a blog I’d like to provide, which is a wide summary of many codes to improve your graph,","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"httpzevross.comblog20140804beautiful-plotting-in-r-a-ggplot2-cheatsheet-3working-with-the-background-colors","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.4.0.4 http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/#working-with-the-background-colors","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"the-author-is-zevzevross.com","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.4.0.5 The author is zev@zevross.com","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"analysis-is-important","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.5 Analysis is important!","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"during-the-first-half-of-semester-i-noticed-that-many-students-including-me-seem-to-have-some-misunderstanding-of-this-course.-we-should-always-remember-that-the-goal-of-data-vitualization-is-not-that-graph-but-what-we-conclude-from-that-graph.-the-reason-to-improve-the-graphs-is-not-writting-several-meaningless-words-as-your-final-analysis.-as-a-data-analyst-i-think-the-time-we-need-to-cost-on-analysis-the-graph-should-always-be-much-more-than-we-draw-it-even-youre-not-asked-to-do-so.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.5.0.1 During the first half of semester, I noticed that many students including me seem to have some misunderstanding of this course. We should always remember that the goal of data vitualization is not that graph but what we conclude from that graph. The reason to improve the graphs is not writting several meaningless words as your final analysis. As a data analyst, I think the time we need to cost on analysis the graph should always be much more than we draw it, even you’re not asked to do so.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"some-other-suggestions","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.6 Some other suggestions","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"although-this-is-a-course-based-on-r-i-still-think-learning-how-to-achieve-the-same-thing-in-python-is-very-useful.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.6.0.1 Although this is a course based on R, I still think learning how to achieve the same thing in python is very useful.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"when-youre-asked-to-find-a-partner-do-it-ealier-than-anyone-else.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.6.0.2 When you’re asked to find a partner, do it ealier than anyone else.","text":"","code":""},{"path":"a-brief-instruction-for-the-half-semester-of-edav5702.html","id":"when-youre-facing-with-some-problems-you-can-also-look-through-the-instructions-in-other-languages-therere-many-high-quality-journals-that-is-not-in-english.-as-an-example-google-tanslater-is-good-enough-to-make-me-understand-the-most-ideas-of-a-instruction-in-japanese.","chapter":"96 A brief instruction for the half semester of EDAV5702","heading":"96.6.0.3 When you’re facing with some problems, you can also look through the instructions in other languages, there’re many high quality journals that is not in English. As an example, Google tanslater is good enough to make me understand the most ideas of a instruction in Japanese.","text":"","code":""},{"path":"anova-tutorial.html","id":"anova-tutorial","chapter":"97 ANOVA tutorial","heading":"97 ANOVA tutorial","text":"Xinrui Bai Zewen ShiThe main motivation community contribution project provide tutorial useful visualization tools process analysis variances, common topic statisticians deal daily basis. project walk users tools visualization complement concrete test results making understandable even audiences little training statistics. first part, background typical problem choice provided manipulations data carried obtain model best fit later use. second part, diagnosis using visualization tools conducted results analyzed.","code":"\n#.libPaths(\"C:/Rpackage\")\nlibrary(car)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(tidyverse)\nlibrary(broom)\n#library(AICcmodavg)"},{"path":"anova-tutorial.html","id":"part-1-background-and-preliminary-data-manipulation","chapter":"97 ANOVA tutorial","heading":"97.1 Part 1: Background and Preliminary Data Manipulation","text":"Analysis variance (ANOVA) collection statistical models associated estimation procedures (“variation” among groups) used analyse differences among means. ANOVA developed statistician Ronald Fisher. ANOVA based law total variance, observed variance particular variable partitioned components attributable different sources variation. simplest form, ANOVA provides statistical test whether two population means equal, therefore generalizes t-test beyond two means.(Wikipedia)","code":""},{"path":"anova-tutorial.html","id":"step-1-taking-a-first-look-at-the-dataset","chapter":"97 ANOVA tutorial","heading":"97.1.1 Step 1: taking a first look at the dataset:","text":"Taking first look data, observed multicollinearity dataset VIF > 10. standard anova test, test group means significantly different others comparing variances. test fails, means least one group significant deviates overall mean.","code":"\ncrop.data <- read.csv(\"resources/anova_tutorial/cropdata.csv\", header = TRUE, colClasses = c(\"factor\", \"factor\", \"factor\", \"numeric\")) # if data is in the $home folder\n\nsummary(crop.data)##  density block  fertilizer     yield      \n##  1:48    1:24   1:32       Min.   :175.4  \n##  2:48    2:24   2:32       1st Qu.:176.5  \n##          3:24   3:32       Median :177.1  \n##          4:24              Mean   :177.0  \n##                            3rd Qu.:177.4  \n##                            Max.   :179.1"},{"path":"anova-tutorial.html","id":"step-2-one-way-anova","chapter":"97 ANOVA tutorial","heading":"97.1.2 Step 2: one-way anova","text":"tests null hypothesis, states samples groups drawn populations mean values. , two estimates made population variance. estimates rely various assumptions (see ). ANOVA produces F-statistic, ratio variance calculated among means variance within samples. group means drawn populations mean values, variance group means lower variance samples, following central limit theorem. higher ratio therefore implies samples drawn populations different mean values.\none-way ANOVA setup, test fertilizer type significant impact final crop yield. Since received p-value smaller 0.001, exists impact.","code":"\nanovaoneway <- aov(yield ~ fertilizer, data = crop.data)\n#diaplay the results\nsummary(anovaoneway)##             Df Sum Sq Mean Sq F value Pr(>F)    \n## fertilizer   2   6.07  3.0340   7.863  7e-04 ***\n## Residuals   93  35.89  0.3859                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova-tutorial.html","id":"step-3two-way-data","chapter":"97 ANOVA tutorial","heading":"97.1.3 Step 3:two-way data","text":"statistics, two-way analysis variance (ANOVA) extension one-way ANOVA examines influence two different categorical independent variables one continuous dependent variable. two-way ANOVA aims assessing main effect independent variable also interaction . two-way ANOVA setup, adding planting density reduced residual variance variables appear significant impact final crop yield since p-values small.","code":"\n#anova adjust for two independent variables\n#start to generate the dataset for two x variables\nanova2way <- aov(yield ~ fertilizer + density, data = crop.data)\nsummary(anova2way)##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## fertilizer   2  6.068   3.034   9.073 0.000253 ***\n## density      1  5.122   5.122  15.316 0.000174 ***\n## Residuals   92 30.765   0.334                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova-tutorial.html","id":"step-4-adding-interaction-terms","chapter":"97 ANOVA tutorial","heading":"97.1.4 Step 4: adding interaction terms","text":"step, exploit interaction two variables significant impact variance, test results high p-value proved .","code":"\n#generate the anova for the data\nanova_inter <- aov(yield ~ fertilizer*density, data = crop.data)\n#display the results\nsummary(anova_inter)##                    Df Sum Sq Mean Sq F value   Pr(>F)    \n## fertilizer          2  6.068   3.034   9.001 0.000273 ***\n## density             1  5.122   5.122  15.195 0.000186 ***\n## fertilizer:density  2  0.428   0.214   0.635 0.532500    \n## Residuals          90 30.337   0.337                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova-tutorial.html","id":"step-5-blocking-variable","chapter":"97 ANOVA tutorial","heading":"97.1.5 Step 5: blocking variable","text":"intuition behind step many crop yield studies, treatments applied within “blocks” field may influence objectivity test. effect difference tested adding third term, shown test result, block term statistically significant.","code":"\n#generate the anova for the data\nanova_block <- aov(yield ~ fertilizer + density + block, data = crop.data)\n#display the results\nsummary(anova_block)##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## fertilizer   2  6.068   3.034   9.018 0.000269 ***\n## density      1  5.122   5.122  15.224 0.000184 ***\n## block        2  0.486   0.243   0.723 0.488329    \n## Residuals   90 30.278   0.336                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova-tutorial.html","id":"step-6-obtaining-the-best-fitting-model","chapter":"97 ANOVA tutorial","heading":"97.1.6 Step 6: obtaining the best-fitting model","text":"plan use aictab() function AICcmodavg plackage obtain best-fitting model among four different ANOVA models.function creates model selection table based one following information criteria: AIC, AICc, QAIC, QAICc. table ranks models based selected information criteria also provides delta AIC Akaike weights. due version software issues. codes won’t work just display process . However can work days ago can display results.Since step, exploited several measures lock potential combinations variables figure best-fitting model dataset. finally chosen two way model best fitting model lowest AIC score. next part, run diagnosis tests model using concrete tests visualization tools.","code":"\n#library(AICcmodavg)\n\n#allanovas <- list(anovaoneway, anova2way, anova_inter, anova_block)\n#allanovasnames <- c(\"one way Anova\", \"two way Anova\", \"Anova wtih interaction term\", \"Anova with a blocking term\")\n\n#aictab(allanovas, modnames = allanovasnames)"},{"path":"anova-tutorial.html","id":"part-2-visualization-and-diagnosis","chapter":"97 ANOVA tutorial","heading":"97.2 Part 2: Visualization and Diagnosis","text":"","code":""},{"path":"anova-tutorial.html","id":"step-1-homoskedasticity-test","chapter":"97 ANOVA tutorial","heading":"97.2.1 Step 1: homoskedasticity test","text":"step, normal Q-Q plot gives us visual clues much dataset deviates normal distribution, much, confirmed shaprp-ratio test p-value 0.8836>0.05While interpreting first set graphs, usually important know red line represents mean residuals horizontal scattered around value 0 1 normality assumption hold, otherwise exists significant outliers bigger normal influence model, causing unneccessary bias.remedial measures can use Box-Cox transformation, fit Generalized Linear Model. may also use nonparametric robust procedures.","code":"\n#Comprehensive test\npar(mfrow=c(2,2))\nplot(anova2way)\npar(mfrow=c(1,1))\n\n#ggplot2 for Q-Q plot\nqqnorm(resid(anova2way));qqline(resid(anova2way), col = 2,lwd=2,lty=2)\n#histogram\nhist(resid(anova2way))"},{"path":"anova-tutorial.html","id":"step-2-parallelism-test","chapter":"97 ANOVA tutorial","heading":"97.2.2 Step 2: parallelism test","text":"P-value=0.353 >0.05, fail reject H0. significant interaction birthweight diet. inference marginal mean differences must performed.Remedial Measures: H0 rejected implies don’t parallelism, relationship becomes much complicated expect. best way just compare group means value x, like birth weight, individually.","code":"\n#check for parallelism\n#two-way anova\nanova_inter<-aov(yield ~ fertilizer*density, data = crop.data)\nsummary(anova_inter)##                    Df Sum Sq Mean Sq F value   Pr(>F)    \n## fertilizer          2  6.068   3.034   9.001 0.000273 ***\n## density             1  5.122   5.122  15.195 0.000186 ***\n## fertilizer:density  2  0.428   0.214   0.635 0.532500    \n## Residuals          90 30.337   0.337                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova-tutorial.html","id":"step-3-unequal-variances-test","chapter":"97 ANOVA tutorial","heading":"97.2.3 Step 3: Unequal Variances test","text":"Bartlett’s test sensitive departures normality. , samples come non-normal distributions, Bartlett’s test may simply testing non-normality. Levene test alternative Bartlett test less sensitive departures normality.Based Bartlett test, p-value 0.3, larger 0.05. fail reject null hypothesis, constant variance assumption satisfies.Based Levene’s test, p-value 0.38, larger 0.05. fail reject null hypothesis, constant variance assumption satisfies.Remedial Measures: can use weighted Least Squares,transform Y /X, fit Generalized Linear Mode solve problem.","code":"\n#bartlett test on data\nbartlett.test(x=crop.data$yield,g=crop.data$density)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  crop.data$yield and crop.data$density\n## Bartlett's K-squared = 0.16171, df = 1, p-value = 0.6876\n#levene test on data\nlibrary(car)\nleveneTest(y=crop.data$yield,group=as.factor(crop.data$density))## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  1   0.152 0.6975\n##       94"},{"path":"anova-tutorial.html","id":"step-4-post-hoc-test","chapter":"97 ANOVA tutorial","heading":"97.2.4 Step 4: post-hoc test","text":"scientific study, post hoc analysis (Latin post hoc, “”) consists statistical analyses specified data seen. typically creates multiple testing problem potential analysis effectively statistical test. Multiple testing procedures sometimes used compensate, often difficult impossible precisely. Post hoc analysis conducted interpreted without adequate consideration problem sometimes called data dredging critics statistical associations finds often spurious.ANOVA tests gives information whether group means significantly differ numerical value differences. get comprehensive results find much actually differ, conduct Tukey’s HSD post-hoc test. Based results, fertilizer types 2 3, fertilizer groups 1 3, two levels planting density demonstrated statistically significant differences.","code":"\ntukeytest<-TukeyHSD(anova2way)\n\ntukeytest##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = yield ~ fertilizer + density, data = crop.data)\n## \n## $fertilizer\n##          diff         lwr       upr     p adj\n## 2-1 0.1761687 -0.16822506 0.5205625 0.4452958\n## 3-1 0.5991256  0.25473179 0.9435194 0.0002219\n## 3-2 0.4229568  0.07856306 0.7673506 0.0119381\n## \n## $density\n##         diff       lwr       upr     p adj\n## 2-1 0.461956 0.2275205 0.6963916 0.0001741"},{"path":"anova-tutorial.html","id":"step-5-visualized-test-results","chapter":"97 ANOVA tutorial","heading":"97.2.5 Step 5: visualized test results","text":"shown graph, confidence interval include zero, means p-value difference smaller 0.05, evidence significant differences.","code":"\ntukey_plot<-aov(yield ~ fertilizer:density, data=crop.data)\ntukey.plot<-TukeyHSD(tukey_plot)\nplot(tukey.plot, las = 1,col = c(\"red\", \"blue\", \"green\", \"orange\"))"},{"path":"anova-tutorial.html","id":"step-6-creating-a-data-frame-with-the-group-labels-and-more-visualizations","chapter":"97 ANOVA tutorial","heading":"97.2.6 Step 6: creating a data frame with the group labels and more visualizations","text":"obtained preliminary visualization groups stacked top , makes difficult read. next step, separate .","code":"\nyielddata <- crop.data %>%\n  group_by(fertilizer, density) %>%\n  summarise(\n    yield = mean(yield)\n  )\n\n\n#mergeDay18_meanData, weight ~ birthweight + Diet\n\n\nyielddata$group <- c(\"a\",\"b\",\"b\",\"b\",\"b\",\"c\")\n\nyielddata## # A tibble: 6 × 4\n## # Groups:   fertilizer [3]\n##   fertilizer density yield group\n##   <fct>      <fct>   <dbl> <chr>\n## 1 1          1        176. a    \n## 2 1          2        177. b    \n## 3 2          1        177. b    \n## 4 2          2        177. b    \n## 5 3          1        177. b    \n## 6 3          2        178. c\nggplot(crop.data, aes(x = density, y = yield, group=density)) +\n  geom_boxplot() +\n  geom_jitter(shape = 15,\n              color = \"steelblue\",\n              position = position_jitter(0.21)) +\n  theme_classic()+\n  \n  ggtitle(\"Boxplot between different density of the crop data\")\nTwoway <- ggplot(crop.data, aes(x = density, y = yield, group=fertilizer, fill=fertilizer),color='darkblue') +\n  geom_point(cex = 1.5, pch = 1.0,position = position_jitter(w = 0.1, h = 0))+\n  \n  ggtitle(\"Two way plot of the crop data based on fertilizer\")+\n  scale_color_gradientn(colours = rainbow(5))\nTwoway\nplot1<- Twoway +\n  stat_summary(fun.data = 'mean_se', geom = 'errorbar', width = 0.2,color='blue') +\n  stat_summary(fun.data = 'mean_se', geom = 'pointrange',color='red') +\n  geom_point(data=yielddata, aes(x=density, y=yield, fill=density),color='green')+\n  \n  ggtitle(\"Two way plot of the crop data with means\")\nplot1\nplot2 <- Twoway +\n  geom_text(data=yielddata, label=yielddata$group, vjust = -8, size = 5) +\n  facet_wrap(~ fertilizer)+\n  \n  ggtitle(\"Two way plot of the crop data with full separation\")\n\nplot2\nplot3 <- Twoway +\n  theme_classic2() +\n  labs(title = \"Crop yield with different kinds of fertilizer and planting density\",\n       x = \"Planting density (0=low density, 1=high density)\",\n       y = \"Yield (bushels)\")\n\nplot3"},{"path":"anova-tutorial.html","id":"part-3-summary","chapter":"97 ANOVA tutorial","heading":"97.3 Part 3: Summary","text":"first part, background typical problem choice provided manipulations data carried obtain model best fit later use. second part, diagnosis using visualization tools conducted results analyzed. However, drifted away original purpose specifically focused visualization decided give step--step tutorial conduct ANOVA test specific instructions, opinion ANOVA test important topic statistics imperative readers understand statistical reasons behind every step can begin interpret visualization tools employed. working project, reinforced understanding visualization concrete mathematical tests work together help statisticians make decisions adjust models arrive certain conclusion.Referenceggplot2 colors : change colors automatically manually?: http://www.sthda.com/english/wiki/ggplot2-colors---change-colors-automatically--manually\nANOVA R: step--step guide, Published March 6, 2020 Rebecca Bevans. Revised July 1, 2021.https://www.scribbr.com/statistics/one-way-anova/\nAnalysis variance: https://en.wikipedia.org/wiki/Analysis_of_variance\nStats R:https://statsandr.com/blog/anova--r/","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"predictive-analytics-using-data-visualization-in-r","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98 Predictive Analytics using Data Visualization in R","text":"Rahulraj Singh","code":"\n# This external GitHub installation is needed to import the makeR library\n# require(devtools)\n#   install_github(\"jbryer/makeR\")"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"building-a-trading-strategy-and-an-optimal-fund-allocation-mechanism-powered-by-dataviz","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.1 Building a Trading Strategy and an optimal Fund Allocation Mechanism, powered by DataViz!","text":"tutorial , walk complete process using Data Visualization concepts R build trading strategy. Along way also introduce concepts Time Series analysis, data stationary perform predictions. obviously prerequisites understanding series R code.","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"lets-get-started-on-building-our-own-day-trading-guide.","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.1.1 Let’s get started on building our own day-trading guide.","text":"tutorial, taking example Apple Inc. stock (ticker: AAPL) (credits go Professor Kosrow Dehnad, introduced us AAPL Finance Structuring class)code chunk, gathering data AAPL, sourcing directly Yahoo, last 8 months. (Please note: trading algorithm can function optimally 6-8 months data. using algorithm build trading strategy, kindly use least 5 years worth data)","code":"\ndata <- getSymbols(\"AAPL\", src=\"yahoo\", from=\"2021-03-01\", to=\"2021-10-31\", auto.assign=FALSE)\ndf = data.frame(date = index(data), data, row.names=NULL)\nhead(df)##         date AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted\n## 1 2021-03-01    123.75    127.93   122.79     127.79   116307900      127.1968\n## 2 2021-03-02    128.41    128.72   125.01     125.12   102260900      124.5392\n## 3 2021-03-03    124.81    125.71   121.84     122.06   112966300      121.4934\n## 4 2021-03-04    121.75    123.60   118.62     120.13   178155000      119.5724\n## 5 2021-03-05    120.98    121.94   117.57     121.42   153766600      120.8564\n## 6 2021-03-08    120.93    121.00   116.21     116.36   154376600      115.8199"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"plots-to-study-the-trends-in-the-market-exploratory-data-analysis","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.1.2 Plots to study the trends in the market (Exploratory Data Analysis)","text":"next sections, walk plots study basic trends market revolving around AAPL.Scatter Plot: visual shows us closing prices stock last 5 months. can clearly observe April-May period hump resulted prices reach lowest May-June. highest closing can seen September post see gradual drop .Candlestick Charts: general financial analysis, candlestick charts used understand market’s sentiment. give day traders detailed reading stock performed every day turn speaks market’s sentiment regard stock. case Apple, see evenly distributed chart tells investors stable trading Apple. Except one day September 10, see wide gap high low.\ncharts created using plotly interactive.Scatter Plot: study amount day trades executed day--day basis, read scatter plot adjusted close prices. shows regular periodic rise stock price Apple.Bar Charts: Study Volume trades day-trade. data AAPL shows trading diversified show us trends patterns trade volumes, irrespective price. shows us important trade company’s stock, investor’s trust factored affected price stock, company.distinct plots used finance industry read understand movement market. Let’s walk charts.Bollinger Band: key analysis tool security Bollinger band. comprised three lines, simple moving average (middle band), upper lower band.\nsqueeze important indicator shown Bollinger band. bands come close, called squeeze. indicates moving average constricting. period low volatility indication future trading opportunities.\nApple’s case , see squeeze June September, best periods day-trade Apple stocks.Multi-Line Chart: graph bifurcated based daily price, weekly moving average monthly moving average. MA significant measure movement stock prices shows period volatility. observe month September, stock prices go strongly monthly moving average showing signs high volatility. periods potential trade opportunities day traders come “high risk high reward” note.Histogram: graph shows us return investing Apple last 6 years. rate important understand pattern growth company’s stock value. see rate return gradually improved recent years shows positive confidence stock.","code":"\nplot(df$AAPL.Close, main = \"Closing Prices of AAPL from April 2021\", ylab=\"Apple Close Price\", x=df$date, xlab=\"Date\")\ncs_chart <- df %>%\n  plot_ly(x = ~df$date, type=\"candlestick\",\n          open = ~df$AAPL.Open, close = ~df$AAPL.Close, high = ~df$AAPL.High, low = ~df$AAPL.Low) %>%\n    layout(title = 'Apple - Last 8 Months', plot_bgcolor = \"#e5ecf6\", xaxis = list(title = 'Dates'), \n         yaxis = list(title = 'Pricing Data'))\ncs_chart\nggplot(df, aes(x=date, y=AAPL.Adjusted)) + geom_point() + geom_smooth(method=lm) + xlab(\"Date\") + ylab(\"Volume of Shares\")\ndf %>%\n    ggplot(aes(x = date, y = AAPL.Volume)) +\n    geom_segment(aes(xend = date, yend = 0, color = AAPL.Volume)) + \n    geom_smooth(method = \"loess\", se = FALSE) +\n    labs(title = \"AAPL Volume Chart\", \n         subtitle = \"Charting Daily Volume\", \n         y = \"Volume\", x = \"\") +\n    theme_tq() +\n    theme(legend.position = \"none\") \nchartSeries(data, name='AAPL (From April 2021 to October 2021)')\naddMACD() \naddBBands() \ndf$ma7 = ma(df$AAPL.Adjusted, order=7)\ndf$ma30 = ma(df$AAPL.Adjusted, order=30)\nggplot() +\n    geom_line(data = df, aes(x = date, y = AAPL.Adjusted, color = \"Daily Price\")) +\n    geom_line(data = df, aes(x = date, y = ma7,   color = \"Weekly Moving Average\"))  +\n    geom_line(data = df, aes(x = date, y = ma30, color = \"Monthly Moving Average\"))  +\n    ylab('AAPL Stock Price') +\n    xlab('Time') +\n    labs(color = 'Trendline')\nreturns <- function(ticker, start_year) \n  {\n    symbol <- getSymbols(ticker, src = 'yahoo', auto.assign = FALSE, warnings = FALSE)\n    data <- periodReturn(symbol, period = 'monthly', subset=paste(start_year, \"::\", sep = \"\"), type = 'log')\n    assign(ticker, data, .GlobalEnv)\n  }\nre = returns('AAPL', 2015)\nhist(re, main= \"Return on Investment for AAPL\", xlab=\"Rate of Return\")\nlines(density(re), col = \"red\")"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"implementation-of-prdictive-analytics-in-stock-prices","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.2 Implementation of Prdictive Analytics in Stock Prices","text":"","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"what-is-predictive-analytics-and-how-does-it-come-up-in-trading-strategies","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.2.1 What is Predictive Analytics and how does it come up in Trading Strategies?","text":"words, science assimilating future behavior object Predictive Analytics. ’s application Finance heavy, since traders base strategies study market forecast. often concrete output trading strategy can ever based algorithm, predictions help building cognitive strategies.Forecasting time series deals predicting next cycle observation events occur future-referenced time frame.","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"augmented-dickey-fuller-test-for-stationarity","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.2.1.1 Augmented Dickey-Fuller Test for Stationarity","text":"Augmented Dickey-Fuller test (ADF), also known Ad-Fuller Test important testing mechanism respect Time Series Analysis. tests basic null hypothesis given input unit root present entire time series sample. alternative hypothesis usually stationarity trend stationarity series.augmented Dickey–Fuller (ADF) statistic, predominantly used test, negative number. negative , stronger rejection hypothesis unit root level confidence.inspiration behind Ad-Fuller test , time series characterized unit root process, case lagged level series (y-1) yield relevant information predicting future changes (y) except changes observed delta (y-1). cases, null hypothesis rejected. side, existing process unit root, states stationary therefore shows reversion mean. , lagged level provide relevant information forecasting future changes.","code":"\nprint(adf.test(df$AAPL.Close))## \n##  Augmented Dickey-Fuller Test\n## \n## data:  df$AAPL.Close\n## Dickey-Fuller = -1.952, Lag order = 5, p-value = 0.5963\n## alternative hypothesis: stationary"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"seasonality-in-time-series-data","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.2.1.2 Seasonality in Time Series Data","text":"Seasonality time series data presence variations occur specific intervals time less year. intervals can hourly, monthly, weekly, quarterly. Various considerations weather climatic changes can cause seasonality.\nPatterns causing Seasonality always cyclic occur periodic patterns.can see, seem apparent periodicity Apple stock price trades 8 months time period.","code":"\ndf$ma7 = ma(df$AAPL.Adjusted, order=7)\nprice_ma = ts(na.omit(df$ma7), frequency=30)\ndecomp = stl(price_ma, s.window=\"periodic\", robust = TRUE)\nplot(decomp)"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"autocorrelation-and-partial-correlation","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.2.1.3 Autocorrelation and Partial Correlation","text":"Autocorrelation: correlation series lags called autocorrelation. Like correlation measures magnitude linear relationship associated two variables, autocorrelation lagged values time series. series considerably autocorrelated, means impact lags forecasting time series high. general terms, correlation factor 1 (Lag = 1) conveys correlation values one period apart . Likewise, lag ‘k’ autocorrelation shows association amongst values ‘k’ time periods separately .Partial Autocorrelation: purpose partial autocorrelation also like autocorrelation conveys information relationship variable lag. partial autocorrelation delivers details pure association lag disregards correlations occur intermediate lags.reading data ACF plot Apple, see gradual decay lags. decay indication significant closing values time interval confirming hypothesis Augmented Dickey Fuller test correct.reading PACF plot Apple, notice lag order high start moves geometric progression throughout timeline. periodic changing movements lag PACF plot confirm seasonal component data changing prices can used build ARIMA model. flat line across time frames, data yielded results ARIMA forecast.","code":"\npar(mar=c(5,6,7,8))\ndf = data.frame(date = index(data), data, row.names=NULL)\nseries = df$AAPL.Close\nAcf(\n  series,\n  lag.max = NULL,\n  type = c(\"correlation\", \"covariance\", \"partial\"),\n  plot = TRUE,\n  na.action = na.contiguous,\n  demean = TRUE,\n  main = \"ACF Series Plot\"\n)\npar(mar=c(5,6,7,8))\ndf = data.frame(date = index(data), data, row.names=NULL)\nseries = df$AAPL.Close\nPacf(\n  series,\n  lag.max = NULL,\n  plot = TRUE,\n  na.action = na.contiguous,\n  demean = TRUE,\n  main = \"PACF Series Plot\"\n)"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"arima---autoregresive-integrating-moving-average","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.3 ARIMA - Autoregresive Integrating Moving Average","text":"general term statistics economics world, auto regressive integrated moving average (ARIMA) used describe moving average construct. found great interest building Time Series forecasting models.ARIMA models applied instances input data shows evidence non-stationarity sense mean, significant variance observed.can break definition ARIMA follows, AR part ARIMA stands evolving variable regressed self-lag (old values). MA part shows regression error linear combination error terms occurred various times past. remaining “” (integrated) indicated data got replaced differences current previous values. individual processes may performed multiple times. purpose features make model fit data well possible.Now model ready, forecast prices next 60 days.prediction can read follows:\n1. blue line represents mean prediction.\n2. darker shaded region around blue line represents 80% confidence interval.\n3. light shaded outer portion represents 95% confidence interval.","code":"\nmodelfit <- auto.arima(df$AAPL.Close, stepwise = TRUE, lambda = NULL)\nmodelfit## Series: df$AAPL.Close \n## ARIMA(3,1,1) with drift \n## \n## Coefficients:\n##           ar1    ar2      ar3     ma1   drift\n##       -0.4559  6e-04  -0.1774  0.3877  0.1373\n## s.e.   0.3188  9e-02   0.0890  0.3208  0.1201\n## \n## sigma^2 estimated as 3.481:  log likelihood=-344.79\n## AIC=701.58   AICc=702.09   BIC=720.39\nprice_forecast <- forecast(modelfit, h=60)\nplot(price_forecast)"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"comparing-predictions-with-actual-stock-prices","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.4 Comparing predictions with Actual Stock Prices","text":"conclude predictive analytics tutorial, let us match generated predictions real stock closing prices October 10 October 31, 2021.matching see mean predictions extremely close actual value moving forward time. definitely instill confidence within us way structured analysis predictions. One important thing note prediction next 30 days based data last 8 months. Therefore, previoud entries going match. converge towards end graph, numbers start coming pattern movement, positive sign.","code":"\ndata2 <- getSymbols(\"AAPL\", src=\"yahoo\", from=\"2021-10-10\", to=\"2021-10-31\", auto.assign=FALSE)\ndf2 = data.frame(date = index(data2), data2, row.names=NULL)\npresent <- as.vector(df2$AAPL.Close)\nmeanvalues <- as.vector(price_forecast$mean)\n\nx  <- df2$date\ny1 <- present\ny2 <- meanvalues[15:1]\ndf_new <- data.frame(x,y1,y2)\n\ng <- ggplot(df_new, aes(x))\ng <- g + geom_line(aes(y=y1), colour=\"red\")\ng <- g + geom_line(aes(y=y2), colour=\"green\")\ng <- g + ylab(\"Y\") + xlab(\"X\")\ng"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"building-a-portfolio-of-stocks","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.5 Building a Portfolio of Stocks","text":"","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"how-to-decide-what-percentage-of-your-portfolio-should-be-invested-where","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.5.1 How to decide what percentage of your portfolio should be invested where?","text":"managing finances, common knowledge diversification yields better results. idea put eggs one basket. decide baskets choose allocate funds?","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"sharpe-ratio","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.5.2 Sharpe Ratio","text":"important construct deciding best outcome trading one stock calculating sharpe ratio. tells pair percentage fund allocation stock yield best outcome.function complete prerequisite steps calculate Sharpe Ratio:\n1. Bring data 3 stocks, historical data last 6 years.\n2. Calculate respective monthly return rates summarize stock gave returns.\n3. Merge return one data frame divide portions portfolio.tutorial, took data Apple, Pfizer Tesla. stocks news various reasons last months, let’s see investment look want invest stocks.interactive line chart shows us individual return rates three stocks. evident Tesla volatile stock three. grown largest, times also shed losses large numbers. moment, looks like investing TSLA yield high results comes high risk-ratio well.starting calculation Sharpe Ratio, split portfolio follows:\n1. Apple - 25%\n2. Pfizer - 25%\n3. Tesla - 50%division instinctive come sort calculation. free experiment based much risk like add portfolio. Modification values result changes Sharpe Ratio.Now, purpose analysis, imagine invested one dollar portfolio based allocations mentioned . dygraph shows return investment 1 dollar portfolio. can observe superior rise post 2020 goes upto 6-fold increase overall returns.","code":"\nreturns <- function(ticker, start_year) {\n  symbol <- getSymbols(ticker, src = 'yahoo', auto.assign = FALSE, warnings = FALSE)\n  data <- periodReturn(symbol, period = 'monthly', subset=paste(start_year, \"::\", sep = \"\"), type = 'log')\n  colnames(data) <- as.character(ticker)\n  assign(ticker, data, .GlobalEnv)\n}\n\nreturns('AAPL', 2015)\nreturns('PFE', 2015)\nreturns('TSLA', 2015)\n\nmerged_returns <- merge.xts(AAPL, PFE, TSLA)\ndygraph(merged_returns, main = \"Apple v/s Pfizer v/s Tesla\") %>%\n  dyAxis(\"y\", label = \"Return Percentage\") %>%\n  dyOptions(colors = RColorBrewer::brewer.pal(3, \"Set2\"))\npercAlloc <- c(.25, .25, .50)\nportfolio_returns <- Return.portfolio(merged_returns, weights = percAlloc)\ndygraph(portfolio_returns, main = \"Portfolio Monthly Return\") %>%\n  dyAxis(\"y\", label = \"Return Percentage\")\ndollar_growth <- Return.portfolio(merged_returns, weights = percAlloc, wealth.index = TRUE)\ndygraph(dollar_growth, main = \"Growth of $1 Invested in Portfolio\") %>%\n  dyAxis(\"y\", label = \"Dollor Return on 1$ investment\")"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"portfolio-sharpe-ratio","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.5.3 Portfolio Sharpe Ratio","text":"Finally, let’s calculate Sharpe Ratio Portfolio","code":"\nsharpe_ratio <- round(SharpeRatio(portfolio_returns, Rf = .0003), 4)\nsharpe_ratio##                               portfolio.returns\n## StdDev Sharpe (Rf=0%, p=95%):            0.2642\n## VaR Sharpe (Rf=0%, p=95%):               0.2528\n## ES Sharpe (Rf=0%, p=95%):                0.1983"},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"improving-the-sharpe-ratio","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.5.4 Improving the Sharpe Ratio","text":"score better?see 0.26 score Sharpe Ratio’s variance considered good. , ideal portfolio allocation score maximized. maximize score multiple ways. Either can export data Excel run -analysis find percentage allocation maximize sharpe ratio. code, manually change percentage allocation ‘percAlloc’ variable created . higher score, better portfolio.","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"conclusion-summary-and-learnings","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.6 Conclusion, Summary and Learnings","text":"Data Visualization can co-exist easy complicated. , important tool data analytics project life cycle. project aimed creating stock price predictor R, simply studying stock price data building predictions ARIMA. Another important take away project importance data visualization art storytelling. Crunching numbers bringing predictions deliver message visualizations .hope way helpful.\nP.S. Please consider investment advice!","code":""},{"path":"predictive-analytics-using-data-visualization-in-r.html","id":"references-8","chapter":"98 Predictive Analytics using Data Visualization in R","heading":"98.7 References","text":"https://www.analyticsvidhya.com/blog/2020/11/stock-market-price-trend-prediction-using-time-series-forecasting/https://www.kdnuggets.com/2020/01/stock-market-forecasting-time-series-analysis.htmlhttps://towardsdatascience.com/time-series-forecasting-predicting-stock-prices-using--arima-model-2e3b3080bd70https://rpubs.com/kapage/523169","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"color-selection-for-ggplot-graphs","chapter":"99 Color selection for ggplot graphs","heading":"99 Color selection for ggplot graphs","text":"Raphaël Adda Deepesh TheruvathHow chose colors RWhen plotting something, choice colors important. R offers huge number ways determine colors use. However, lot R users really know choose .motivation behind cheat sheet presents different methods choosing colors R.use histogram illustrate chose colorsClassic naming colors\nsimplest way determine colors plot call color name:ExampleColors number\nmethod selecting colors based numbers defined REach number greater 0 related color:\ngraph showing link numbers colors:, number 1 8 corresponds color. number greater 8, color just color rest euclidiean division per 8 (number divisible 8, color corresponds color 8 (grey)Using primary coloursThe method presented relatively easy restrictive. can use 8 colors plots, sophisticated figures may require colors.\ncolors composed mix three primary colors: red, green, blue. screens, pixel characterized superposition three colors: one corresponds vector size three, vector input intensity Red, Green, Blue colors (RGB).\ninstance, White color superposition Red, Green, Blue, , thanks R function RGB, applied vector (1,1,1), obtain white color.\nvector (1/2,0,1/2), obtain mix red blue colors maximum intensity: thus get kind Purple.Thus, thanks RGB function, can reproduce color want.Mixing colours\nprevious section handy can obtain every color. However, people prefer mix color.\nfunction present allows us mix two different colors:\ncolorRamp takes argument vector two colors returns function: function takes float argument p 0 1 returns return mix two colors (proportion (1-p) color 1 p color 2), coded using RGB colour model. However, color coded 0 255 need divide 255 obtain RGB vector number 0 1 obtain vector usable rgb function presented .\nexample: want 90% pink 10% green mix fill histogram.Creating differents shades colorsAll previous methods help express colors R. However; require knowledge colors. people don’t know reproduce colors using RGB color model mixing two colors, simpler way exists determine colors.\nRColorBrewer allows creation different shades color visualization .\nbrewer. pal function takes two arguments: integer n set colors, returns vector n shades set.\nexample: want use purple plot, just need display different shades set “Purples” choose one interests us ., assume 5th shades purple one want use. Thus, correspond “brewer.pal(10,”Purples“)[5]”.order know set colors can use, function “display.brewer.()” displays set colors different colors can use.#Data visualization ranking RJingyuan Chen Ling SunThis tutorial aimed help people get good start ranking analysis making use data visualization tools. making tutorial, hope beginner students can learn use interesting visually appealing plots conventional bar chart showing ranking data. reorganize simplify existing reference materials, add tips discussions hope providing specialized tutorial ranking visualization. choose introduce lollipop charts, circular bar charts, word clouds, radar charts, heat maps discussion.","code":"\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7)\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7,fill = \"grey\",col =\"white\")\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7,fill = \"black\",col =\"blue\")\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7,fill = 3,col = 1)\nx = seq(1,16)\ny = rep(0,16)\nplot(x, y, col = x, pch = 19)\naxis(1, at = 1:20)\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7,fill = rgb(1,1,1),col = rgb(0.5,0,0.5))\npal <- colorRamp(c(\"pink\", \"green\"))\nmixed_col = pal(0.1)\nmixed_col##       [,1]  [,2]  [,3]\n## [1,] 229.5 198.3 182.7\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7,fill = rgb(mixed_col/255),col = \"white\")\ndisplay.brewer.pal(9,\"Purples\")\nfill_color = brewer.pal(9,\"Purples\")[5]\nggplot(mtcars,aes(x=mpg))+\n  geom_histogram(bins=7,fill = fill_color,col = \"white\")\ndisplay.brewer.all()\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(wordcloud2)\nlibrary(fmsb)\nlibrary(heatmaply)"},{"path":"color-selection-for-ggplot-graphs.html","id":"lollipop-charts","chapter":"99 Color selection for ggplot graphs","heading":"99.0.1 Lollipop Charts","text":"lollipop chart hybrid form chart bar chart Cleveland dot plot. lollipop chart typically contains categorical variables y-axis measured second (continuous) variable x-axis. Similar Cleveland dot plot, emphasis dot draw readers attention specific x-axis value achieved category. line meant minimalistic approach easily tie category relative point without drawing much attention line . lollipop chart great comparing multiple categories aids reader aligning categories points minimizes amount ink graphic.","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"tutorial---ggplot2","chapter":"99 Color selection for ggplot graphs","heading":"99.0.1.1 Tutorial - ggplot2","text":"Input data:data frame contains categorical variable continuous variable.Library:tutorial focuses build lollipop chart ggolot2 library, can easily used substitute conventional bar chart.","code":"\n#Create a data frame as an example\nname = letters[1:10]\nscore = c(40,90,37,39,35,22,28,29,34,21)\ndf = data.frame(name, score)\n\nggplot(data=df, aes(x = name, y = score, color=name))+\n  geom_point(size=5)+\n  theme_bw()+\n  labs(title = \"Lollipop Chart\", x = \"Name\", y = \"score\")+\n  geom_segment(aes(x=name,xend=name,y=0,yend=score), size = 2)"},{"path":"color-selection-for-ggplot-graphs.html","id":"circular-bar-plots","chapter":"99 Color selection for ggplot graphs","heading":"99.0.2 Circular Bar plots","text":"Circular barplot variation conventional bar chart. name suggests, Circular barplot visually appealing, must also used extra care circular barplot uses polar rather Cartesian coordinates, means category share Y-axis. Circular bar charts great choice ranking periodic data.","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"tutorial---ggplot2-1","chapter":"99 Color selection for ggplot graphs","heading":"99.0.2.1 Tutorial - ggplot2","text":"Input data:data frame contains categorical variable continuous variable.Library:tutorial focuses build circular barplot ggolot2 ‘tidyverse’ library, can easily used substitute conventional bar chart.","code":"\n# Create a data frame as an example\ndata = data.frame(id=seq(1,60), \n                  individual=paste( \"Candidate\", seq(1,60), sep=\"\"),\n                  value=sample( seq(10,100), 60, replace=T))\n\n# Then we get the name and the y position of each label\nlabel_data = data\n \n# calculate the ANGLE of each labels\nnumber_of_bar = nrow(label_data)\nangle =  90 - 360 * (label_data$id-0.5) /number_of_bar     \n# Here, we substract 0.5 because the letter must have the angle of the center of the bars so that we can avoid extreme right(1) or extreme left (0).\n \n# calculate the alignment of labels: right or left\n# If I am on the left part of the plot, my labels have currently an angle < -90\nlabel_data$hjust = ifelse( angle < -90, 1, 0)\n \n# flip angle BY to make them readable\nlabel_data$angle = ifelse(angle < -90, angle+180, angle)\n \n \n# Start the plot\nplot = ggplot(data, aes(x=as.factor(id), y=value)) +\n  \n  # This add the bars with a blue color\n  geom_bar(stat=\"identity\", fill=alpha(\"skyblue\", 0.7)) +\n  \n  # Limits of the plot = very important. The negative value controls the size of the inner circle, the positive one is useful to add size over each bar\n  ylim(-100,120) +\n  \n  # Custom the theme: no axis title and no cartesian grid, Adjust the margin to make in sort labels are not truncated!\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid = element_blank(),\n    plot.margin = unit(rep(-1,4), \"cm\")) +\n  \n  # This makes the coordinate polar instead of cartesian.\n  coord_polar(start = 0) +\n\ngeom_text(data=label_data, aes(x=id, y=value+10, label=individual, hjust=hjust), color=\"black\", fontface=\"bold\",alpha=0.6, size=2.5, angle= label_data$angle, inherit.aes = FALSE ) \n\nplot"},{"path":"color-selection-for-ggplot-graphs.html","id":"word-cloud","chapter":"99 Color selection for ggplot graphs","heading":"99.0.3 Word cloud","text":"Word cloud visualization technique shows frequent words text letting size words represents frequency.","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"tutorial---wordcloud2","chapter":"99 Color selection for ggplot graphs","heading":"99.0.3.1 Tutorial - Wordcloud2","text":"Input data:data frame including word frequency column, can obtained text mining via script function.Library:tutorial focuses build Word Cloud Wordcloud2 library, convenient options.Options:","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"example-and-tips","chapter":"99 Color selection for ggplot graphs","heading":"99.0.3.2 Example and Tips","text":"Tips:\nsyntax Wordcloud2 complicated, takes lot work create aesthetically pleasing word cloud suitable color, font, shape, details.","code":"\n#demoFreq is a data frame whose first column is word and second column shows the corresponding frequency.\nhead(demoFreq)##          word freq\n## oil       oil   85\n## said     said   73\n## prices prices   48\n## opec     opec   42\n## mln       mln   31\n## the       the   26\nwordcloud2(data = demoFreq, \n           size = 2, \n           color = \"random-light\", \n           backgroundColor = \"grey\", \n           fontFamily = \"Arial\", \n           fontWeight = 'normal',\n           minRotation = -pi/6, \n           maxRotation = -pi/6,\n           minSize = 10,\n           rotateRatio = 0.5,\n           shape = \"diamond\")"},{"path":"color-selection-for-ggplot-graphs.html","id":"radar-charts","chapter":"99 Color selection for ggplot graphs","heading":"99.0.4 Radar Charts","text":"approach complex ranking tasks, frequently rankings expected reflect performances multiple dimensions together. easy job ground truth weight put dimension. Radar charts tool can provide overview performances help us get start experiments producing ideal ranking. section, see visualize compare two students’ academic performances radar charts.","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"tutorial---fmsb","chapter":"99 Color selection for ggplot graphs","heading":"99.0.4.1 Tutorial - fmsb","text":"Input data:row represents entity. column quantitative variable. Note first 2 rows provide minimum maximum values allowed variable.Library:section, using fmsb library build radar charts.Options:","code":"\n#create data - student scores\ndata <- as.data.frame(matrix( c(89,85,60,66,73,90,64,66,81,72) , ncol=10))\ncolnames(data) <- c(\"math\" , \"english\" , \"biology\" , \"music\" , \"R-coding\", \"data-viz\" , \"french\" , \"physic\", \"statistic\", \"sport\" )\n \n#add the max and min of each variable\ndata <- rbind(rep(100,10) , rep(0,10) , data)\n\n#parameters for arranging two plots side by side\npar(mfrow = c(1, 2))\n\n#default radar chart \nradarchart(data, seg = 5, title = 'default radar chart')\n\n#helper function to produce a customized radar chart\ncustomize_radarchart <- function(data, color = \"#00AFBB\", \n                                        vlabels = colnames(data), vlcex = 1,\n                                        caxislabels = NULL, title = NULL, ...){\n  radarchart(\n    data, axistype = 1, seg = length(caxislabels)-1,\n    # Customize the polygon\n    pcol = color, pfcol = scales::alpha(color, 0.5), plwd = 2, plty = 1,\n    # Customize the grid\n    cglcol = \"grey\", cglty = 1, cglwd = 0.8,\n    # Customize the axis\n    axislabcol = \"grey\", \n    # Variable labels\n    vlcex = vlcex, vlabels = vlabels,\n    caxislabels = caxislabels, title = title, ...\n  )\n}\n\n#customized radar chart \ncustomize_radarchart(data, caxislabels = c(0, 20, 40, 60, 80, 100), title = 'customized radar chart')"},{"path":"color-selection-for-ggplot-graphs.html","id":"tips-for-users","chapter":"99 Color selection for ggplot graphs","heading":"99.0.4.2 Tips for Users","text":"Radar charts good comparing overall performances multi-dimensional objects, circular layout makes harder read values exactly. example, obvious subject, math data-viz, student achieved best score. single vertical horizontal axis still efficient way compare quantitative values. One solution supplementing radar charts single axis plots, lollipop plots.Furthermore, radar charts can misleading. Readers may feel led focus highlighted polygon. However, shape highly depends ordering categories around plot. changing category ordering, can produce different plots.Comparing areas also gives rise -evaluation differences, area ploygon increases quadratically edges increase. another example, one student scored 40 every subject another student scored 80 , polygon right looks four times large left one.","code":"\n#create data - student scores\ndata <- as.data.frame(matrix( c(40,40,40,40,40,40,40,40,40,40) , ncol=10))\ncolnames(data) <- c(\"math\" , \"english\" , \"biology\" , \"music\" , \"R-coding\", \"data-viz\" , \"french\" , \"physic\", \"statistic\", \"sport\" )\n \n#add the max and min of each variable\ndata <- rbind(rep(100,10) , rep(0,10) , data)\n\n#parameters for arranging two plots side by side\npar(mfrow = c(1, 2))\n\n#default radar chart\ncustomize_radarchart(data, caxislabels = c(0, 20, 40, 60, 80, 100), title = 'student 1')\n\n#create data - student scores\ndata <- as.data.frame(matrix( c(80,80,80,80,80,80,80,80,80,80) , ncol=10))\ncolnames(data) <- c(\"math\" , \"english\" , \"biology\" , \"music\" , \"R-coding\", \"data-viz\" , \"french\" , \"physic\", \"statistic\", \"sport\" )\n \n#add the max and min of each variable\ndata <- rbind(rep(100,10) , rep(0,10) , data)\n\n#reordered catogories\ncustomize_radarchart(data, caxislabels = c(0, 20, 40, 60, 80, 100), title = 'student 2')"},{"path":"color-selection-for-ggplot-graphs.html","id":"heat-maps","chapter":"99 Color selection for ggplot graphs","heading":"99.0.5 Heat Maps","text":"Rankings systematic approach represent datasets. take another step closer goal, heat map can provide inspirations. name , heat maps useful detect possible outliers can mess rankings. Clustering analysis based heat map visualizations also helpful ranking adjustment move move certain groups entities achieve better ranking evaluation results. section show perform heat map visualizations help boost ranking performances.","code":""},{"path":"color-selection-for-ggplot-graphs.html","id":"tutorial---heatmap","chapter":"99 Color selection for ggplot graphs","heading":"99.0.5.1 Tutorial - heatmap()","text":"Input data:Heatmaps work best continuous data. row represents entity. column continuous-valued attribution.Library:like introduce base R heatmap() handy tool simple syntax supports useful features hierarchical clustering trees customized representations, can help ranking adjustment.Options:","code":"\n#import data\ncars_data <- as.matrix(mtcars)\n\n#one-line code for producing heatmap\nheatmap(cars_data, scale = \"column\", col = hcl.colors(50), Colv = NA, xlab=\"Variable\", ylab=\"Car Model\", main=\"base R heatmap\", margin = c(5,7)) "},{"path":"color-selection-for-ggplot-graphs.html","id":"interactive-heat-maps","chapter":"99 Color selection for ggplot graphs","heading":"99.0.5.2 Interactive Heat Maps","text":"Library:section, show use heatmaply, interactive clustering heat map library built upon plotly. provide short piece example code . Users free explore beyond tutorial.Options:Discussion:allowing readers focus different parts interest, interactive plots great enhancing communication generating new ideas. plotly also powerful tool produce interactive plots, including interactive heat maps, support features shown . heatmaply specialized library maintains useful features seen base R heatmap().","code":"\n##import data\ncars_data <- as.matrix(mtcars)\n\nheatmaply(cars_data, \n        dendrogram = \"row\",\n        scale = \"column\",\n        xlab = \"Feature\", ylab = \"Car Model\", \n        main = \"Interactive heatmap\",\n        margins = c(60,100,40,20),\n        grid_color = \"white\",\n        grid_width = 0.00001,\n        titleX = FALSE,\n        hide_colorbar = TRUE,\n        branches_lwd = 0.1,\n        label_names = c(\"Car Model\", \"Feature\", \"Value\"),\n        fontsize_row = 5, fontsize_col = 5,\n        labCol = colnames(cars_data),\n        labRow = rownames(cars_data),\n        heatmap_layers = theme(axis.line=element_blank())\n        )"},{"path":"color-selection-for-ggplot-graphs.html","id":"references-9","chapter":"99 Color selection for ggplot graphs","heading":"99.0.6 References","text":"Dawei Lang Guan-tin Chien (2018) Wordcloud2 introduction https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.htmlTal Galili Alan O’Callaghan (2021) Introduction heatmaply https://cran.r-project.org/web/packages/heatmaply/vignettes/heatmaply.htmlUC R Programming (2016) Lollipop Charts https://uc-r.github.io/lollipopYan Holtz (2018) Data Viz https://www.data--viz.com/","code":""},{"path":"useful-resources-for-r-beginners.html","id":"useful-resources-for-r-beginners","chapter":"100 Useful resources for R beginners","heading":"100 Useful resources for R beginners","text":"Freddy Wong\nmade simple pdf R beginners kick start self learninghttp://github.com/ww2615/tips/blob/main/selflearntipsforRbeginers.pdf","code":""},{"path":"tutorial-on-using-mongodb-with-r.html","id":"tutorial-on-using-mongodb-with-r","chapter":"101 Tutorial on Using MongoDB with R","heading":"101 Tutorial on Using MongoDB with R","text":"Yiran Shu (ys3373)","code":"\nlibrary(mongolite)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(reshape2)\nlibrary(stats)"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"overview-8","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.1 Overview","text":"Note: knit Rmd file, first install MongoDB Compass add data database. 2 steps described part 3 part 4.tutorial talks use MongoDB database system R. introduce install MongoDB database system mongolite package, perform CRUD operations mongolite package visualize data retrieved MongoDB database.","code":""},{"path":"tutorial-on-using-mongodb-with-r.html","id":"introduction-22","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.2 Introduction","text":"MongoDB popular NoSQL database system. Different relational database management systems, MongoDB document database used build highly available scalable internet applications [1]. MongoDB stores data collections (similar tables MySQL). collection composed documents (similar records MySQL). document like JSON-format, consisting attribute-value pairs, real data entities. MongoDB flexible schema approach, approariate store semi-structured unstructured data. Since data usually semi-structured unstrcutured practice, MongoDB popular database management system web applications.MongoDB CRUD operations can learned https://docs.mongodb.com/manual/crud/. easy learn use. Furthermore, MongoDB Compass powerful desktop tool performing operations visualizing MongoDB database. Now MongoDB useful databases data analysis, like introduce use R.","code":""},{"path":"tutorial-on-using-mongodb-with-r.html","id":"installation-7","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.3 Installation","text":"link download MongoDB Compass: https://www.mongodb.com/try/download/compassTo use MongoDb R, need install mongolite package. recent R Mongo driver others deprecated. use package [2].install.packages(\"mongolite\")","code":""},{"path":"tutorial-on-using-mongodb-with-r.html","id":"add-data-to-the-database","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.4 Add Data to the Database","text":"took video step Youtube: https://www.youtube.com/watch?v=BEu8rcslJv8 reference.insert data MongoDB database system, need first create database. “Create Database” button MongoDB Compass. Click create database collection. created database called school collection called students.Creaste DatabaseI used data https://github.com/ozlerhakan/mongodb-json-files/blob/master/datasets/students.json. just modified dataset little. attached dataset file \nused data https://github.com/ozlerhakan/mongodb-json-files/blob/master/datasets/students.json. just modified dataset little. attached modified dataset file submission, resources/mongodb_and_r/dataset_editted.txt.insert documents students collection, copy text resources/mongodb_and_r/dataset_editted.txt suppose content. , open \"_MongoSH Beta\" console input MongoDB commands db.students.insertMany(content) add documents collection just created. content documents want insert. collection going use. insertMany commands can insert multiple documents time. Refer https://docs.mongodb.com/manual/tutorial/insert-documents/ information insertMany command.collection looks like inserting documents:Collection AppearanceObviously, dataset grades student class.","code":""},{"path":"tutorial-on-using-mongodb-with-r.html","id":"connection-2","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.5 Connection","text":"absolutely need connect database perform operations collection. need connection string, like url. connection string can copied MongoDB Compass. indicate database name collection name connect.Connection StringIn order check connection successful, output number documents school collection.200 documents school collection total, indicates connection successful.","code":"\nconnection_string = 'mongodb://127.0.0.1:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false'\nstudent_collection = mongo(collection=\"students\", db=\"school\", url=connection_string)\nstudent_collection$count()"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"retrieve-data-from-mongodb","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.6 Retrieve Data from MongoDB","text":"Now connected MongoDB database collection school. can use find command retrieve data MongoDB database. Refer https://docs.mongodb.com/manual/reference/method/db.collection.find/ information find command.want show first 5 students highest exam scores, can use following codeIf want show students whose exam score higher 80 quiz score higher homework score, can use MongoDB $expr (expressions) compare two fields document [2].","code":"\nstudent_collection$find(sort='{\"exam\" : -1}', limit=5, fields='{}')\nstudent_collection$find('{\"exam\":{\"$gt\":80}, \"$expr\": {\"$gt\": [\"$quiz\",\"$homework\"]}}')"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"other-crud-operations-with-mongolite-3","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.7 Other CRUD Operations with Mongolite [3]","text":"aaddition retrieving data MongoDB database mongolite package, can also use perform CRUD operations.","code":""},{"path":"tutorial-on-using-mongodb-with-r.html","id":"insert-command","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.7.1 Insert Command","text":"example, want add student named “Yiran”, whose exam score 93.3, quiz score 86.1 homework score 90.1, can use insertcommand. Refer https://docs.mongodb.com/manual/reference/insert-methods/ information insert command.can now retrieve document just inserted.","code":"\nstudent_collection$insert(c('{ \"name\" : \"Yiran\", \"exam\" : 93.3, \"quiz\" : 86.1, \"homework\": 90.1 }'))\nstudent_collection$find('{\"name\": \"Yiran\"}')"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"update-command","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.7.2 Update Command","text":"want modify attribute document, can use update command $set expression. Refer https://docs.mongodb.com/manual/reference/method/db.collection.update/ information.can see Yiran’s exam score modified 92.4.","code":"\nstudent_collection$update('{\"name\":\"Yiran\"}', '{\"$set\":{\"exam\": 92.4}}')\nstudent_collection$find('{\"name\": \"Yiran\"}')"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"remove-command","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.7.3 Remove Command","text":"want delete document database, can use remove command. Refer https://docs.mongodb.com/v4.2/reference/method/db.collection.remove/ information remove command.Now find anything Yiran","code":"\nstudent_collection$remove('{\"name\" : \"Yiran\"}')\nstudent_collection$find('{\"name\": \"Yiran\"}')"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"visualization-1","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.8 Visualization","text":"easy convert data obtained APIs mongolite package dataframe R.converting data dataframe, can use ggplot2 visualize data.Now want explore correlations homework quiz, homework exam, quiz exam.looks like 3 pairs variables correlation. Next, going perform chi-square tests verify .3 p-values much smaller 0.01. 3 pairs variables independent 1% significance level.","code":"\ndf <- as.data.frame(student_collection$find())\nhead(df)\nhist1 <- ggplot(df, aes(x=exam)) + geom_histogram(color=\"white\", fill=\"lightblue\")\nhist2 <- ggplot(df, aes(x=quiz)) + geom_histogram(color=\"white\", fill=\"lightblue\")\nhist3 <- ggplot(df, aes(x=homework)) + geom_histogram(color=\"white\", fill=\"lightblue\")\n\nplot_grid(hist1, hist2, hist3, ncol=2)\nmelt_df <- melt(df, measure.vars=c('homework', 'exam', 'quiz'))\nggplot(melt_df) + geom_boxplot(aes(x=variable, y=value, color=variable))\nscatter1 <- ggplot(df, aes(x=homework, y=quiz)) + geom_point(color=\"lightblue\")\nscatter2 <- ggplot(df, aes(x=homework, y=exam)) + geom_point(color=\"lightblue\")\nscatter3 <- ggplot(df, aes(x=quiz, y=exam)) + geom_point(color=\"lightblue\")\n\nplot_grid(scatter1, scatter2, scatter3, ncol=2)\nXtest_homework_quiz <- chisq.test(table(df$homework, df$quiz), simulate.p.value=TRUE, correct=FALSE)\nXtest_homework_quiz$p.value\nXtest_homework_exam <- chisq.test(table(df$homework, df$exam), simulate.p.value=TRUE, correct=FALSE)\nXtest_homework_exam$p.value\nXtest_quiz_exam <- chisq.test(table(df$quiz, df$exam), simulate.p.value=TRUE, correct=FALSE)\nXtest_quiz_exam$p.value"},{"path":"tutorial-on-using-mongodb-with-r.html","id":"references-10","chapter":"101 Tutorial on Using MongoDB with R","heading":"101.9 References:","text":"[1] https://www.mongodb.com/-use-mongodb[2] https://www.mongodb.com/languages/mongodb--r-example[3] https://jeroen.github.io/mongolite/manipulate-data.html","code":""},{"path":"rdoc---an-alfred-worflow-to-search-r-documentation.html","id":"rdoc---an-alfred-worflow-to-search-r-documentation","chapter":"102 rdoc - An Alfred Worflow to Search R Documentation","heading":"102 rdoc - An Alfred Worflow to Search R Documentation","text":"Darvesh Gorhe","code":""},{"path":"rdoc---an-alfred-worflow-to-search-r-documentation.html","id":"motivation-9","chapter":"102 rdoc - An Alfred Worflow to Search R Documentation","heading":"102.1 Motivation","text":"Looking documentation can often distracting time consuming process. often involves moving separate window RStudio preferred IDE. can remedied second monitor hardly practical solution cases. easier see temporary window information need disappears keystroke.","code":""},{"path":"rdoc---an-alfred-worflow-to-search-r-documentation.html","id":"alfred-and-alfred-workflows","chapter":"102 rdoc - An Alfred Worflow to Search R Documentation","heading":"102.2 Alfred and Alfred Workflows","text":"Fortunately Alfred provides functionality ability write scripts utilize core features. Normally Alfred used like macOS’s built-Spotlight feature - file searching, calculator, opening programs, etc. However, workflows allow developers extend Alfred’s functionality python. Users can download workflows Alfred’s website anywhere else choose.","code":""},{"path":"rdoc---an-alfred-worflow-to-search-r-documentation.html","id":"where-to-get-rdoc","chapter":"102 rdoc - An Alfred Worflow to Search R Documentation","heading":"102.3 Where to Get rdoc","text":"can find download latest version rdoc releases page. Download file titled “Rdoc.alfredworkflow.zip”, unzip file open file open Alfred ask install workflow. Make sure downloaded latest version Alfred. Unfortunately, need paid Powerpack use workflow functionality. one-time purchase think worthwhile.","code":""},{"path":"rdoc---an-alfred-worflow-to-search-r-documentation.html","id":"about","chapter":"102 rdoc - An Alfred Worflow to Search R Documentation","heading":"102.4 About","text":"Get function descriptions arguments rdocumentation.org like “rdoc ggplot2 aes_eval”, without open web browser. Currently descriptions function arguments stored offline solution ideal ’re planning somewhere without internet connection.","code":""},{"path":"rdoc---an-alfred-worflow-to-search-r-documentation.html","id":"usage","chapter":"102 rdoc - An Alfred Worflow to Search R Documentation","heading":"102.5 Usage","text":"Type rdoc followed query form <library> <function>\nggplot2\n\ndplyr\n\nstringr\n\ntidyverse\n\ntibble\n\nodbc\n\nforeign\n\ntidyr\n\nhtmlwidgets\n\nvcd\n\nxml\n\njsonlite\n\nhttr\n\ndevtools\nCMD + L view details\nCMD + C copy formatted function clipboardTo open web page documentation press: ENTER","code":""},{"path":"data-frame-reshape.html","id":"data-frame-reshape","chapter":"103 Data Frame Reshape","heading":"103 Data Frame Reshape","text":"Jingwen Bai","code":""},{"path":"data-frame-reshape.html","id":"introduction-23","chapter":"103 Data Frame Reshape","heading":"103.0.1 Introduction","text":"tutorial focusing reshape data frame R using pacakges reshape2 tidyr.\nFirst, need know wide-format data long-format data. Wide data column variable. long data using variable values.","code":""},{"path":"data-frame-reshape.html","id":"build-a-simple-data-frame","chapter":"103 Data Frame Reshape","heading":"103.0.1.1 Build a simple data frame","text":"created simple wide data df, column variable.","code":"\ndf <- data.frame(\n  #x = c(1, 2, 3), \n player=c('A', 'B', 'C', 'D'),\n #gender=c('F', 'M', 'F', 'M'),\n classYear=c(2, 4, 3, 3),\n year1=c(12, 15, 19, 19),\n year2=c(22, 29, 18, 12),\n year3=c(17, 17, 22, 25),check.names = FALSE)\nhead(df)"},{"path":"data-frame-reshape.html","id":"convert-wide-format-data-to-long-format-data","chapter":"103 Data Frame Reshape","heading":"103.0.2 Convert wide-format data to long-format data","text":"","code":""},{"path":"data-frame-reshape.html","id":"method-1-use-melt-in-package-reshape2","chapter":"103 Data Frame Reshape","heading":"103.0.2.1 Method 1: Use melt() in package reshape2","text":"Melt takes wide-format data melts long-format data.\nDefault parameter:ID variables variables . default, melt assumed columns numeric values variables values. player id variables.Now lets try combined ID variables","code":"\nlibrary(reshape2)\ndf_melt <- reshape2::melt(df)\ndf_melt\n# assign variable and value's column name \nlibrary(reshape2)\ndf_melt_name <- reshape2::melt(df,id.vars = c(\"player\"), variable.name = \"year\",value.name = \"value\")\ndf_melt_name\ndf_melt_multi <- reshape2::melt(df,id.vars = c(\"player\" , 'classYear'), \n                               variable.name = \"year\")\ndf_melt_multi"},{"path":"data-frame-reshape.html","id":"method-2-use-gather-in-tidyr","chapter":"103 Data Frame Reshape","heading":"103.0.2.2 Method 2: Use gather() in tidyr","text":"gather() function tidyr package can used “gather” key-value pair across multiple columns.Now, let’s try gather values two columns","code":"\n#gather data from columns except player column\ndf_gather <- tidyr::gather(data = df,key = 'year', value = 'value', -player) \n\ndf_gather\ndf_gather_multi <- tidyr::gather(data = df,key = 'year', value = 'value', 3:5)\n# df_gather <- tidyr::gather(data = df,key = 'year', value = 'value', -x) <- same \n\ndf_gather_multi"},{"path":"data-frame-reshape.html","id":"convert-long-format-data-to-wide-format-data","chapter":"103 Data Frame Reshape","heading":"103.0.3 Convert long-format data to wide-format data","text":"Now learned convert wide-format data long-format data. Now let’s try inverse\n#### Method 1: Use dcast() package reshape2\ndcast formula takes form LHS ~ RHS.Now let’s revert df_melt_multi","code":"\ndf_cast <- reshape2::dcast(df_melt, player~variable,value.var = 'value')\nhead(df_cast)\ndf_cast_muli <- reshape2::dcast(df_melt_multi, player + classYear ~ year,value.var = 'value')\nhead(df_cast)"},{"path":"data-frame-reshape.html","id":"method-2-use-spread-in-package-tidyr","chapter":"103 Data Frame Reshape","heading":"103.0.3.1 Method 2: Use spread() in package tidyr","text":"Revert df_gather_multi using spread():spread() doesn’t like dcast(), don’t consider columns used gathering columns ID variables , need specidy columns name key value.","code":"\ndf_spread <- tidyr::spread(df_gather,year,value)\nhead(df_spread)\ndf_spread <- tidyr::spread(df_gather_multi, year,value)\nhead(df_spread)"},{"path":"using-postgresql-databse-in-r-with-macos-environment.html","id":"using-postgresql-databse-in-r-with-macos-environment","chapter":"104 Using PostgreSQL Databse in R with MacOS Environment","heading":"104 Using PostgreSQL Databse in R with MacOS Environment","text":"Wei Luo","code":""},{"path":"using-postgresql-databse-in-r-with-macos-environment.html","id":"requirements","chapter":"104 Using PostgreSQL Databse in R with MacOS Environment","heading":"104.0.1 Requirements","text":"required tools packages :","code":"        1. R installation (assumed)\n        2. PostgreSQL Database installation\n        3. RMariaDB package\n        "},{"path":"using-postgresql-databse-in-r-with-macos-environment.html","id":"postgresql-installation-guide-with-homebrew","chapter":"104 Using PostgreSQL Databse in R with MacOS Environment","heading":"104.0.2 PostgreSQL installation guide with HomeBrew","text":"Open terminal laptop run following command:install homebrew service mac.(see Updating Homebrew message running command. went long, can press Crtl + c stop.)successfully installed homebrew:confirm Postgres installed correctly, run following command make sure outputs version information (error message):","code":"1. /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"2. sudo brew install postgresql3. psql --version"},{"path":"using-postgresql-databse-in-r-with-macos-environment.html","id":"postgresql-setup","chapter":"104 Using PostgreSQL Databse in R with MacOS Environment","heading":"104.0.3 PostgreSQL setup","text":"start psql server:start postgres, type command terminal:can create new Database PostgreSQl using command:","code":"1. pg_ctl -D /usr/local/var/postgres start2. psql postgres3. CREATE DATABASE name_of_your_choice;"},{"path":"using-postgresql-databse-in-r-with-macos-environment.html","id":"connect-to-postgresql-database-in-r-studio","chapter":"104 Using PostgreSQL Databse in R with MacOS Environment","heading":"104.0.4 Connect to PostGreSQL Database in R Studio","text":"Required R packages:order Connect PostgreSQL database, run following command R studio:can also run SQL queries R studio connecting PostgreSQL Database:data fetched PostgreSQL data. can data visualization dataset using R.","code":"1. odbc\n2. DBI\n3. dplyr\n4. pool\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# SQLite database\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n#copy_to(con, nycflights13::flights, \"FLIGHTS\")\n\n# ODBC databases (requires a live database connection)\n#con <- dbConnect(odbc::odbc(), \"SQL Server\")\n#con <- dbConnect(odbc::odbc(), \"Snowflake\")\n    #data <- dbSendQuery(con, \"your query\")\n    #dbBind(data, list(\"col A\", \"MSY\"))\n    #dbFetch(airport)"},{"path":"using-postgresql-databse-in-r-with-macos-environment.html","id":"walking-through-an-example.","chapter":"104 Using PostgreSQL Databse in R with MacOS Environment","heading":"104.0.5 Walking through an example.","text":"section, going use simple csv file initial dataset, upload PostgreSQL database fetch using R R studio.attachment, find example csv file called “part.csv”Create file called “loaddata.sql”:create file called “ddl.sql”:Create file called “setup.sh”:run setup.sh, please change corresponding path loaddata.sql.Run setup.sh ./setup.sh bash setup.sh, PostgreSQL fill form called “part”.Configure DB connection details:can connect database use SQL queries find whatever want!","code":"BEGIN;\nCOPY part FROM 'change it to your absolute path/part.csv' WITH (FORMAT csv, DELIMITER '|');\nCOMMIT;DROP TABLE IF EXISTS part CASCADE;\n\nCREATE TABLE  part ( p_partkey integer, p_1 integer,  p_2 integer, p_3 integer, p_4 integer, p_5 integer, p_6 integer, p_7 integer, p_8 integer, primary key (p_partkey));CUR_PATH=`pwd`\n\npsql yourdatabase < ${CUR_PATH}/ddl.sql\necho \"Created tables\"\n\npsql yourdatabase < ${CUR_PATH}/loaddata.sql\necho \"Loaded data into tables\"\n#con <- DBI::dbConnect(odbc::odbc(),\n#                      Driver   = \"[your driver's name]\",\n#                      Server   = \"[your server's path]\",\n#                      Database = \"[your database's name]\",\n#                      UID      = rstudioapi::askForPassword(\"Database user\"),\n#                      PWD      = rstudioapi::askForPassword(\"Database #password\"),\n#                      Port     = 5432)"},{"path":"spark-tutorial-in-r.html","id":"spark-tutorial-in-r","chapter":"105 Spark Tutorial In R","heading":"105 Spark Tutorial In R","text":"Yunhan Jin Zeyu JinWhy using Spark?working small-scale datasets, within memory limit, can perform steps R, without using Spark. However, data fit memory computation simply slow, can slightly modify approach incorporating Spark.Spark parallel computation engine works large scale provides SQL engine modeling libraries. can perform operations including data selection, transformation, modeling. Spark also includes tools performing specialized computational work like graph analysis, stream processing, many others.spark required local set-ups prevent bookdown rendering related R codes, able present code book . However, detail tutorials codes can found data needed codes can downloaded dropbox.tutorial guide procedures listed :Install set Spark local machine connect local clusterUse spark load datasets RLoad Spark web interface additional informationData analysis using Spark RModeling data SparkData wrangling tutorial cheatsheetData visualizationYou can learn concepts following sample project showcase tutorial.","code":""},{"path":"d-visualization-with-rgl-and-scatterplot3d.html","id":"d-visualization-with-rgl-and-scatterplot3d","chapter":"106 3D Visualization with rgl and scatterplot3d","heading":"106 3D Visualization with rgl and scatterplot3d","text":"Liyi Zhang","code":"\nlibrary(tidyverse)\nlibrary(rgl)\nlibrary(scatterplot3d)"},{"path":"d-visualization-with-rgl-and-scatterplot3d.html","id":"introduction-24","chapter":"106 3D Visualization with rgl and scatterplot3d","heading":"106.0.1 Introduction","text":"tutorial, :Learn plot 3 dimensions package providing html-friendly static graphLearn plot 3 dimensions different package gives interactive plot (knitted html)Study input colors texts 3d plotsApply plots visualize latent structures learned variational auto-encoders (VAE)Notice plots rgl shown html file.","code":""},{"path":"d-visualization-with-rgl-and-scatterplot3d.html","id":"toy-data","chapter":"106 3D Visualization with rgl and scatterplot3d","heading":"106.0.2 Toy Data","text":"first use toy data learn use 3d plotting function plot3d scatterplot3d.First, generate 3d diagonal Gaussian. Note parameters set datapoints dimension closely around 1, 2, 3, respectively:plot3d scatterplot3d take input N--3 data.Coloring. Oftentimes, N datapoints come different classes, want plot different colors. can done using colors argument, , functions, N-vector looks like c('red', 'red', 'blue', ...).Plotting. first plot rgl:Add text. rgl, can also add text point using text3d function, takes data input also N-vector texts attached point:Difference plot3d scatterplot3d. plot3d gives interactive plot, appear html. hand, scatterplot3d appears html interactive. thus good .","code":"\nX <- rnorm(300, mean=c(1,2,3), sd=c(0.1,0.1,0.1))\ndim(X) <- c(3,100)\nX <- t(X) # N-by-D\n# Get indices of colors, or classes. Right now classes are random, and the first 70 data points all come from class 1 (red)\nclasses <- sample(c(1,2,3), 100, replace=TRUE)\nclasses[1:70] <- 1\ncolors <- rainbow(3)[classes]\n# Plot with rgl\nplot3d(X, col=colors)\n# Plot with rgl:\nplot3d(X, col=colors)\ntext3d(X, texts=as.character(classes))\n# Plot with scatterplot3d\nscatterplot3d(X, color=colors)\n# Alternatively, a version with class names:\n# scatterplot3d(X, color=colors, pch=as.character(classes))"},{"path":"d-visualization-with-rgl-and-scatterplot3d.html","id":"variational-auto-encoder-representations-on-mnist","chapter":"106 3D Visualization with rgl and scatterplot3d","heading":"106.0.3 Variational Auto-Encoder Representations on MNIST","text":"now look application 3d visualization. MNIST dataset pictures handwritten digits, picture high-dimensional 28 x 28 pixels. image, Variational Auto-Encoders (VAE) learn low-dimensional latent variable ‘generates’ image neural network-parameterized probability function. shall now plot 3d latent variables sampled trained VAE, point supposedly generate picture handwritten digit.trained VAE code Python, training method follows original papers (Kingma & Welling, 2014; Rezende, et al., 2014).visualizations learned fact show VAE number problems. example additionally shows rgl interactive plots informative static plots html, least example.visualizations show latent manifold clearly distinguish classes. particularly evident rotate rgl plots, show points two different classes quite mixed together. suggests VAE trained maximizing evidence lower bound amortized variational inference unable give clear human-like interpretations. Indeed, previous research suggests, training VAE tends favor reconstructing image learning meaningful latent variable structure. Semi-supervised training advanced inference algorithms might improve learning latent structures.","code":"\n# Load data:\ntest_labels <- as.numeric(unlist(read.table('https://raw.githubusercontent.com/jtr13/cc21fall2/main/resources/r_3d_visualization/test_labels.csv', header=FALSE)))\nlatents <- read.table('https://raw.githubusercontent.com/jtr13/cc21fall2/main/resources/r_3d_visualization/test_3d_latents.csv', header=FALSE)\n\n# Select a subset of the data:\ntest_labels <- test_labels[1:1000]\nlatents <- latents[1:1000,]\n\n# 10 classes are hard to visualize, so we record indices corresponding to classes and look at 2 classes at a time:\nindices <- list()\nfor(i in 0:9){\n  indices[[i+1]] <- which(test_labels==i)\n}\n\n# We choose digits '3' and '8':\nindices1 <- c(indices[[4]],\n              indices[[9]])\ncolors <- rainbow(10)[test_labels[indices1] + 1]\ntexts <- as.character(test_labels[indices1])\n\n# Plot:\nplot3d(latents[indices1,], col=colors)\ntext3d(latents[indices1,], texts=texts)\n\nscatterplot3d(latents[indices1,], color=colors, pch=texts)\n# We choose digits '4' and '8':\nindices2 <- c(indices[[5]],\n              indices[[9]])\ncolors <- rainbow(10)[test_labels[indices2] + 1]\ntexts <- as.character(test_labels[indices2])\n\nplot3d(latents[indices2,], col=colors)\ntext3d(latents[indices2,], texts=texts)\n\nscatterplot3d(latents[indices2,], color=colors, pch=texts)"},{"path":"d-visualization-with-rgl-and-scatterplot3d.html","id":"references-11","chapter":"106 3D Visualization with rgl and scatterplot3d","heading":"106.0.4 References","text":"3d plotting:https://planspace.org/2013/02/03/pca-3d-visualization--clustering--r/https://stackoverflow.com/questions/60589690/-can--make--3d-plot--r---clusters-obtained--kmeansOn VAE:Diederik P. Kingma M. Welling. Auto-encoding variational Bayes. CoRR, abs/1312.6114, 2014.Danilo Jimenez Rezende, S. Mohamed, Daan Wierstra. Stochastic backpropagation approximate inference deep generative models. 2014.","code":""},{"path":"base-r-vs.-ggplot2-visualization.html","id":"base-r-vs.-ggplot2-visualization","chapter":"107 Base r vs. ggplot2 visualization","heading":"107 Base r vs. ggplot2 visualization","text":"Yiquan Li Zezhong Fan","code":""},{"path":"base-r-vs.-ggplot2-visualization.html","id":"scatterplot-1","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.1 Scatterplot","text":"Base Rggplot2From comparison , can see dot style, dot color, dot size, dot thickness, axes labels graph title, ggplot2 base r can adjust features single statement integrate.try fancy tasks, fit best regression line data points, label data point, see base r can longer just add features original plot, write separate statement create new graphs original dataset, superimpose plots original one. (saperate statment, need call dataset ‘mtcars’ ) inefficient also hard read need find “plot()” function careful functions , “abline()” “text()”, also create new features top first plot.Instead, ggplot2 can create new features simply adding new component original statement (Dataset ‘mtcars’ called ). Adding “geom_smooth()” can fit visualize best regression line data point. also gives extra information blur around line shows 95% confidence interval fitted values. Adding “geom_text()” can easily label data point. way, plots created organized way, code efficient readable.","code":"\n# dataset\ndata(\"mtcars\")\nplot(mtcars$wt,mtcars$mpg, main=\"Scatterplot in Base R\",\n   xlab=\"Car Weight\", ylab=\"MPG\", \n   pch=4, col = \"blue\", lwd=1, cex = 2)\nabline(lm(mtcars$mpg~mtcars$wt), col=\"red\")\ntext(mtcars$wt, mtcars$mpg, labels=rownames(mtcars), cex=0.5, font=2)\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point(size=5, shape=4, color=\"blue\", stroke=1) + \n  geom_smooth(method=lm, color=\"red\") +\n  ggtitle(\"Scatterplot in ggplot2\") +\n  xlab(\"Car Weight\") + # for the x axis label\n  geom_text(label=rownames(mtcars),cex=3)"},{"path":"base-r-vs.-ggplot2-visualization.html","id":"line-chart","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.2 Line chart","text":"","code":""},{"path":"base-r-vs.-ggplot2-visualization.html","id":"single-line","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.2.1 Single line","text":"Base Rggplot2For two plots , try use similar code possible create line charts base r using ggplot2. can see resulting plots different. line chart created base r messy created order original data set. However, line chart created using ggplot2, automatically sorts independent variable plots x-axis. , resulting plot much clear can show much better relationship two variables. perspective, ggplot2 cleverer base r terms giving meaningful infomation.","code":"\nplot(mtcars$wt, mtcars$mpg, main=\"Line chart in Base R\", type = \"o\",col=\"red\")\nggplot(data=mtcars, aes(x=wt, y=mpg, group=1)) +\n  geom_line(colour=\"red\", size=1.5) +\n  geom_point(colour=\"red\", size=4, shape=21, fill=\"white\") +\n  ggtitle(\"Line chart in ggplot2\") "},{"path":"base-r-vs.-ggplot2-visualization.html","id":"multiple-line","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.2.2 Multiple line","text":"Base Rggplot2For multiple-line chart, see problem scatter plot . want add another line original plot, base r need write another statement using “lines()”, makes code harder read understand. ggplot2, clear whenever add “geom_line()”, new line appear “ggplot()”.","code":"\n# data created\nx <- 1:5\nv <- c(7,12,28,3,41)\nt <- c(14,7,6,19,3)\n\nplot(x, v, type = \"l\",col = \"red\", ylab = \"y\", main = \"Multiple line in Base R\")\nlines(x, t, type = \"l\", col = \"blue\")\nggplot() + \n  geom_line(aes(x = x, y = v), color = \"blue\") +\n  geom_line(aes(x = x, y = t), color = \"red\") +\n  ylab('y') + \n  ggtitle(\"Multiple lines in ggplot2\") "},{"path":"base-r-vs.-ggplot2-visualization.html","id":"line-chart-by-groups","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.2.3 Line chart by groups","text":"Base Rggplot2If want use different color represent different groups, draw lines different groups graph, using ggplot2 convenient. need transform feature want group factor. , one argument ‘col=feature’ can job us gives perfect lines different color.base r, can see fulfill task, need write much codes takes much time. first need order dependent variable x-axis. , need generate dataframes group, create one line chart using one dataframe, add lines one one., complicated tasks, ggplot2 gives fast easy solution whereas base r still requires manual implementation.","code":"\nnewdata <- mtcars[order(mtcars$wt),]\ncyl_4=newdata[newdata$cyl==4,]\ncyl_6=newdata[newdata$cyl==6,]\ncyl_8=newdata[newdata$cyl==8,]\nplot(cyl_4$wt, cyl_4$mpg, type = \"l\", col=\"red\", xlim = c(min(mtcars$wt), max(mtcars$wt)), ylim=c(min(mtcars$mpg), max(mtcars$mpg)), main = \"Line chart by groups in base r\")\nlines(cyl_6$wt, cyl_6$mpg, type = \"l\", col = \"green\")\nlines(cyl_8$wt, cyl_8$mpg, type = \"l\", col = \"blue\")\nmtcars$cyl <- as.factor(mtcars$cyl)\nggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_line() + ggtitle(\"Line chart by groups in ggplot2\") "},{"path":"base-r-vs.-ggplot2-visualization.html","id":"histogram-3","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.3 histogram","text":"Base Rggplot2For histogram, base r ggplot2 can reach effect using minimum code. want add density curve top histogram, ggplot2 readable.","code":"\nhist(mtcars$mpg, freq=FALSE,breaks=10,border=\"blue\", \n     col=\"green\", density=50, right=TRUE, main=\"Histogram in base r\") \nlines(density(mtcars$mpg), col=\"red\")\nggplot(data=mtcars, aes(mpg)) + \n  geom_histogram(aes(y=..density..), bins = 12, closed = \"right\",color=\"blue\", fill=\"green\",alpha=0.5) +\n  geom_density(alpha=0.6, color= \"red\") + \n  ggtitle(\"Histogram in ggplot2\") "},{"path":"base-r-vs.-ggplot2-visualization.html","id":"bar-chart","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.4 Bar Chart","text":"Base Rggplot2From comparison two graphs, can find ggplot2, background grey grids make users visualize scales clearly. Besides, title labels lighter normal plots R . perspectives codes, can say coding style different ggplot2 R. ggplot2, can see code divided different components codes linked ‘+’. However, normal R, use function R set parameters labels need inside funtion .","code":"\n# dataset\ndata = data.frame(sex = factor(c('Male','Female'),levels = c('Male','Female')), height = c(175.26,162.56))\n\nbarplot(data$height, names.arg = data$sex, col = '#DD8888', main = 'RPlot:Mean Height between Sex in US' )\nggplot(data = data,aes(x = sex, y = height))+\n  geom_bar(colour = 'black',fill = '#AFC0CB',stat = 'identity')+\n  xlab('Sex')+ylab('Mean Height in US')+\n  ggtitle('GGPlot: Mean Height between Sex in US')"},{"path":"base-r-vs.-ggplot2-visualization.html","id":"boxplot-2","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.5 Boxplot","text":"Base Rggplot2we can also see ggplot2, background grey grids make users visualize scales clearly. Another big difference breaks y scale. default breaks different ggplot normal r. code style also different. ggplot2, can see code divided different components codes linked ‘+’. However, normal R, use function R set parameters labels need inside function .","code":"\n# data set\nx <- rnorm(1000)\ny <- runif(1000)                                             \nz <- rpois(1000, 3)\n\ndata <- data.frame(values = c(x, y, z),                     \n                   group = c(rep(\"x\", 1000),\n                             rep(\"y\", 1000),\n                             rep(\"z\", 1000)))\nboxplot(values ~ group, data, main = 'normal r boxplot')  \nggplot(data,aes(x = group, y = values))+\n  geom_boxplot(fill = 'blue')+\n  ggtitle('ggplot: boxplot')"},{"path":"base-r-vs.-ggplot2-visualization.html","id":"piechart","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.6 Piechart:","text":"Base Rggplot2First, coding style also different. ggplot2, plot divided different components including xlab ylab title pie chart. normal r plot, just use function called ‘pie’. One interesting thing ggplot first use geom_col covert bar plot pie chart. looking graphs also different. default colors sequence groups also different. ggplot, can find scale around pie chart can discover percentage group pie chart.","code":"\n# dataset\ndf <- data.frame(\n  group = c(\"Male\", \"Female\", \"Child\"),\n  value = c(25, 25, 50))\nslices = df$value\n\npie(slices,labels = df$group, main = 'normal r plot: piechart' )\nggplot(df, aes(x=\"\", y=value, fill=group))+\n  geom_col()+\n  coord_polar(theta = 'y')+\n  ggtitle('ggplot: piechart')"},{"path":"base-r-vs.-ggplot2-visualization.html","id":"multiple-variables-1","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.7 Multiple Variables","text":"Base Rggplot2For code part, ggplot2, just need use components ggplot packages use function called geom bar visualize 2 variables, set parameter ‘fill’. However, normal r plot, need first convert data frame matrix apply function called ‘barplot’. graph made ggplot clear. default color grey black can see labels x direction also legends default. However, graph made normal r packages doesn’t default legends x labels.","code":"\n# dataset\ndata <- data.frame(\n  sex = factor(c(\"Female\",\"Female\",\"Female\",\"Male\",\"Male\",\"Male\")),\n  time = factor(c(\"Breakfast\",\"Lunch\",\"Dinner\",\"Breakfast\",\"Lunch\",\"Dinner\")),\n  total_bill = c(13.53, 16.81,18.91, 16.24, 17.42,21.56))\n\ndatamat <- matrix( data$total_bill,\n                   nrow = 2,\n                   byrow=TRUE,\n                   dimnames = list(c(\"Female\", \"Male\"), c(\"Breakfast\",\"Lunch\", \"Dinner\"))\n                   )\nbarplot(datamat,beside = TRUE, border = NA, main = 'normal r plot: multiple variables.')\nggplot(data = data, aes(x= time, y = total_bill, fill = sex))+\n  geom_bar(stat = 'identity',colour = 'black',position=position_dodge())+\n  ggtitle('ggplot2: multiple variables')"},{"path":"base-r-vs.-ggplot2-visualization.html","id":"facet","chapter":"107 Base r vs. ggplot2 visualization","heading":"107.0.8 Facet","text":"Base Rggplot2In ggplot2, can just use function ‘facet_grid’ draw multiple graphs based variables, however, normal R, -loop loop different levels variables want compare plot wihich much complex ggplot2. looking graphs also different. default y-scale different. y-scale gg-plot however,scales graph normal r different. Besides, also grid background ggplot makes us see scale straight-forward.","code":"\n# dataset\nx <- rnorm(100)\ny <- runif(100)                                             \nz <- rpois(100, 3)\ndata <- data.frame(values = c(x, y, z),  \n                   values2 = c(x+1,y+1,z+1),\n                   group = c(rep(\"x\", 100),\n                             rep(\"y\", 100),\n                             rep(\"z\", 100)))\n\n\npar(mfrow=c(1,3))\ngroups <- unique(data$group)\ngroups## [1] \"x\" \"y\" \"z\"\nfor (i in 1:length(groups)) {\n  curdata = data[data$group == groups[i],]\n  plot(curdata$values,curdata$values2,\n    main=groups[i])\n}\nggplot(data, aes(x=values, y=values2))+\n  geom_point(shape=1)+facet_grid(. ~ group)+\n  ggtitle('ggplot: facet')"},{"path":"improved-density-estimation.html","id":"improved-density-estimation","chapter":"108 Improved density estimation","heading":"108 Improved density estimation","text":"Abel Perez Vargas","code":""},{"path":"improved-density-estimation.html","id":"introduction-25","chapter":"108 Improved density estimation","heading":"108.1 Introduction","text":"field data analysis common come across non-negative variables, waiting times, prices, distances, age weight, well non-positive variables money due, level sea freezing point temperature.thinking modeling variables, may first interested exploring visualizing data. among first techniques one may want implement, univariate visualization.Density plots useful technique unveils distributional form variables, may help us identify patterns may hidden traditional histogram (e.g. “gaps” data may happen inside one specific histogram class).However, density plots may sometimes give false impression know--non-negative variable negative values like example using simulations \\(\\Gamma(2,2)\\) distribution.can also case data exhibits values exactly equal 0, case standard density plot help us detect exploration order .","code":"\nset.seed(123)\nplot(density(rgamma(100, 2, 2) ), lty=2, lwd=2, col=\"red\", main=\"Densities\")\ncurve(dgamma(x, 2, 2), from=-1, to=5, lwd=2, add=T)\nlegend(\"topright\", legend=c(\"True density\", \"Estimated density\"), \n       lty=c(1, 2), col=c(\"black\", \"red\"))\nset.seed(123) \ndata <- c(rep(0, 10), rgamma(100, 1, 1))\nplot(density( data ), lty=2, lwd=2, col=\"red\",  main=\"Densities\")\nlegend(\"topright\", legend=\"Estimated density\", \n       lty=2, col=\"red\")\nprint(paste(\"Frequency of 0:\", sum(data==0) ))## [1] \"Frequency of 0: 10\""},{"path":"improved-density-estimation.html","id":"generalizing-kernel-smoothing-for-densities","chapter":"108 Improved density estimation","heading":"108.2 Generalizing Kernel Smoothing for Densities","text":"tutorial focus density plots, formally, density estimation based kernels (Venables & Ripley, 2002) data may come mixture discrete continuous random variables (e.g. data exhibiting 0’s), avoid positive density 0 data exhibits behavior., first perform algorithm detects repeated values data tests significance Binomial test proportion following hypothesis:\n\\[H_0: p_i=\\frac{1}{n} ~~ vs ~~ H_1: p_i>\\frac{1}{n},\\]\n\\(\\) index values repetitions.repeated values found statistically significant (, \\(H_0\\) rejected \\(\\alpha=0.05\\)), model data mixture discrete random variable support repeated values, continuous random variable data without repetitions, implies following density:\n\\[ f_X(x)=\\sum_{^*}\\hat{p}_{^*} \\delta_{ \\{ x_{^*} \\} }(x) ~ + ~ \\left( 1-\\sum_{^*}\\hat{p}_{^*} \\right)g(x),\\]\n\\(g\\) density estimate data without significant repeated values: \\(x_{^*}\\), \\(\\hat{p}_{^*}\\) represents estimated probability mass (proportion) values, \\(\\delta_{ \\{ k \\}}(x)\\) Dirac’s delta, function equal infinity \\(k\\), \\(0\\) otherwise, integrates one.previous representation useful keep \\(f_X\\) valid density function, visualization purposes plot \\(\\hat{p}_{^*}\\) Dirac’s delta represented vertical infinite line.density \\(g\\) modified standard stats::density function R avoid positive density non-admissible regions (like negative numbers data positive). end, slightly modify kernel method density estimation, generalizing gaussian kernel truncated normal smoothing. case non-negative data, kernel truncated distinct \\(0\\) \\(x\\geq0\\) (similar case non-positive data).","code":""},{"path":"improved-density-estimation.html","id":"plot_density-function","chapter":"108 Improved density estimation","heading":"108.3 plot_density Function","text":"introduce plot_density, function adjusts presence non-negative, non-positive real data avoid assigning density unlikely values, also automatically detects presence significant repeated values data.","code":"\n# plot_density function\n#\n# Inputs:\n#\n# x: data, a numeric vector possibly with repeated values\n# k: split parameter (default=0), the value for which the function will\n#    evaluate whether or not the data is all \"to the left\", \"to the right\"\n#    or neither from k\n# bw: bandwidth, either the character \"nrd0\" as in stats::density, or a number\n#     for manual bandwidth selection\n# main, xlab, ylab: graphical parameters to be passed to base::plot\n#\n# Output: data density plot with displayed relative frequency of repeated vales (if that's the case)\nplot_density <- function(x, thr=0, bw=\"nrd0\",\n                         main=\"Density Plot\", xlab=\"x\", ylab=\"density\"){\n  \n  if(sum(is.na(x))>0 ){\n    stop(\"NAs found\")\n  }else{\n    \n    # Repeated Values detection\n    \n    n <- length(x)\n    x <- sort(x)\n    y <- x[-1][x[-1]==x[-n]]\n    vals <- NULL\n    \n    # Smallest difference distinct than 0 to be used for plot margins\n    diff <- x[-1]-x[-n]\n    diff <- min(diff[diff>0])\n  \n    if(length(y)>0 ){\n      rep <- table(y) + 1# frequency of the repeated values\n      M <- sapply(rep, binom.test, n=n, p=1/n,\n             alternative=\"greater\")# binomial test p-values\n      M <- unlist(M[row.names(M)==\"p.value\", ])\n      \n      vals <- names(rep)[M<0.05]\n      freq <- rep[M<0.05]\n    \n      # If significant values where detected, they are stored in 'vals'\n      if(length(vals)>0){\n        vals <- as.numeric(vals)\n      }\n    }\n    \n    # banwidth\n    \n    if(bw==\"nrd0\"){\n      # Silverman's rule of thumb: Silverman (1986, page 48, eqn (3.31)))\n      IQ <- quantile(x, 0.75) - quantile(x, 0.25)\n      if(IQ>0){\n        bw <- 0.9*min((sd(x)/1.34*n)^(-1/5), (IQ/1.34*n)^(-1/5))\n      }else{\n        bw <- 0.9*(sd(x)/1.34*n)^(-1/5)\n      }\n      \n    }\n    \n    \n    # Density plotting\n    \n    \n    if( min(x)>=thr ){\n      # Non-negative case: Gaussian kernel modified to positive truncated Normal\n      \n      \n      # Truncated normal kernel\n      k <- function(x, m, sd){\n        if(x<0){\n          0\n        }else{\n          dnorm(x, m, sd)/(1-pnorm(0, m, sd)) \n        }\n      }\n      \n      \n      if(length(y)>0 & length(vals)>0){\n        # Discrete probability mass will be plotted\n        \n        p <- freq/n# mass probabilities\n        c <- x[!x %in% vals]# continuous component\n        \n        # continous density component re-scaled with the mass probability\n        d <- function(x, c, bw){\n          (1-sum(p))*mean(k(x, c, bw))\n        }\n        \n        # Plot\n        grid <- c( min(x)-diff, c, max(x)+diff )\n        plot(grid, sapply( grid, d, c, bw ), main=main, xlab=xlab,\n             ylab=ylab, col=\"darkgray\", type=\"l\")\n        points(vals, p, pch=16)\n        for(j in 1:length(vals)){lines(c(vals[j], vals[j]), c(0, p[j]) )}\n        legend(\"topright\", legend=\"Mass Probability\", col=\"black\", pch=16)\n      \n      }else{\n        # No probability mass found\n        \n        # continous density\n        d <- function(x, c, bw){\n          mean(k(x, c, bw))\n        }\n        \n        # Plot\n        grid <- c( min(x)-diff, x, max(x)+diff )\n        plot(grid, sapply( grid, d, x, bw ), main=main, xlab=xlab,\n             ylab=ylab, col=\"gray\", type=\"l\")\n        \n      }\n      \n      \n    }else{\n      \n      \n      if( max(x)<=thr ){\n        # Non-positive case: Gaussian kernel modified to negative truncated Normal\n        \n        # Truncated normal kernel\n        k <- function(x, m, sd){\n          if(x>0){\n            0\n          }else{\n            dnorm(x, m, sd)/pnorm(0, m, sd) \n          }\n        }\n      \n        \n        \n        if(length(y)>0 & length(vals)>0){\n          # Discrete probability mass will be plotted\n          \n          p <- freq/n# mass probabilities\n          c <- x[!x %in% vals]# continuous component\n        \n          # continous density component re-scaled with the mass probability\n          d <- function(x, c, bw){\n            (1-sum(p))*mean(k(x, c, bw))\n          }\n        \n          # Plot\n          grid <- c( min(x)-diff, c, max(x)+diff )\n          plot(grid, sapply( grid, d, c, bw ), main=main, xlab=xlab,\n               ylab=ylab, col=\"gray\", type=\"l\")\n          points(vals, p, pch=16)\n          for(j in 1:length(vals)){lines(c(vals[j], vals[j]), c(0, p[j]) )}\n          legend(\"topright\", legend=\"Mass Probability\", col=\"black\", pch=16)\n        \n        \n        }else{\n          # No probability mass found\n          \n          # continous density\n          d <- function(x, c, bw){\n            mean(k(x, c, bw))\n          }\n        \n          # Plot\n          grid <- c( min(x)-diff, x, max(x)+diff )\n          plot(grid, sapply( grid, d, x, bw ), main=main, xlab=xlab,\n               ylab=ylab, col=\"gray\", type=\"l\")\n        \n        }\n        \n        \n      \n      }else{\n        # Real case: Gaussian kernel kept\n        \n        \n        if(length(y)>0 & length(vals)>0){\n          # Discrete probability mass will be plotted\n          \n          p <- freq/n# mass probabilities\n          c <- x[!x %in% vals]# continuous component\n        \n          # continous density component re-scaled with the mass probability\n          d <- function(x, c, bw){\n            (1-sum(p))*mean(dnorm(x, c, bw))\n          }\n        \n          # Plot\n          grid <- c( min(x)-diff, c, max(x)+diff )\n          plot(grid, sapply( grid, d, c, bw ), main=main, xlab=xlab,\n               ylab=ylab, col=\"gray\", type=\"l\")\n          points(vals, p, pch=16)\n          for(j in 1:length(vals)){lines(c(vals[j], vals[j]), c(0, p[j]) )}\n          legend(\"topright\", legend=\"Mass Probability\", col=\"black\", pch=16)\n        \n        \n        }else{\n          # No probability mass found\n          \n          # continous density\n          d <- function(x, c, bw){\n            mean(dnorm(x, c, bw))\n          }\n        \n          # Plot\n          grid <- c( min(x)-diff, x, max(x)+diff )\n          plot(grid, sapply( grid, d, x, bw ), main=main, xlab=xlab,\n               ylab=ylab, col=\"gray\", type=\"l\")\n        \n        }\n        \n        \n      }\n      \n    }\n    \n  }\n  \n  \n  \n}"},{"path":"improved-density-estimation.html","id":"examples","chapter":"108 Improved density estimation","heading":"108.4 Examples","text":"Now show examples function useful.","code":""},{"path":"improved-density-estimation.html","id":"non-negative-data-with-repetitions","chapter":"108 Improved density estimation","heading":"108.4.1 Non-negative data with repetitions","text":"Imagine data waiting time people bank branch. context may observations exactly equal 0 indicating clients got attended directly .previous example data non-negative exhibits repetitions 0. plot_density detects repetitions statistically significant displays proportion single dot connected black line. density now doesn’t exhibits positive probability negative numbers (opossed stats::density blue).","code":"\nset.seed(123)\ntimes <- c(rep(0, 15), rexp(200))\nplot_density(times)\nlines(density(times), col=\"blue\", lty=2)\nlegend(\"top\", legend=c(\"plot_density\", \"Usual density\"),\n       col=c(\"darkgray\", \"blue\"), lty=c(1, 2), cex=0.7)"},{"path":"improved-density-estimation.html","id":"non-positive-data-with-repetitions","chapter":"108 Improved density estimation","heading":"108.4.2 Non-positive data with repetitions","text":"now center attention negative data, e.g. credit due. cases density positive values counter intuitive.case two probability mass points 0 -1, may associated cards 0 balance, many 1 due , fo example, cash back promotion using $1,000 credit (assume data thousands).plot_density detects repetitions excludes data giving broader picture , standard density method gives impression bimodality around -1 actually two repeated values repeated.","code":"\nset.seed(123)\ncredit <- c(rep(0, 20), rep(-1, 30), -rgamma(300, 8, 1))\nplot_density(credit, bw=0.5)\nlines(density(credit), col=\"blue\", lty=2)\nlegend(\"topleft\", legend=c(\"plot_density\", \"Usual density\"),\n       col=c(\"darkgray\", \"blue\"), lty=c(1, 2), cex=0.7)"},{"path":"improved-density-estimation.html","id":"real-data-with-repetitions","chapter":"108 Improved density estimation","heading":"108.4.3 Real data with repetitions","text":"now center case real data may repetitions, e.g. mortgage loan amounts frequent observe loans multiples $1,000.case also manually tune bandwidth changing bw parameter. Due span data, density lower probability mass, however, lines still plotted let us know repetitions displaying. helps avoid false impression many values around first last modalities displayed blue density.","code":"\nset.seed(123)\nloans <- c(rep(1000, 50), rep(2000, 40), rep(3000, 60), rep(4000, 35), rep(5000, 45), rnorm(500, 3000, 800) )\nplot_density(loans, bw=100)\nlines(density(loans), col=\"blue\", lty=2)\nlegend(\"topleft\", legend=c(\"plot_density\", \"Usual density\"),\n       col=c(\"darkgray\", \"blue\"), lty=c(1, 2), cex=0.7)"},{"path":"improved-density-estimation.html","id":"real-data-without-repetitions","chapter":"108 Improved density estimation","heading":"108.4.4 Real data without repetitions","text":"finally focus general case real data repetitions, case plot_density closely resembles stats::density:","code":"\nset.seed(123)\ndata <- rt(1000, 3) \nplot_density(data)\nlines(density(data), col=\"blue\", lty=2)\nlegend(\"topright\", legend=c(\"plot_density\", \"Usual density\"),\n       col=c(\"darkgray\", \"blue\"), lty=c(1, 2), cex=0.7)"},{"path":"improved-density-estimation.html","id":"conclussion","chapter":"108 Improved density estimation","heading":"108.5 Conclussion","text":"introduced function, plot_density, can helpful univariate visualization cases strictly non-negative non-positive data possible pressence repetitions.generalizes stats::density using truncated normal kernels continuous density, mixing discrete probability mass components repetitions found.","code":""},{"path":"improved-density-estimation.html","id":"references-12","chapter":"108 Improved density estimation","heading":"108.6 References","text":"Silverman, B. W. (1986). Density Estimation. London: Chapman Hall.Silverman, B. W. (1986). Density Estimation. London: Chapman Hall.Venables, W.N. & Ripley, B.D. (2002). Modern Applied Statistics S, Springer, 4th edition.Venables, W.N. & Ripley, B.D. (2002). Modern Applied Statistics S, Springer, 4th edition.","code":""},{"path":"plotting-theme-for-columbia.html","id":"plotting-theme-for-columbia","chapter":"109 Plotting theme for Columbia","heading":"109 Plotting theme for Columbia","text":"Jonathan Huynh","code":"\n# install.packages(\"showtext\", dependencies = TRUE)\nlibrary(openintro)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(showtext)\ntheme_columbia_u <- function(x_lab_rot = 0) {\n  font_add_google(\"Cinzel\", \"Cinzel\") # Similar to Trajan\n  font_add_google(\"Montserrat\", \"Montserrat\") # Similar to Gotham\n  showtext_auto()\n  title_font <- \"Cinzel\"\n  font <- \"Montserrat\"\n  \n  theme_classic() %+replace%\n    \n    theme(\n      axis.ticks = element_blank(),\n      \n      plot.title = element_text(\n                   family = title_font,\n                   size = 20,\n                   face = \"bold\",\n                   hjust = 0,\n                   vjust = 1),\n      \n      plot.subtitle = element_text(\n                   family = font,\n                   size = 14),\n      \n      plot.caption = element_text(\n                   family = font,\n                   size = 8,\n                   hjust = 1,\n                   vjust = 1),\n      \n      axis.title = element_text(\n                   family = font,\n                   size = 10),\n      \n      axis.text = element_text(\n                   family = font,\n                   size = 8),\n      \n      axis.text.x = element_text(angle = x_lab_rot, \n                                 hjust = 1,\n                                 margin=margin(5, b = 15)),\n      \n      legend.text = element_text(\n                   family = font,\n                   size = 8),\n      \n      legend.margin = margin(5)\n      \n    )\n}\n\ncolumbia_colors <- c(\n  `primaryBlue` = \"#1D4F91\",\n  `secondaryGray` = \"#53565A\",\n  `secondaryBlue` = \"#0077C8\",\n  `accentGreen` = \"#228848\",\n  `accentMagenta` = \"#AE2573\",\n  `accentOrange` = \"#FC4C02\",\n  `accentYellow` = \"#FFA300\",\n  `accentGray` = \"#75787B\")\n\ncolumbia_cols <- function(...) {\n  cols <- c(...)\n\n  if (is.null(cols))\n    return (columbia_colors)\n\n  columbia_colors[cols]\n}\n\ncolumbia_palettes <- list(\n  `main` = columbia_cols(\"primaryBlue\", \"secondaryBlue\", \"secondaryGray\"),\n  `secondary` = columbia_cols(\"secondaryGray\", \"secondaryBlue\"),\n  `coolAccent` = columbia_cols(\"accentGreen\", \"secondaryBlue\"),\n  `warmAccent` = columbia_cols(\"accentMagenta\", \"accentOrange\", \"accentYellow\"),\n  `accents` = columbia_cols(\"accentGreen\", \"accentMagenta\", \"accentOrange\", \"accentYellow\", \"accentGray\")\n)\n\ncolumbia_pal <- function(palette = \"main\", reverse = FALSE, ...) {\n  pal <- columbia_palettes[[palette]]\n  \n  if (reverse) pal <- rev(pal)\n  \n  colorRampPalette(pal, ...)\n}\n\nscale_color_columbia <- function(palette = \"main\", discrete = TRUE, reverse = FALSE, ...) {\n  pal <- columbia_pal(palette = palette, reverse = reverse)\n\n  if (discrete) {\n    discrete_scale(\"colour\", paste0(\"columbia_\", palette), palette = pal, ...)\n  } else {\n    scale_color_gradientn(colours = pal(256), ...)\n    print(\"error\")\n  }\n}\n\nscale_fill_columbia <- function(palette = \"main\", discrete = TRUE, reverse = FALSE, ...) {\n  pal <- columbia_pal(palette = palette, reverse = reverse)\n\n  if (discrete) {\n    discrete_scale(\"fill\", paste0(\"columbia_\", palette), palette = pal, ...)\n  } else {\n    scale_fill_gradientn(colours = pal(256), ...)\n  }\n}\niris <- iris\n\nggplot(iris, aes(Sepal.Width, Sepal.Length, color = Species)) +\n  geom_point(size = 4) +\n  labs(x = \"Sepal Width\", \n       y = \"Sepal Length\", \n       title = \"Sepal Length vs. Sepal Width\",\n       caption = \"Source: Anderson, Edgar  \\nBulletin of the American Iris Society,\\n1935 | Anderson, Edgar\") +\n  scale_color_columbia() +\n  theme_columbia_u()\n# Index Columbia colors to pass into fill\nggplot(ames, aes(fct_infreq(Roof.Style))) +\n    geom_bar(fill = columbia_colors[\"primaryBlue\"]) +\n    ggtitle(\"Number of Roof Styles\") +\n    xlab(\"Roof Style\") +\n    theme_bw(16) +\n    theme(panel.grid.major.x = element_blank()) +\n    scale_color_columbia() +\n    theme_columbia_u()\n# Packages if you want to import the DSI logo!\nif (!require(\"RCurl\", character.only = TRUE)) {\n    install.packages(\"RCurl\", dependencies = TRUE)\n}\nif (!require(\"png\", character.only = TRUE)) {\n    install.packages(\"png\", dependencies = TRUE)\n}\nlibrary(RCurl)\nlibrary(png)\ndsi_logo <- readPNG(getURLContent(\"https://datascience.columbia.edu/wp-content/uploads/2021/02/DSI-Vertical_000d74_RGB_Blue_091720.png\"))\n\nggplot(iris, aes(Sepal.Width, Sepal.Length, color = Species)) +\n  geom_point(size = 4) +\n  labs(x = \"Sepal Width\", \n       y = \"Sepal Length\", \n       title = \"Sepal Length vs. Sepal Width\",\n       caption = \"Source: Anderson, Edgar  \\nBulletin of the American Iris Society,\\n1935 | Anderson, Edgar\") +\n  scale_color_columbia(palette = \"secondary\") +\n  theme_columbia_u()\n# Use you want to add in the DSI logo or some other logo image\ngrid::grid.raster(dsi_logo, x = 0, y = 0, just = c('left', 'bottom'), width = unit(0.5, 'inches'), height = unit(0.5, 'inches'))\nggplot(iris, aes(Sepal.Width, Sepal.Length)) + \n  geom_tile(aes(fill = Species)) +\n  scale_fill_columbia(palette = \"coolAccent\") + \n  theme_columbia_u()\nggplot(mpg, aes(class, fill = class)) +\n  geom_bar() +\n  labs(title = \"Count of Manufacturers\") +\n  scale_fill_columbia(palette = \"warmAccent\") + \n  theme_columbia_u(x_lab_rot = 45)"},{"path":"geospatial-data-visualization.html","id":"geospatial-data-visualization","chapter":"110 Geospatial Data Visualization","heading":"110 Geospatial Data Visualization","text":"Nitya Krishna Kumar Binny Manojkumar NaikMaps great way showcase data tell story. powerful way depicting comparing events occur different locations time. general public, also intuitive engaging way understanding otherwise complicated data.Maps R good way visualizing Geospatial Data revealing underlying features connections different locations. Visualizing geospatial data aids communicate different variables correlate geographical locations layering variables maps. general, plot maps want study geographic locations based shape, size, distances places, based specific important features geographic location.","code":""},{"path":"geospatial-data-visualization.html","id":"geospatial-data","chapter":"110 Geospatial Data Visualization","heading":"110.0.1 Geospatial Data","text":"Geospatial data used build map visualizations typically contain following information:\n- Location (Latitude Longitude)\n- Attribute (Name location, population, event data, etc)\noccasions may also contain temporal data.","code":""},{"path":"geospatial-data-visualization.html","id":"types-of-maps","chapter":"110 Geospatial Data Visualization","heading":"110.1 Types of Maps","text":"many different types maps can created using R listed follows:\n) Background Map\nii) Choropleth\niii) Hexbin map\niv) Cartogram\nv) Connection\nvi) Bubble mapIn tutorial, concentrate three useful common maps:\n) Chloropleth Map.\nii) Bubble Map.\niii) Connection Map.","code":""},{"path":"geospatial-data-visualization.html","id":"getting-the-data","chapter":"110 Geospatial Data Visualization","heading":"110.1.1 Getting the Data","text":"Packages needed:Let’s use USArrests dataset R create chloropleth bubble map. information dataset can found : https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/USArrestsThis dataset provides us various attributes lack coordinate information. include , merged USArrests dataset state subset map_data function ggplot2. Specific countries regions can specified map_data. examples include map_data(\"world\"), map_data(\"county\"), map_data(\"UK\").information map_data function can found : https://www.rdocumentation.org/packages/ggplot2/versions/3.3.5/topics/map_dataFor purpose tutorial, use state data map_data. following important attributes:\n- coordinates: long lat depict state boundaries (like (x,y) points).\n- group: denotes whether two adjacent points connected line.\n- order: order connect points.\n- region: name state point belongs .Let’s merge state data USArrests data.maps package can used plot, however concentrate using ggplot2.","code":"\nlibrary(ggplot2) # to visualize the map\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(maps) # boundaries of common places such as continents, countries, states, and counties\nlibrary(viridis)\nlibrary(geosphere)\nhead(USArrests, 10)\nstates_map <- map_data(\"state\")\nhead(states_map, 10)\ncrimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrime_map <- merge(states_map, crimes, by.x = \"region\", by.y = \"state\")\ncrime_map <- arrange(crime_map, group, order)\nhead(crime_map, 10)"},{"path":"geospatial-data-visualization.html","id":"chloropleth","chapter":"110 Geospatial Data Visualization","heading":"110.1.2 Chloropleth","text":"Chloropleth Maps great showing depicting trends based numeric attribute color. addition numeric data, Chloropleth maps require geospatial object provides geographic region boundaries. visually pleasing intuitive. following map compares number rape cases across states. Note Alaska Hawaii included including regions make overall map look compressed thus difficult read.Let’s look important aspects:\n- geom_polygon useful : drawing USA map specified geographic boundary conditions.\n- coord_map(\"mercator\") specifies use common mercator map projection.information coord_map() can found : https://ggplot2.tidyverse.org/reference/coord_map.html.\ninformation mercator projection can found : https://desktop.arcgis.com/en/arcmap/latest/map/projections/mercator.htmThe graph uses default color scheme slightly unintuitive. Let’s change fill:Advantages Chloropleth Maps:\n) obvious advantage Chloropleth Maps widely popular frequantly use want plot value levels indicate average values numeric variable global local geographic areas based graduated color scale.Disadvantages Chloropleth Maps:\n) work average values, get detailed information internal conditions features.\nii) difficult interpret changing conditions boundaries plots based insignificant color changes.\niii) Choropleth maps works constant density within areas interest. real object density varies, expressiveness map gets distorted.","code":"\ncrime1 <- ggplot(crime_map, aes(x = long, y = lat, group = group, fill = Rape)) +\n  geom_polygon(colour = \"black\") +\n  coord_map(\"mercator\")\ncrime1\ncrime_p <- ggplot(crimes, aes(map_id = state, fill = Rape)) +\n  geom_map(map = states_map, colour = \"black\") +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  coord_map(\"mercator\") + xlab(\"long\") + ylab(\"lat\")\n\ncrime_p +\n  scale_fill_gradient2(low = \"yellow\", mid = \"thistle1\", high = \"red4\",\n                       midpoint = median(crimes$Rape))"},{"path":"geospatial-data-visualization.html","id":"bubble-maps","chapter":"110 Geospatial Data Visualization","heading":"110.1.3 Bubble Maps","text":"way choloropleth map uses color depict numeric attribute/value pertaining region, bubble map uses circles varying sizes. Theses circles can plotted two ways:\n- one bubble geographic coordinate\n- one bubble per regionIn latter case, required find center region plot circle.\ncan done following manner:center, lets plot circles map.Bubble maps easy way compare two attributes well. can done adding color scale circles. Let’s compare assault cases rape cases.Advantages Bubble Maps:\n) useful rendering relative comparisons among numeric variable interest.Disadvantages Bubble Maps:\n) difficult estimate actual values just based circle sizes.\nii) used larger datasets becomes difficult read overlapping circles draw kind meaningful insights.","code":"\nstate_centroids <- crime_map %>%\n  group_by(region) %>%\n  summarize(long = mean(range(long)), \n            lat = mean(range(lat)), \n            rape = mean(range(Rape)),\n            assault = mean(range(Assault)))\nnames(state_centroids)[1] <- \"state\"\nhead(state_centroids, 10)\nbubble <- ggplot(crimes, aes(map_id = state)) +\n  geom_map(map = states_map, colour = \"white\") +\n  geom_point(data = state_centroids, aes(x=long, y=lat, size=rape), color=\"salmon\") +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  scale_size_continuous(range=c(1,10)) +\n  #scale_color_viridis(trans=\"log\", direction = -1) +\n  ylim(25,50) +\n  xlim(-125, -70) +\n  coord_map(\"mercator\") \n\nbubble\nbubble <- ggplot(crimes, aes(map_id = state)) +\n  geom_map(map = states_map, colour = \"white\") +\n  geom_point(data = state_centroids, aes(x=long, y=lat, size=rape, color=assault)) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  scale_size_continuous(range=c(1,10)) +\n  scale_color_viridis(trans=\"log\", direction = -1) +\n  ylim(25,50) +\n  xlim(-125, -70) +\n  coord_map(\"mercator\") \n\nbubble"},{"path":"geospatial-data-visualization.html","id":"connection-map","chapter":"110 Geospatial Data Visualization","heading":"110.1.4 Connection Map","text":"connection Map great way show connections(route) several positions map. points map shows possible locations interest lines connect points depicts route exists two points.tutorial, make use gcIntermediate() function geosphere package works draw shortest routes two locations instead just making straight lines.explain use Connection Maps, make use USA Flights dataset. information dataset can found : https://www.kaggle.com/flashgordon/usa-airport-datasetThis dataset contains information number flights, number passenger, number seats associated flights flew different origin airports USA destination airports. also provides longitude latitude values origin, destination airports cities.comparison purposes, plotted map dsiplay possible locations (form points), flights JFK airport New York throughout given dataset fly . Moreover, lines depicts actual flights scheduled JFK airport New York different locations 1st December 2009 (last date available dataset)Overall, plotted background USA map using map() function gave customized color palette make individual states distinctly visible. Moreover, filtered dataset locations flights JFK scheduled throughout years given dataset. Next, cleaned data remove NAN values longitude latitude. Furthermore, used points() function plot destination airports map black color.Now, specific two different dates filtered data based fly date flights also filtered actual destination airports flights JFK flew particular dates. filtering done using filter() function. points() function used plot origin airport JFK red color. rbind() function used bind rows contains data destination airports particular dates interest. Eventually, dataframe get using utilized label destination airports maps.points plotted, next task establish lines showing routes airports. purpose, use gcIntermediate() establish link origin destination airports. Finally, links drawn form lines using lines() function.Advantages Connection Maps:\n) efficient visualizing geospatial data connections different locations need interpreted.\nii) Allows track path different location sets.Disadvantages Connection Maps:\n) becomes difficult read understand paths overlapping lines showing route.\nii) larger data points, overall connection plot becomes clumsy crowded points lines gets difficult interpret obvious details well.","code":"\ndf <- read_csv(\"Airports2.csv\")\nhead(df, 10)\n#map showing the number of flights that flew from JFK airport to different locations on 1st December 2009.\npar(mar=c(0,0,0,0))\ncc<-c(\"#DF536B\",\"#61D04F\",\"#2297E6\",\"#28E2E5\",\"#CD0BBC\",\"#F5C710\",\"gray62\" )\nmaps::map('state', fill = TRUE,col = cc, bg = \"grey\")\n\ndf <- filter(df, df$Origin_airport == \"JFK\" )\ndf <- na.omit(df, cols = c(\"Dest_airport_long\", \"Dest_airport_lat\"))\n\npoints(x = df$Dest_airport_long, y = df$Dest_airport_lat, col=\"black\", cex=0.8, pch=20)\n\ndf <- filter(df, df$Fly_date == \"2009-12-01\")\ndf <- filter(df, df$Destination_airport == \"ATL\" | \n                 df$Destination_airport == \"BGR\" | \n                 df$Destination_airport == \"BOS\" | \n                 df$Destination_airport == \"CAK\" | \n                 df$Destination_airport == \"DFW\" | \n                 df$Destination_airport == \"DOV\" |\n                 df$Destination_airport == \"MIA\"\n)\npoints(x = df$Org_airport_long, y = df$Org_airport_lat, col=\"red\", cex=2, pch=20)\n\ndon = rbind(JFK=c(-73.8, 40.6),ATL=c(-84.4281, 33.6367),DOV=c(-75.466, 39.1295),MIA=c(-80.2906, 25.7932),BOS=c(-71.0052, 42.3643),BGR=c(-68.8281, 44.8074),DFW=c(-97.038, 32.8968)) %>% as.data.frame()\ncolnames(don) = c(\"long\",\"lat\")\n\nJFK <- c(-73.8, 40.6)\nATL <- c(-84.4281, 33.6367)\nDOV <- c(-75.466, 39.1295)\nMIA <- c(-80.2906, 25.7932)\nBOS <- c(-71.0052, 42.3643)\nBGR <- c(-68.8281, 44.8074)\nDFW <- c(-97.038, 32.8968)\n\ninter1 <- gcIntermediate(JFK, DOV, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter2 <- gcIntermediate(JFK, MIA, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter3 <- gcIntermediate(JFK, BOS, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter4 <- gcIntermediate(JFK, BGR, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter5 <- gcIntermediate(JFK, DFW, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter6 <- gcIntermediate(JFK, ATL, n=50, addStartEnd=TRUE, breakAtDateLine=F)\n\nlines(inter1, col=\"pink\", lwd=2)\nlines(inter2, col=\"pink\", lwd=2)\nlines(inter3, col=\"pink\", lwd=2)\nlines(inter4, col=\"pink\", lwd=2)\nlines(inter5, col=\"pink\", lwd=2)\nlines(inter6, col=\"pink\", lwd=2)\n\ntext(rownames(don),x=don$long,y=don$lat,col=\"white\",cex=0.8,pos=2)\ndf1 <- read_csv(\"Airports2.csv\")\n\npar(mar=c(0,0,0,0))\ncc<-c(\"#DF536B\",\"#61D04F\",\"#2297E6\",\"#28E2E5\",\"#CD0BBC\",\"#F5C710\",\"gray62\" )\nmaps::map('state', fill = TRUE,col = cc, bg = \"grey\")\n\ndf1 <- filter(df1, df1$Origin_airport == \"JFK\" )\ndf1 <- na.omit(df1, cols = c(\"Dest_airport_long\", \"Dest_airport_lat\"))\n\npoints(x = df1$Dest_airport_long, y = df1$Dest_airport_lat, col=\"black\", cex=0.8, pch=20)\n\ndf1 <- filter(df1, df1$Fly_date == \"1990-01-01\")\ndf1 <- filter(df1, df1$Destination_airport == \"ATL\" | \n               df1$Destination_airport == \"BOS\" | \n               df1$Destination_airport == \"BUF\" | \n               df1$Destination_airport == \"CPR\" |\n               df1$Destination_airport == \"DAY\" | \n               df1$Destination_airport == \"DFW\" | \n               df1$Destination_airport == \"DTW\" |\n               df1$Destination_airport == \"HOU\" | \n               df1$Destination_airport == \"IAH\" | \n               df1$Destination_airport == \"MIA\" |\n               df1$Destination_airport == \"ORD\" | \n               df1$Destination_airport == \"TPA\" \n)\npoints(x = df1$Org_airport_long, y = df1$Org_airport_lat, col=\"red\", cex=2, pch=20)\n\ndon1 = rbind(ATL=c(-84.4281, 33.6367),BOS=c(-71.0052, 42.3643),BUF=c(-78.7322, 42.9405),CPR=c(-106.464, 42.908),DAY=c(-84.2194, 39.9024),DFW=c(-97.038, 32.8968),DTW=c(-83.3534, 42.2124), HOU=c(-95.2789, 29.6465), IAH=c(-95.3414, 29.9844), MIA=c(-80.2906, 25.7932), ORD=c(-87.9048, 41.9786), TPA=c(-82.5332, 27.9755)) %>% as.data.frame()\ncolnames(don1) = c(\"long\",\"lat\")\n\nATL <- c(-84.4281, 33.6367)\nBOS <- c(-71.0052, 42.3643)\nBUF <- c(-78.7322, 42.9405)\nCPR <- c(-106.464, 42.908)\nDAY <- c(-84.2194, 39.9024)\nDFW <- c(-97.038, 32.8968)\nDTW <- c(-83.3534, 42.2124)\nHOU<- c(-95.2789, 29.6465)\nIAH <- c(-95.3414, 29.9844)\nMIA <- c(-80.2906, 25.7932)\nORD <- c(-87.9048, 41.9786)\nTPA <- c(-82.5332, 27.9755)\n\ninter1 <- gcIntermediate(JFK, ATL, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter2 <- gcIntermediate(JFK, BOS, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter3 <- gcIntermediate(JFK, BUF, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter4 <- gcIntermediate(JFK, CPR, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter5 <- gcIntermediate(JFK, BOS, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter6 <- gcIntermediate(JFK, DAY, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter7 <- gcIntermediate(JFK, DFW, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter8 <- gcIntermediate(JFK, DTW, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter9 <- gcIntermediate(JFK, HOU, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter10 <- gcIntermediate(JFK, IAH, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter11<- gcIntermediate(JFK, MIA, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter12 <- gcIntermediate(JFK, ORD, n=50, addStartEnd=TRUE, breakAtDateLine=F) \ninter13 <- gcIntermediate(JFK, TPA, n=50, addStartEnd=TRUE, breakAtDateLine=F) \n\nlines(inter1, col=\"pink\", lwd=2)\nlines(inter2, col=\"pink\", lwd=2)\nlines(inter3, col=\"pink\", lwd=2)\nlines(inter4, col=\"pink\", lwd=2)\nlines(inter5, col=\"pink\", lwd=2)\nlines(inter6, col=\"pink\", lwd=2)\nlines(inter7, col=\"pink\", lwd=2)\nlines(inter8, col=\"pink\", lwd=2)\nlines(inter9, col=\"pink\", lwd=2)\nlines(inter10, col=\"pink\", lwd=2)\nlines(inter11, col=\"pink\", lwd=2)\nlines(inter12, col=\"pink\", lwd=2)\nlines(inter13, col=\"pink\", lwd=2)\n\ntext(rownames(don1),x=don1$long,y=don1$lat,col=\"white\",cex=0.8,pos=2)"},{"path":"geospatial-data-visualization.html","id":"references-13","chapter":"110 Geospatial Data Visualization","heading":"110.2 References","text":"https://www.sciencedirect.com/topics/computer-science/geospatial-datahttps://www.r-graph-gallery.com/map.html","code":""},{"path":"neo4j-python-implementation.html","id":"neo4j-python-implementation","chapter":"111 Neo4j python implementation","heading":"111 Neo4j python implementation","text":"Yuxin CuiGithub Link:\nhttps://github.com/yc4007/5702","code":""},{"path":"super-ggformat.html","id":"super-ggformat","chapter":"112 Super ggformat","heading":"112 Super ggformat","text":"Zihan Wang","code":""},{"path":"super-ggformat.html","id":"introduction-26","chapter":"112 Super ggformat","heading":"112.1 Introduction","text":"ggformat (https://github.com/jtr13/ggformat) add-tool clean style ggplot2 code, useful tidy single sentence writing R code.However, ggformat perfect – although works well single ggplot2 sentence, ’s power limited handle multiple sentences, long sentences comments. Therefore, propose ggformat++ can solve aforementioned drawbacks ggformat. Github Repo ggformat++ : (https://github.com/hannawong/super_ggformat)","code":""},{"path":"super-ggformat.html","id":"how-it-differs-from-ggformat","chapter":"112 Super ggformat","heading":"112.2 How it differs from ggformat","text":"","code":""},{"path":"super-ggformat.html","id":"handle-multiple-sentences","chapter":"112 Super ggformat","heading":"112.2.1 Handle Multiple Sentences","text":"salient difference ggformat ggformat++deals multiple sentences.original ggformat fails generate correct code used multiple sentences::5 “sentences” code block – first three sentences imports libraries, fourth sentence modifies dataframe df, last sentence draw plot using parcoords. However, ggformat clearly differentiate sentences result wrong answer simply stacking sentences together:However, ggformat++ developed can identify different “sentences” split , ggformat++ also moves library() import sentence top code block. output ggformat++ shown , “sentence” split new line.source code, use function def prologue(full_str), def parse_sentences(str_collapse),def parse_small_sentences(large_sentence) split sentences according \\n , <- , right parenthesis ). use function def arrange_sentence_order(atom_sentences) rearrange library() sentence top code block.","code":"\n# BEFORE\nlibrary(parcoords)\nlibrary(webshot)\nlibrary(d3r)\nsel_df_<-df %>%filter(Year == 2020) %>%select(County,Region,Murder,Rape,Robbery)%>%group_by(County,Region)\nparcoords(data = sel_df_,brushMode = '1D-axes',color = list(colorBy = \"Region\"),queue = TRUE,withD3 = TRUE)##After using ggformat\nlibrary(parcoords)library(webshot)library(d3r)sel_df_<-df %>%\n  filter(Year == 2020) %>%\n  select(County,Region,Murder,Rape,Robbery)%>%\n  group_by(County,Region)parcoords(data = sel_df_,brushMode = '1D-axes',color = list(colorBy = \"Region\"),queue = TRUE,withD3 = TRUE)\n## AFTER USING GGFORMAT++\nlibrary(d3r)\n\nlibrary(webshot)\n\nlibrary(parcoords)\n\nsel_df_<-df%>%\n    filter(Year==2020)%>%\n    select(County,Region,Murder,Rape,Robbery)%>%\n    group_by(County,Region)\n\nparcoords(data=sel_df_,brushMode='1D-axes',color=list(colorBy=\"Region\"),queue=TRUE,withD3=TRUE)"},{"path":"super-ggformat.html","id":"handle-comments","chapter":"112 Super ggformat","heading":"112.2.2 Handle Comments","text":"ggformat trouble handling comments, example:two comments code : \"###aaaaa\" \"####bbbb\", however, ggformat gives wrong answer mistakening codes comments:fixed problem ggplot++ identifying comments:","code":"\n##BEFORE\ndt <- seattlepets %>% ###aaaaa\n  filter(species %in% target) %>% group_by(animal_name, species) %>% ####bbbb\n  summarize(n = n()) %>%mutate(s = sum(n)) %>%filter(!is.na(animal_name)) %>%ungroup()\n##AFTER USING GGFORMAT\ndt <- seattlepets %>%\n  ###aaaaa  filter(species %in% target) %>%\n  group_by(animal_name, species) %>%\n  ####bbbb  summarize(n = n()) %>%\n  mutate(s = sum(n)) %>%\n  filter(!is.na(animal_name)) %>%\n  ungroup()\n##AFTER USING GGFORMAT++\ndt<-seattlepets%>%###aaaaa\n    filter(species%in%target)%>%\n    group_by(animal_name,species)%>%####bbbb\n    summarize(n=n())%>%\n    mutate(s=sum(n))%>%\n    filter(!is.na(animal_name))%>%\n    ungroup()"},{"path":"super-ggformat.html","id":"wrapping-long-sentences","chapter":"112 Super ggformat","heading":"112.2.3 Wrapping Long Sentences","text":"github ggformat, says ggformat trouble wrapping long sentences.implement function def wrap_long_sentences(sent) ggformat++ split long sentence comma, example mentioned can perfectly formatted ggformat++:ggformat++` can also deal extremely long sentences wrapping multiple times. example, extremely long sentence like :ggformat++ wraps long sentence mentioned four times:","code":"\n##BEFORE\nggplot() +\n  geom_ribbon(data = ribbon, aes(ymin = min, ymax = max, x = x.ribbon, fill = 'lightgreen')) +\n  geom_line(data = ribbon, aes(x = x.ribbon, y = avg, color = 'black')) +\n  geom_line(data = data, aes(x = x, y = new.data, color = 'red')) +\n  scale_fill_identity(name = 'the fill', guide = 'legend', labels = c('m1')) +\n  scale_colour_manual(name = 'the colour', values = c('black' = 'black', 'red' = 'red'), labels = c('c2', 'c1')) +\n  xlab('x') +\n  ylab('density')\n## AFTER USING GGFORMAT++\nggplot()+\n    geom_ribbon(data=ribbon,aes(ymin=min,ymax=max,x=x.ribbon,fill='lightgreen'))+\n    geom_line(data=ribbon,aes(x=x.ribbon,y=avg,color='black'))+\n    geom_line(data=data,aes(x=x,y=new.data,color='red'))+\n    scale_fill_identity(name='the fill',guide='legend',labels=c('m1'))+\n    scale_colour_manual(name='the colour',\n        values=c('black'='black','red'='red'),labels=c('c2','c1'))+\n    xlab('x')+\n    ylab('density')\n## BEFORE\nggplot() +\n  geom_ribbon(data = ribbon, aes(ymin = min, ymax = max, x = x.ribbon, fill = 'lightgreen')) +\n  geom_line(data = ribbon, aes(x = x.ribbon, y = avg, color = 'black')) +\n  geom_line(data = data, aes(x = x, y = new.data, color = 'red')) +\n  scale_fill_identity(name = 'the fill', guide = 'legend', labels = c('m1')) +\n  scale_colour_manual(name = 'the colour', values = c('black' = 'black', 'red' = 'red'),values = c('black' = 'black', 'red' = 'red'),values = c('black' = 'black', 'red' = 'red'),values = c('black' = 'black', 'red' = 'red'),values = c('black' = 'black', 'red' = 'red'),values = c('black' = 'black', 'red' = 'red'),\n    labels = c('c2', 'c1')) +\n  xlab('x') +\n  ylab('density')\n##AFTER USING GGFORMAT++\nggplot()+\n    geom_ribbon(data=ribbon,aes(ymin=min,ymax=max,x=x.ribbon,fill='lightgreen'))+\n    geom_line(data=ribbon,aes(x=x.ribbon,y=avg,color='black'))+\n    geom_line(data=data,aes(x=x,y=new.data,color='red'))+\n    scale_fill_identity(name='thefill',guide='legend',labels=c('m1'))+\n    scale_colour_manual(name='thecolour',values=c('black'='black',\n        'red'='red'),values=c('black'='black','red'='red'),values=c('black'='black',\n        'red'='red'),values=c('black'='black','red'='red'),values=c('black'='black',\n        'red'='red'),values=c('black'='black','red'='red'),labels=c('c2','c1'))+\n    xlab('x')+\n    ylab('density')"},{"path":"super-ggformat.html","id":"ways-to-improve-ggformat","chapter":"112 Super ggformat","heading":"112.3 Ways to improve ggformat++","text":"using hand-crafted rules definitely results bad cases. reliable way format code build compiler, know roles token (e.g. identifier, function, operators, comments…). Therefore, code can format according roles.developed compiler based minidecaf antlr year ago, C++. thought reusing code build compiler R, finally give workload designing context-free language describe full grammar R intimidating. However, believe developers R consider adding format module compiler R, give much reliable result ggformat ggformat++.Moreover, develop ggformat++ python. order build add-tool R, needs transplanted R language.","code":""},{"path":"data_composition_plot.html","id":"data_composition_plot","chapter":"113 Data_Composition_Plot","heading":"113 Data_Composition_Plot","text":"Xinfu Su Yihan Wang","code":"\nlibrary(viridis)\nlibrary(hrbrthemes)\nlibrary(waterfalls)\nlibrary(tidyverse)\n# create a dataset\nseason <- c(rep(\"2021-1\" , 3) , rep(\"2021-4\" , 3) , rep(\"2021-7\" , 3) , rep(\"2021-10\" , 3) )\ncondition <- rep(c(\"Healthy\" , \"Infected\" , \"Serious\") , 4)\ncount <- abs(rnorm(12, 100, 1000))\ndata <- data.frame(season,condition,count)\nggplot(data, aes(fill=condition, y=count, x=season)) + \n    geom_bar(position=\"stack\", stat=\"identity\")+\n    scale_fill_viridis(discrete = T, option = \"F\") +\n    ggtitle(\"COVID-19 Seasonal Behavior\") +\n    theme_ipsum()\nggplot(data, aes(fill=condition, y=count, x=season)) + \n    geom_bar(position=\"fill\", stat=\"identity\")+\n    scale_fill_viridis(discrete = T, option = \"F\") +\n    ggtitle(\"COVID-19 Seasonal Behavior\") +\n    theme_ipsum()\nggplot(data, aes(fill=condition, y=count, x=condition)) + \n    geom_bar(position=\"dodge\", stat=\"identity\")+facet_wrap(~season)+\n    scale_fill_viridis(discrete = T, option = \"G\")+\n    theme(legend.position=\"none\") +\n    ggtitle(\"COVID-19 Seasonal Behavior\")\nggplot(data, aes(fill=condition, y=count, x=season)) + \n    geom_bar(position=\"dodge\", stat=\"identity\")+\n    scale_fill_viridis(discrete = T, option = \"G\") +\n    ggtitle(\"COVID-19 Seasonal Behavior\") +\n    theme_ipsum()\ntime <- as.numeric(rep(seq(1,7),each=7))\nvalue <- runif(49, 10, 100)             \nemploy <- rep(LETTERS[1:7],times=7)  \ndata <- data.frame(time, value, employ)\n\nggplot(data, aes(x=time, y=value, fill=employ))+geom_area()+ \n    geom_area(alpha=0.5, size=0.1, colour=\"black\") +\n    scale_fill_viridis(discrete = T)+theme_ipsum()+\n    ggtitle(\"Daily Income of each Employee\")\ndata <- data  %>%\n  group_by(time, employ) %>%\n  summarise(n = sum(value)) %>%\n  mutate(percentage = n / sum(n))\n\nggplot(data, aes(x=time, y=percentage, fill=employ)) + \n    geom_area()+ \n    geom_area(alpha=0.5, size=0.1, colour=\"white\") +\n    scale_fill_viridis(discrete = T)+theme_ipsum()+\n    ggtitle(\"Daily   Income of each Employee\")\nslices <- c(10,5,2,16,17)\nbands <- c('Velvet Underground', 'Beatles', 'Nirvana', 'Pink Floyd', 'Dirty Fingers')\npie(slices, labels = bands, col=rainbow(length(bands)), main='Pie Chart of Bands')\nslices <- c(10,5,2,16,17)\nbands <- c('Velvet Underground', 'Beatles', 'Nirvana', 'Pink Floyd', 'Dirty Fingers')\npct <- round(slices/sum(slices)*100)\nbands <- paste(bands, pct)\nbands <- paste(bands,\"%\",sep=\"\")\npie(slices, labels=bands,col=rainbow(length(bands)), clockwise = TRUE, main='Pie Chart of Bands With Percentage')\nvalue <- c(1000, 3000, -2000, 600, -2500)\nbands <- c('Velvet Underground', 'Beatles', 'Nirvana', 'Pink Floyd', 'Dirty Fingers')\ndf <- data.frame(x = bands,y = value)\nwaterfall(df, calc_total = TRUE, rect_width = 0.3, linetype = 1)\nwaterfall(df, fill_by_sign = FALSE, fill_colours = 2:7)"},{"path":"ggplot2_treemapify.html","id":"ggplot2_treemapify","chapter":"114 ggplot2_treemapify","heading":"114 ggplot2_treemapify","text":"","code":""},{"path":"ggplot2_treemapify.html","id":"download-treemapify","chapter":"114 ggplot2_treemapify","heading":"114.1 Download treemapify","text":"August 2017, R language added new geometric objects supporting ggplot2 dendrograms. , need resort assistance third-party packages. can downloaded either using “install.package(”treemapify“)” “devtools::install_github(”wilkox/treemapify“)”.downloading package, additional functon ggplot2 named “geom_treemap()”.","code":"\n## Load packages needed for this instruction\nlibrary(\"ggplot2\")\nlibrary(\"treemapify\")\nlibrary(\"RColorBrewer\")"},{"path":"ggplot2_treemapify.html","id":"basic-visualization-of-treemap","chapter":"114 ggplot2_treemapify","heading":"114.2 Basic Visualization of Treemap","text":"give example using treemapify package draw treemap figure, can first get know kind figures:tree diagram, numerical variables converted rectangular area sizes, types variables distinguished labels.","code":"\nexample <- data.frame(value<-c(100, 23, 1300, 1083, 260, 17, 30),\n                      kind<-c('A','B','C','D','E','F','G'))\nexample$kind <- as.factor(example$kind)\nggplot(example, aes(area=value,fill=kind,label=kind)) +\n  geom_treemap()+\n  geom_treemap_text(place='center')\nlibrary(treemap)\ngroup=c(rep(\"group-1\", 3), rep(\"group-2\",2), rep(\"group-3\",4))\nsubgroup=paste(\"subgroup\" , c(1,2,3,1,2,1,2,3,4), sep=\"-\")\nvalue=c(10, 6, 7, 9, 10, 7, 2, 2,20)\ndata=data.frame(group,subgroup,value)\n\n# treemap\ntreemap(data,\n        index=c(\"group\",\"subgroup\"),   #sub plots two groups\n        vSize=\"value\",  # Sizes are allocated according to the numeric variable\n        type=\"index\")  # Color according to the classification\ntreemap(data, index=c(\"group\",\"subgroup\"), vSize=\"value\", \n   type=\"index\",              \n   palette = \"Set3\",\n  title=\"TreemapExample\") "},{"path":"ggplot2_treemapify.html","id":"visualization-using-treemapify-package","chapter":"114 ggplot2_treemapify","heading":"114.3 Visualization using treemapify package","text":"example using data set G20 included treemapfiy package. can first preview data set:data set describes economic indicators 20 countries participating summit. contains five fields, namely global region (region), country name (country), GDP indicator (gdp_mil_usd) (kind secondary calculation Indicator), human development index (hdi), already economic development level (econ_classification).tree diagram special type graph without explicit coordinate system. Relying square algorithm, sample population square divided single rectangular box according actual observation value accounted population. Therefore, needs least one numeric variable input parameter.","code":"\nstr(G20)## 'data.frame':    20 obs. of  6 variables:\n##  $ region             : Factor w/ 8 levels \"Africa\",\"Asia\",..: 1 6 6 6 8 8 2 2 2 2 ...\n##  $ country            : Factor w/ 20 levels \"Argentina\",\"Australia\",..: 16 20 4 13 3 1 5 12 17 9 ...\n##  $ gdp_mil_usd        : int  384315 15684750 1819081 1177116 2395968 474954 8227037 5963969 1155872 1824832 ...\n##  $ hdi                : num  0.629 0.937 0.911 0.775 0.73 0.811 0.699 0.912 0.909 0.554 ...\n##  $ econ_classification: Factor w/ 2 levels \"Advanced\",\"Developing\": 2 1 1 2 2 2 2 1 1 2 ...\n##  $ hemisphere         : Factor w/ 2 levels \"Northern\",\"Southern\": 2 1 1 1 2 2 1 1 1 1 ...\nhead(G20)##          region       country gdp_mil_usd   hdi econ_classification hemisphere\n## 1        Africa  South Africa      384315 0.629          Developing   Southern\n## 2 North America United States    15684750 0.937            Advanced   Northern\n## 3 North America        Canada     1819081 0.911            Advanced   Northern\n## 4 North America        Mexico     1177116 0.775          Developing   Northern\n## 5 South America        Brazil     2395968 0.730          Developing   Southern\n## 6 South America     Argentina      474954 0.811          Developing   Southern"},{"path":"ggplot2_treemapify.html","id":"plot-a-simple-treemap-figure","chapter":"114 ggplot2_treemapify","heading":"114.3.1 Plot a simple Treemap figure","text":"area defines square size numeric variable, fill color can defined separately. color can often used alone way expressing numerical metric.","code":"\nggplot(G20, aes(area = gdp_mil_usd)) + \n geom_treemap()\n## Draw the plot with simple blue color\nggplot(G20, aes(area = gdp_mil_usd)) + \n geom_treemap(fill=\"light blue\")\n## Draw the plot using scale_fill_distiller with palette\nggplot(G20, aes(area = gdp_mil_usd, fill = hdi)) + \n geom_treemap()+\n scale_fill_distiller(palette=\"Blues\")"},{"path":"ggplot2_treemapify.html","id":"add-labels-onto-the-treemap","chapter":"114 ggplot2_treemapify","heading":"114.3.2 Add labels onto the treemap","text":"author package wrote optimized text label function geom_treemap_text ggplot treemap (treemap exceeds category three traditional coordinate systems. explicit coordinate system. algorithm relatively special one use Geom_text() add tags).place parameter controls position label box relative surroundings, grow (set default True) controls whether label adaptive box size (largely scaled )","code":"\n  ggplot(G20, aes(area = gdp_mil_usd, fill = hdi, label = country)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"Blue\", \n                    grow = TRUE, # default value\n                    place = \"center\") +\n  scale_fill_distiller(palette=\"Blues\")"},{"path":"ggplot2_treemapify.html","id":"subgroups","chapter":"114 ggplot2_treemapify","heading":"114.3.3 Subgroups","text":"package supports sub-groups, extensively applied practical application scenarios. example, observing size national indicators, also want obtain overall indicators region country belongs. adding sub-groups, can obtain information two dimensions. setting subgroup parameter (categorical variable) aesthetic mapping, function can automatically complete variable aggregation calculation subgroup within function, use frame show size subcategory graph formation.reflow parameter used control whether label adapts size rectangular block. original size exceeds rectangular block, automatically displayed new line.feel using sub-group achieve good visualization, geom_treemap also supports facet_grid function ggplot2. benefit ggplot2 extension functions can inherit advanced graphics properties derived ggplot2.","code":"\n  ggplot(G20, aes(area = gdp_mil_usd, \n                   fill = hdi, \n                   label = country,\n                   subgroup = region)) +\n  geom_treemap() +\n  geom_treemap_subgroup_border() +\n  geom_treemap_subgroup_text(place = \"center\", \n                             alpha = 0.3, \n                             colour =\"black\", \n                             min.size = 0) +\n  geom_treemap_text(colour = \"Blue\", \n                    place = \"topleft\", \n                    reflow = TRUE)+\n  scale_fill_distiller(palette=\"Blues\")\n  ggplot(G20, aes(area = gdp_mil_usd, \n                   fill = region, \n                   label = country)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"black\") +\n  facet_wrap( ~ econ_classification) +\n  scale_fill_brewer(palette=\"Blues\")+\n  labs(title = \"The G-20 major economies by economic classification\",\n       caption = \"The area of each country is proportional to its relative GDP within the economic group (advanced or developing)\",\n       fill = \"Region\" ) +\n  theme(legend.position = \"bottom\",\n        plot.caption=element_text(hjust=0)) "},{"path":"ggplot2_treemapify.html","id":"source","chapter":"114 ggplot2_treemapify","heading":"114.4 Source","text":"https://cran.r-project.org/web/packages/treemapify/vignettes/introduction--treemapify.html\\https://github.com/wilkox/treemapify","code":""},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"tutorial-for-scatter-plot-with-marginal-distribution","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115 Tutorial for scatter plot with marginal distribution","text":"Ziyu FangThis tutorial introduces draw customize scatter plots marginal distribution map boundaries, including marginal histogram marginal density plot.mainly two approaches commonly used.simple method: using two R libraries, ggExtra ggplot2. way, can generate marginal distribution map easily.advanced method: using two R libraries. cowplot ggpubr. approach allows users flexiblity designing drawing plots.","code":""},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"using-ggextra-with-ggplot2","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.1 Using ggExtra with ggplot2","text":"","code":"\nlibrary(ggplot2)\nlibrary(ggExtra)\nlibrary(RColorBrewer)\nlibrary(cowplot)\nlibrary(ggpubr)"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"traditional-scatter-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.1.1 Traditional Scatter Plot","text":"sample code draws simple 2D scatter plot. assign color species following plot.Using ggExtra ggplot2 literally take one line code generate marginal histogram density map. want overlap marginal dsitributions assigned colors, can choose set groupColour TRUE, recommend use. parameter passes assigned colors code marginal distribution. Also remember set groupFill TRUE. parameter quite self-explanatory, fill marginal distribution color assigned groupColour.method, can choose several methods display marginal distribution, provided ggExtra ggplot2. recommend 5 types:","code":"\np <- ggplot(iris) +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species), alpha = 0.6, shape = 16) +  \n  scale_color_brewer(palette = \"Dark2\") + \n  theme_bw() +\n  theme(legend.position = \"bottom\") + \n  labs(x = \"Sepal Length\", y = \"Sepal Width\") \np"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"plot-marginal-density-function","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.1.2 Plot Marginal Density Function","text":"","code":"\nggMarginal(p, type = \"density\", groupColour = TRUE, groupFill = TRUE)\nggMarginal(p, type = \"histogram\", groupColour = TRUE, groupFill = TRUE)\nggMarginal(p, type = \"boxplot\", groupColour = TRUE, groupFill = TRUE)"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"plot-marginal-violin-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.1.3 Plot Marginal Violin Plot","text":"ggExtra ggplot2 provide flexibility, allows customise marginal distribution. believe ’s designer want . whole point libraries wrap whole complicated mechanism let quicklt draw standard scatter plot marginal distributions.can come original designs implement ggExtra ggplot2. believe , unideal process. users like design marginal distribution, recommend next method.","code":"\nggMarginal(p, type = \"violin\", groupColour = TRUE, groupFill = TRUE)\nggMarginal(p, type = \"densigram\", groupColour = TRUE, groupFill = TRUE)"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"using-cowplot-and-ggpubr","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.2 Using cowplot and ggpubr","text":"previous approach, let’s start scatter plot marginal distribution. figure add marginal distribution . want use cowplot ggpubr, recommend start scatter plot function ggscatter case, instead ggplot.","code":""},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"plot-another-kind-of-scatter-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.2.1 Plot Another Kind of Scatter Plot","text":"","code":"\nsp <- ggscatter(iris, x = \"Sepal.Length\", y = \"Sepal.Width\",\n                color = \"Species\", palette = \"Accent\",\n                size = 3, alpha = 0.6) +\n  border() +\n  theme(legend.position = \"bottom\")\nsp"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"gapped-marginal-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.2.2 Gapped Marginal Plot","text":"","code":""},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"marginal-density-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.2.2.1 Marginal Density Plot","text":"process like plotting 3 subplots: horizontal margin, vertical margin scatter plot .Since already scatter plot, first step generate two plot: horizontal margin vertical margin.Notice tplot vertical rotate 90 degree. Now can see really designing customising module ., need put together using plot_grid, might feel familiar . can adjust width, height, aligning method, many options manipulating built-parameters plot_grid.little tip: remove legends puting subplots together. Legends destroy pretty layout.Let’s revise whole process, challenge applying procedure draw boxplot marginal distribution.","code":"\n# Marginal density plot of x (top panel) and y (right panel)\nxplot <- ggdensity(iris, \"Sepal.Length\", fill = \"Species\",\n                   palette = \"Accent\")\nyplot <- ggdensity(iris, \"Sepal.Width\", fill = \"Species\", \n                   palette = \"Accent\") +\n  rotate()\n# Cleaning the plots\nsp <- sp + rremove(\"legend\")\nyplot <- yplot + clean_theme() + rremove(\"legend\")\nxplot <- xplot + clean_theme() + rremove(\"legend\")\n# Arranging the plot using cowplot\nplot_grid(xplot, NULL, sp, yplot, ncol = 2, align = \"hv\", \n          rel_widths = c(2, 1), rel_heights = c(1, 2))"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"uncompressed-marginal-box-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.2.2.2 Uncompressed Marginal Box Plot","text":"many customizations can decide process, ’s cowplot ggpubr idea solution creating original, non-standard marginal distribution. boss rediculously asks make horizaontal margin density map, vertical margin density map, now confident make happen.","code":"\n# Marginal boxplot of x (top panel) and y (right panel)\nxplot <- ggboxplot(iris, x = \"Species\", y = \"Sepal.Length\", \n                   color = \"Species\", fill = \"Species\", palette = \"Accent\",\n                   alpha = 0.5, ggtheme = theme_bw())+\n  rotate()\nyplot <- ggboxplot(iris, x = \"Species\", y = \"Sepal.Width\",\n                   color = \"Species\", fill = \"Species\", palette = \"Accent\",\n                   alpha = 0.5, ggtheme = theme_bw())\n# Cleaning the plots\nsp <- sp + rremove(\"legend\")\nyplot <- yplot + clean_theme() + rremove(\"legend\")\nxplot <- xplot + clean_theme() + rremove(\"legend\")\n# Arranging the plot using cowplot\nplot_grid(xplot, NULL, sp, yplot, ncol = 2, align = \"hv\", \n          rel_widths = c(2, 1), rel_heights = c(1, 2))"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"connect-marginal-plot","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.2.3 Connect Marginal Plot","text":"know people may dislike gaps scatter plot margin distributions. Unfortunately, want get rid , plot_grid replaced. use another function, axis_canvas combine marginal distribution axis creating margin. , libraries provide really use function use: insert_xaxis_grob. function job plot_grid previod example.clsoe code, find follows process previous example: creating 3 subplots connecting together. process just uses different, yet similar functions. Similarly, know customize .","code":"\n# Main plot\npmain <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  color_palette(\"Accent\")\n# Marginal densities along x axis\nxdens <- axis_canvas(pmain, axis = \"x\") +\n  geom_density(data = iris, aes(x = Sepal.Length, fill = Species),\n               alpha = 0.7, size = 0.2) +\n  fill_palette(\"Accent\")\n# Marginal densities along y axis\n# Need to set coord_flip = TRUE, if you plan to use coord_flip()\nydens <- axis_canvas(pmain, axis = \"y\", coord_flip = TRUE) +\n  geom_density(data = iris, aes(x = Sepal.Width, fill = Species),\n               alpha = 0.7, size = 0.2) +\n  coord_flip() +\n  fill_palette(\"Accent\")\np1 <- insert_xaxis_grob(pmain, xdens, grid::unit(.2, \"null\"), position = \"top\")\np2 <- insert_yaxis_grob(p1, ydens, grid::unit(.2, \"null\"), position = \"right\")\nggdraw(p2)"},{"path":"tutorial-for-scatter-plot-with-marginal-distribution.html","id":"sources-7","chapter":"115 Tutorial for scatter plot with marginal distribution","heading":"115.3 Sources","text":"https://cran.r-project.org/web/packages/ggExtra/index.htmlhttps://exts.ggplot2.tidyverse.org/ggExtra.htmlhttps://deanattali.com/2015/03/29/ggExtra-r-package/https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.htmlhttps://cran.r-project.org/web/packages/cowplot/index.htmlhttps://rpkgs.datanovia.com/ggpubr/","code":""},{"path":"d-data-and-potential-relationship-visualization.html","id":"d-data-and-potential-relationship-visualization","chapter":"116 3D data and potential relationship visualization","heading":"116 3D data and potential relationship visualization","text":"Yinghao LiThis project visualize data points 3D visualize potential relationship dimensions.hope introduce tool data visualization python highly used modeling tasks.Instead visualize 2D data, wish introduce methods plot data distribution linear relationships 3D space.GitHub Repo link: https://github.com/yinghao11/STAT5702CC","code":""},{"path":"using-python-in-r.html","id":"using-python-in-r","chapter":"117 Using python in r","heading":"117 Using python in r","text":"Xingyu LuNote: chunks set eval=FALSE since set run Python.1. Background\nR python two popular programming languages data scientists. Sometimes might hard decide one use, especially beginners.However, ’s easy us compare languages. One languages isn’t better another cases. example, R almost best language exploratory data analysis python readable can integrated part workflow easily. following figure comparison screenshot resource 1As beginner R, enjoyed beautiful plots R can make, also suffered obscure data processing functions limited online resources documentations., reticulate package can allow using python R becomes life saver R beginners already familiar python.2. reticulate packagereticulate package allows users use python modules, classes functions R. Although guarantee benefits python, can least give convenience.() Setup\nSurprisingly, ’s quite easy set python R Markdown. thing need include following block:path variable path pointing python using. find path, can following:Windows: use following code cmdMacOS: use follwing code terminal(b) UsageAfter setup, can use python block normally just r blocks R Markdown files: wrap python code ```{python} ``` just like R code.\nexample, want load wait list data HW3 Q3, couldYou can manipulate data Python, easier R.Note: useful function: py_install(python package)(c) Plot using Python packagesIt’s also possible make plots using python packages like matplotlib seaborn.\nexample, want see distribution final status students.(d) Integrate RUpto now, using Python functions. ’s like just treating RStudio another editor Python. can integrate python R, enjoy coding experience Python nice plots R time?Well, ’s quite simple.(d-1) ’s happening Python block R Markdown?\nrunning blocks RStudio, observe following messages Console:means console currently python mode, run python code console just like IDEs. course, can’t use R codes unless exit REPL return R, just message states.\nAlso, Environment, also observe following data variables:Notice longer observe variables Environment window exit REPL mode console.(d-2) Load variable REPL mode R modeThen pass preprocessed variable, df_status back R mode? Actually variable called py R environment, can access variables REPL mode via $ operator. Sowhich loads preprocessed data R mode.(d-3) Specific exampleRecall Q3 HW3. make plot, suffered lot preprocessing data R, code used homework follows:data used plot figure actually df_W. lot preprocessing steps like handling data type, filling missing value etc. must fancy powerful functions R allows accomplish task easily. However, largely increase productivity take advantage experience Python. , use Python redo process, bewhich finishes preprocessing thenWhich yields plot. code simplicity didn’t change lot. However, productivity increased: takes around 5 hours preprocess data R 20 minutes Python.3. Conclusion\nAlthough might unnecessary R expert use Python R Markdown, really useful R beginners little bit set take advantage using Python R.4. Resources\n1. https://www.datacamp.com/community/tutorials/r--python--data-analysis?utm_source=adwords_ppc&utm_campaignid=12492439679&utm_adgroupid=122563407961&utm_device=c&utm_keyword=python%20r&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=504158803093&utm_targetid=dsa-296652664266&utm_loc_interest_ms=&utm_loc_physical_ms=9073479&gclid=CjwKCAjwq9mLBhB2EiwAuYdMtVLKk_tdQSa2uN-sVjFzkaIiqZYLKwE7Qcd2iYZHHmpqd0EaXKOksBoCTLIQAvD_BwE\n2. https://cran.r-project.org/web/packages/reticulate/vignettes/calling_python.html\n3. https://stackoverflow.com/questions/647515/-can--find--python--installed--windows\n4. https://cran.r-project.org/web/packages/reticulate/vignettes/r_markdown.html\n=======\n# Using python rXingyu Lu1. Background\nR python two popular programming languages data scientists. Sometimes might hard decide one use, especially beginners.However, ’s easy us compare languages. One languages isn’t better another cases. example, R almost best language exploratory data analysis python readable can integrated part workflow easily. following figure comparison screenshot resource 1As beginner R, enjoyed beautiful plots R can make, also suffered obscure data processing functions limited online resources documentations., reticulate package can allow using python R becomes life saver R beginners already familiar python.2. reticulate packagereticulate package allows users use python modules, classes functions R. Although guarantee benefits python, can least give convenience.() Setup\nSurprisingly, ’s quite easy set python R Markdown. thing need include following block:path variable path pointing python using. find path, can following:Windows: use following code cmdMacOS: use follwing code terminal(b) UsageAfter setup, can use python block normally just r blocks R Markdown files: wrap python code ```{python} ``` just like R code.\nexample, want load wait list data HW3 Q3, couldYou can manipulate data Python, easier R.Note: useful function: py_install(python package)(c) Plot using Python packagesIt’s also possible make plots using python packages like matplotlib seaborn.\nexample, want see distribution final status students.(d) Integrate RUpto now, using Python functions. ’s like just treating RStudio another editor Python. can integrate python R, enjoy coding experience Python nice plots R time?Well, ’s quite simple.(d-1) ’s happening Python block R Markdown?\nrunning blocks RStudio, observe following messages Console:means console currently python mode, run python code console just like IDEs. course, can’t use R codes unless exit REPL return R, just message states.\nAlso, Environment, also observe following data variables:Notice longer observe variables Environment window exit REPL mode console.(d-2) Load variable REPL mode R modeThen pass preprocessed variable, df_status back R mode? Actually variable called py R environment, can access variables REPL mode via $ operator. Sowhich loads preprocessed data R mode.(d-3) Specific exampleRecall Q3 HW3. make plot, suffered lot preprocessing data R, code used homework follows:data used plot figure actually df_W. lot preprocessing steps like handling data type, filling missing value etc. must fancy powerful functions R allows accomplish task easily. However, largely increase productivity take advantage experience Python. , use Python redo process, bewhich finishes preprocessing thenWhich yields plot. code simplicity didn’t change lot. However, productivity increased: takes around 5 hours preprocess data R 20 minutes Python.3. Conclusion\nAlthough might unnecessary R expert use Python R Markdown, really useful R beginners little bit set take advantage using Python R.4. Resources\n1. https://www.datacamp.com/community/tutorials/r--python--data-analysis?utm_source=adwords_ppc&utm_campaignid=12492439679&utm_adgroupid=122563407961&utm_device=c&utm_keyword=python%20r&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=504158803093&utm_targetid=dsa-296652664266&utm_loc_interest_ms=&utm_loc_physical_ms=9073479&gclid=CjwKCAjwq9mLBhB2EiwAuYdMtVLKk_tdQSa2uN-sVjFzkaIiqZYLKwE7Qcd2iYZHHmpqd0EaXKOksBoCTLIQAvD_BwE\n2. https://cran.r-project.org/web/packages/reticulate/vignettes/calling_python.html\n3. https://stackoverflow.com/questions/647515/-can--find--python--installed--windows\n4. https://cran.r-project.org/web/packages/reticulate/vignettes/r_markdown.html","code":"\npath <- '/usr/local/opt/python@3.7/bin/python3'\nlibrary(reticulate)\nuse_python(path)python -c \"import os, sys; print(os.path.dirname(sys.executable))\"which pythonimport pandas as pd\ndf = pd.read_csv('stats_wl.csv')\ndf.head()import seaborn as sns\ndf_status = df[['Name', 'Status']].drop_duplicates()\nprint(df_status.shape)\nsns.set(font_scale=0.5)\nsns.countplot(y='Status', data=df_status).set_title('Countplot for Final Status of all Students')\ndata <- py$df_status\nhead(data)\nlibrary(ggalluvial)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tidyr)\ndf <- read.csv('resources/use_python_in_r/stats_wl.csv')\n\ndf_W <- \ndf %>%\n  pivot_wider(names_from = Date, values_from = Priority)\nhead <- colnames(df_W)\ndf_W <- data.frame(lapply(df_W, as.character), stringsAsFactors=FALSE)\nnames(df_W) <- head\nfor (r in 1:nrow(df_W)){\n  present <- FALSE\n  for (c in 3:ncol(df_W)) {\n    if (!present && !is.na(df_W[r,c])){\n      present <- TRUE\n    }\n    if (present){\n      if (is.na(df_W[r,c])){\n        status <- df_W[r, 2]\n        df_W[r,c] <- ifelse(status=='Registered', 'R', ifelse(status=='Dropped Class', 'D', 'L'))\n      }\n      if (!(df_W[r, c] %in% c('R', 'D', 'L')) && nchar(df_W[r,c])==1) {\n        df_W[r, c] <- paste('0', df_W[r, c], sep='')\n      }\n    }\n  }\n}\n\ndf_W %>% \n  to_lodes_form(axis = 2:15) %>%\n  drop_na() %>%\n  subset(x != 'Status') %>%\n  mutate(status=ifelse(stratum=='R', 'Registered', ifelse(stratum=='L', 'Left List', ifelse(stratum=='D', 'Dropped Class', 'Joined')))) %>%\n  ggplot(aes(alluvium = alluvium, x = x, stratum = stratum, y = 1, fill=status)) +\n    geom_alluvium(color = \"black\") +\n    geom_stratum() +\n    geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) +\n    labs(x='Data', y='Student')df = pd.read_csv('resources/use_python_in_r/stats_wl.csv')\nstudents = set(df['Name'])\ndates = list(set(df['Date']))\ndates.sort()\nname_priority_map = {}\nfor e in df.values:\n  name, date, priority = e[0], e[1], e[2]\n  if name not in name_priority_map:\n    name_priority_map[name] = {}\n  name_priority_map[name][date] = str(priority).zfill(2)\nname_status_map = {e[0]: e[1][0] for e in df_status.values}\nprocessed = []\nfor s, m in name_priority_map.items():\n  entry = []\n  appear = False\n  for d in dates:\n    if d in m:\n      appear = True\n    if not appear:\n      entry.append('Unseen')\n    elif d in m:\n      entry.append(m[d])\n    else:\n      entry.append(name_status_map[s])\n  processed.append([s] + entry)\ndf_processed = pd.DataFrame(processed, columns=['Name'] + dates)\nlibrary(naniar)\ndf_p <- as.data.frame(py$df_processed)\ndf_p %>% \n  replace_with_na_all(condition = ~. == 'Unseen') %>%\n  to_lodes_form(axis = 2:14) %>%\n  drop_na() %>%\n  mutate(status=ifelse(stratum=='R', 'Registered', ifelse(stratum=='L', 'Left List', ifelse(stratum=='D', 'Dropped Class', 'Joined')))) %>%\n  ggplot(aes(alluvium = alluvium, x = x, stratum = stratum, y = 1, fill=status)) +\n    geom_alluvium(color = \"black\") +\n    geom_stratum() +\n    geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) +\n    labs(x='Data', y='Student')\npath <- '/usr/local/opt/python@3.7/bin/python3'\nlibrary(reticulate)\nuse_python(path)python -c \"import os, sys; print(os.path.dirname(sys.executable))\"which pythonimport pandas as pd\ndf = pd.read_csv('stats_wl.csv')\ndf.head()import seaborn as sns\ndf_status = df[['Name', 'Status']].drop_duplicates()\nprint(df_status.shape)\nsns.set(font_scale=0.5)\nsns.countplot(y='Status', data=df_status).set_title('Countplot for Final Status of all Students')\ndata <- py$df_status\nhead(data)\nlibrary(ggalluvial)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tidyr)\ndf <- read.csv('resources/use_python_in_r/stats_wl.csv')\n\ndf_W <- \ndf %>%\n  pivot_wider(names_from = Date, values_from = Priority)\nhead <- colnames(df_W)\ndf_W <- data.frame(lapply(df_W, as.character), stringsAsFactors=FALSE)\nnames(df_W) <- head\nfor (r in 1:nrow(df_W)){\n  present <- FALSE\n  for (c in 3:ncol(df_W)) {\n    if (!present && !is.na(df_W[r,c])){\n      present <- TRUE\n    }\n    if (present){\n      if (is.na(df_W[r,c])){\n        status <- df_W[r, 2]\n        df_W[r,c] <- ifelse(status=='Registered', 'R', ifelse(status=='Dropped Class', 'D', 'L'))\n      }\n      if (!(df_W[r, c] %in% c('R', 'D', 'L')) && nchar(df_W[r,c])==1) {\n        df_W[r, c] <- paste('0', df_W[r, c], sep='')\n      }\n    }\n  }\n}\n\ndf_W %>% \n  to_lodes_form(axis = 2:15) %>%\n  drop_na() %>%\n  subset(x != 'Status') %>%\n  mutate(status=ifelse(stratum=='R', 'Registered', ifelse(stratum=='L', 'Left List', ifelse(stratum=='D', 'Dropped Class', 'Joined')))) %>%\n  ggplot(aes(alluvium = alluvium, x = x, stratum = stratum, y = 1, fill=status)) +\n    geom_alluvium(color = \"black\") +\n    geom_stratum() +\n    geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) +\n    labs(x='Data', y='Student')df = pd.read_csv('resources/use_python_in_r/stats_wl.csv')\nstudents = set(df['Name'])\ndates = list(set(df['Date']))\ndates.sort()\nname_priority_map = {}\nfor e in df.values:\n  name, date, priority = e[0], e[1], e[2]\n  if name not in name_priority_map:\n    name_priority_map[name] = {}\n  name_priority_map[name][date] = str(priority).zfill(2)\nname_status_map = {e[0]: e[1][0] for e in df_status.values}\nprocessed = []\nfor s, m in name_priority_map.items():\n  entry = []\n  appear = False\n  for d in dates:\n    if d in m:\n      appear = True\n    if not appear:\n      entry.append('Unseen')\n    elif d in m:\n      entry.append(m[d])\n    else:\n      entry.append(name_status_map[s])\n  processed.append([s] + entry)\ndf_processed = pd.DataFrame(processed, columns=['Name'] + dates)\nlibrary(naniar)\ndf_p <- as.data.frame(py$df_processed)\ndf_p %>% \n  replace_with_na_all(condition = ~. == 'Unseen') %>%\n  to_lodes_form(axis = 2:14) %>%\n  drop_na() %>%\n  mutate(status=ifelse(stratum=='R', 'Registered', ifelse(stratum=='L', 'Left List', ifelse(stratum=='D', 'Dropped Class', 'Joined')))) %>%\n  ggplot(aes(alluvium = alluvium, x = x, stratum = stratum, y = 1, fill=status)) +\n    geom_alluvium(color = \"black\") +\n    geom_stratum() +\n    geom_text(stat = \"stratum\", aes(label = paste(after_stat(stratum)))) +\n    labs(x='Data', y='Student')"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"plotting-graph-with-r-v.s.-python","chapter":"118 Plotting graph with R v.s. Python","heading":"118 Plotting graph with R v.s. Python","text":"Qi MengIn document, ’ll try summerize basic plots ’ve learned course STAT GR5702 Exploratory Data Analysis Visualization try match R codes corresponding Python codes. examples provided document relatively simple. introduce/use commonly used parameters examples. Hope document can help people get insights python packages/function use want create plots.plots introduce document include Histogram, Boxplot, Density Curve, Ridgeline Plot, QQ Plot, Scatter Plot, Heatmap, Parallel Coordinate Plots Mosaic Plot.document, referred codes used lecture problem sets. also python codes referred online cited bottom section.","code":"\nlibrary(reticulate)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(openintro)\nlibrary(GGally)\nlibrary(parcoords)\nlibrary(vcd)"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"histogram-4","chapter":"118 Plotting graph with R v.s. Python","heading":"118.1 Histogram","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r","chapter":"118 Plotting graph with R v.s. Python","heading":"118.1.1 R","text":"discussed two ways create histogram R, one uses base R method one uses ggplot2.","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"base-r-5","chapter":"118 Plotting graph with R v.s. Python","heading":"118.1.1.1 Base R","text":"input data can column.border: color histogram border.col: color filled histogram.right: TRUE/FALSE value. TRUE stands right-closed intervals, vise versa.main: plot title.xlab: label x-axis.","code":"\nx <- c(1, 5, 10, 20, 40, 50, 51, 53, 55, 56, 60, 65, 65, 68)\nhist(x, border=\"blue\", col = \"lightblue\", right = FALSE, main = 'Base R Histogram', xlab=\"data\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"ggplot2-6","chapter":"118 Plotting graph with R v.s. Python","heading":"118.1.1.2 ggplot2","text":"input data data frame. column used histogram specified aesgeom_histogram one plotting histogram.\ncolor: color histogram border.\nfill: color filled histogram.\nbinwidth: set width bin.\ncenter: set number number want one column center .\ncolor: color histogram border.fill: color filled histogram.binwidth: set width bin.center: set number number want one column center .scale_x_continuous: set scale x-axis setting min value, max value interval value.labs: set labels title plot\ntitle: plot title.\nx: label x-axis.\ny: label y-axis.\ntitle: plot title.x: label x-axis.y: label y-axis.","code":"\nx <- c(1, 5, 10, 20, 40, 50, 51, 53, 55, 56, 60, 65, 65, 68)\ndf <- data.frame(x)\nggplot(df, aes(x)) +\n  geom_histogram(color = \"blue\", fill = \"lightblue\", binwidth = 10, center = 5) +\n  scale_x_continuous(breaks = seq(0, 70, by = 5)) +\n  labs(title = \"ggplot Histogram\", x = \"data\", y = \"frequency\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-5","chapter":"118 Plotting graph with R v.s. Python","heading":"118.1.2 Python","text":"options/packages plot histogram python. ’ll use matplotlib.pyplot plot histogram example.plt.subplot: wrapper create fig axes plots fit .plt.hist: function plot histogram.\nx: data plotted.\nbins: number bins.\nedgecolor: color border.\ncolor: color filled histogram.\nx: data plotted.bins: number bins.edgecolor: color border.color: color filled histogram.plt.set_title: plot title (subplot).plt.set_xlable: label x-axis (subplot).plt.set_ylable: label y-axis (subplot).","code":"import matplotlib.pyplot as plt\nimport numpy as np\nx = [1, 5, 10, 20, 40, 50, 51, 53, 55, 56, 60, 65, 65, 68]\ndf = np.array(x)\nfig,ax = plt.subplots(1, 1, figsize=(5,5));\nax.hist(x=df, bins=7, edgecolor='blue', color='lightblue')\nax.set_title(\"Python Histogram\")\nax.set_xlabel(\"data\")\nax.set_ylabel(\"frequency\")\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"boxplot-3","chapter":"118 Plotting graph with R v.s. Python","heading":"118.2 Boxplot","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-1","chapter":"118 Plotting graph with R v.s. Python","heading":"118.2.1 R","text":"discussed two ways create boxplot R, one uses base R method one uses ggplot2.","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"base-r-6","chapter":"118 Plotting graph with R v.s. Python","heading":"118.2.1.1 Base R","text":"input data can column.main: plot title.xlab: label x-axis.ylab: label y-axis.border: color boxplot border.col: color filled boxplot","code":"\nx <- c(25, 50, 51, 53, 55, 56, 60, 65, 65, 68)\nboxplot(x, main=\"Base R Boxplot\", xlab=\"x label\", ylab=\"y label\", border=\"blue\", col=\"lightblue\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"ggplot2-7","chapter":"118 Plotting graph with R v.s. Python","heading":"118.2.1.2 ggplot2","text":"input data data frame. column used boxplot specified aesgeom_boxplot one plotting boxplot\ncolor: color boxplot border.\nfill: color filled boxplot.\ncolor: color boxplot border.fill: color filled boxplot.coord_flip: Flip cartesian coordinates.","code":"\nx <- c(25, 50, 51, 53, 55, 56, 60, 65, 65, 68)\ndf <- data.frame(x)\nggplot(df, aes(x)) +\n  geom_boxplot(color = 'blue', fill = 'lightblue') +\n  coord_flip() +\n  labs(title = \"ggplot Boxplot\", x = \"x label\", y = \"y label\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-6","chapter":"118 Plotting graph with R v.s. Python","heading":"118.2.2 Python","text":"options/packages plot boxplot python. ’ll use matplotlib.pyplot plot boxplot example.plt.boxplot: function plot boxplot.\nx: data plotted.\nvert: decide want vertical boxplot.\nx: data plotted.vert: decide want vertical boxplot.","code":"x = [25, 50, 51, 53, 55, 56, 60, 65, 65, 68]\ndf = np.array(x)\nfig,ax = plt.subplots(1, 1, figsize=(5,5));\nax.boxplot(x=df, vert=False)\nax.set_title(\"Python Boxplot\")\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"density-curve","chapter":"118 Plotting graph with R v.s. Python","heading":"118.3 Density Curve","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-2","chapter":"118 Plotting graph with R v.s. Python","heading":"118.3.1 R","text":"plot density curve using ggplot2.input data data frame. column used density curve specified aesgeom_density one plotting density curve.\ncolor: color density curve border.\nfill: color filled density curve\nadjust: multiplicate bandwidth adjustment, adjust=.5 means using half default bandwidth.\nbw: smoothing bandwidth used.\nalpha: set transparency plot.\ncolor: color density curve border.fill: color filled density curveadjust: multiplicate bandwidth adjustment, adjust=.5 means using half default bandwidth.bw: smoothing bandwidth used.alpha: set transparency plot.","code":"\nx <- c(1, 5, 10, 20, 40, 50, 51, 53, 55, 56, 60, 65, 65, 68)\ndf <- data.frame(x)\nggplot(df, aes(x)) +\n  geom_density(color='blue', fill=\"lightblue\", adjust=.5, bw=5, alpha=.5) +\n  labs(title = \"ggplot Density Plot\", x = \"x label\", y = \"y label\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-7","chapter":"118 Plotting graph with R v.s. Python","heading":"118.3.2 Python","text":"options/packages plot density curve python. ’ll use seaborn plot density curve example.plt.boxplot: function plot density curve\nx: data plotted.\nbw_adjust: multiplicate bandwidth adjustment. Increasing value make curve smoother.\nfill: decide fill area curve color.\nax: choose ax plot . Can use axes created plt\nx: data plotted.bw_adjust: multiplicate bandwidth adjustment. Increasing value make curve smoother.fill: decide fill area curve color.ax: choose ax plot . Can use axes created plt","code":"import seaborn as sns\nx = [1, 5, 10, 20, 40, 50, 51, 53, 55, 56, 60, 65, 65, 68]\ndf = np.array(x)\nfig,ax = plt.subplots(1, 1, figsize=(5,5));\nsns.kdeplot(x, bw_adjust=.5, fill=True, ax=ax)\nax.set_title(\"Python Density Curve\")\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"ridgeline-plot-1","chapter":"118 Plotting graph with R v.s. Python","heading":"118.4 Ridgeline Plot","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-3","chapter":"118 Plotting graph with R v.s. Python","heading":"118.4.1 R","text":"plot ridgeline plots using ggplot2.input data data frame. columns used ridgeline plots specified aes.\nreorder: function can used reorder columns based values, median example .\nreorder: function can used reorder columns based values, median example .geom_density_ridges one plotting ridgeline plot.\ncolor: color ridgeline plot border.\nfill: color filled ridgeline plot.\nalpha: set transparency plot.\ncolor: color ridgeline plot border.fill: color filled ridgeline plot.alpha: set transparency plot.","code":"\nggplot(loans_full_schema, aes(x = loan_amount, y = reorder(loan_purpose, loan_amount, median))) +\n  geom_density_ridges(color='blue', fill=\"lightblue\", alpha=.5) +\n  labs(title = \"ggplot Density Plot\", x = \"x label\", y = \"y label\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-8","chapter":"118 Plotting graph with R v.s. Python","heading":"118.4.2 Python","text":"’ll use joypy package plot ridgeline plot example. ’ll use preset iris dataset displayingjoypy.joyplot: function plot ridgeline plot\nx: data plotted.\n: passing column name , get density plot value grouped column.\nx: data plotted.: passing column name , get density plot value grouped column.source: https://deepnote.com/@deepnote/Joyplot-Introduction-RmbhozJJRC6alCu8xcsbHQ","code":"import joypy\nimport pandas as pd\niris = sns.load_dataset('iris')\nfig, axes = joypy.joyplot(data=iris, by = 'species')\nax = plt.gca()\nplt. show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"qq-plot-1","chapter":"118 Plotting graph with R v.s. Python","heading":"118.5 QQ Plot","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-4","chapter":"118 Plotting graph with R v.s. Python","heading":"118.5.1 R","text":"plot QQ Plot plots using qqline. ’ll create 200 exponential data displayingqqnorm: produces normal QQ plot values y passed .qqline one plotting QQ line\ny: data used plot QQ line.\ncolor: color QQ line.\ny: data used plot QQ line.color: color QQ line.","code":"\ny <- rexp(200, 5)\nqqnorm(y=y)\nqqline(y=y, col = \"red\")"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-9","chapter":"118 Plotting graph with R v.s. Python","heading":"118.5.2 Python","text":"’ll use scipy.stats package plot QQ plot example. ’ll create 200 exponential data displayingstats.probplot: function plot QQ plot\nx: data plotted.\ndist: distribution distribution function name\nplot: plots quantiles given\nx: data plotted.dist: distribution distribution function nameplot: plots quantiles given","code":"import scipy.stats as stats\ny = np.random.exponential(scale = 5, size=200)\nstats.probplot(x=y, dist=\"norm\", plot=plt)\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"scatter-plot-2","chapter":"118 Plotting graph with R v.s. Python","heading":"118.6 Scatter Plot","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-5","chapter":"118 Plotting graph with R v.s. Python","heading":"118.6.1 R","text":"plot scatter plot using ggplot2. example , used ames dataset openintro library. dependent variable area independent variable price.input data data frame. columns(x/y) used scatter plot specified aes.geom_point one plotting scatter plot.\nsize: size dot plot.\nalpha: set transparency plot.\nsize: size dot plot.alpha: set transparency plot.","code":"\ndata(ames)\nggplot(ames, aes(x = area, y = price)) + geom_point(size = 0.6, alpha = 0.2)"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-10","chapter":"118 Plotting graph with R v.s. Python","heading":"118.6.2 Python","text":"options/packages plot scatter plot python. ’ll use matplotlib.pyplot plot scatter plot example. ’ll generate 1000 random (x,y) data displayingplt.scatter: function plot scatter plot\nx: data x-axis plotted.\ny: data y-axis plotted.\nalpha: set transparency plot.\nx: data x-axis plotted.y: data y-axis plotted.alpha: set transparency plot.","code":"x = np.random.rand(1000)\ny = np.random.rand(1000)\nfig,ax = plt.subplots(1, 1, figsize=(5,5));\nax.scatter(x, y, alpha=0.5)\nax.set_title(\"Scatter Plot\")\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"heatmap-1","chapter":"118 Plotting graph with R v.s. Python","heading":"118.7 Heatmap","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-6","chapter":"118 Plotting graph with R v.s. Python","heading":"118.7.1 R","text":"plot heatmap using ggplot2. example , used ames dataset openintro library. dependent variable area independent variable price.input data data frame. columns(x/y) used heatmap specified aes.geom_bin_2d one plotting heatmap.\nbins: number bins vertical horizontal directions\nbins: number bins vertical horizontal directions","code":"\ndata(ames)\nggplot(ames, aes(x = area, y = price)) + \n  geom_bin_2d(bins = 50)"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-11","chapter":"118 Plotting graph with R v.s. Python","heading":"118.7.2 Python","text":"options/packages plot heatmap python. ’ll use seaborn plot heatmap example. example , use preset dataset flights display heatmap Python.\n* sns.heatmap: function plot heatmap\n* data: 2D dataset can coerced ndarray.\n* ax: ax heatmap plotted .source: https://seaborn.pydata.org/generated/seaborn.heatmap.html","code":"fig,ax = plt.subplots(1, 1, figsize=(5,5));\ndf = sns.load_dataset(\"flights\")\ndf = df.pivot(\"month\", \"year\", \"passengers\")\nsns.heatmap(data=df, ax=ax)\nax.set_title(\"Scatter Plot\")\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"parallel-coordinate-plots","chapter":"118 Plotting graph with R v.s. Python","heading":"118.8 Parallel Coordinate Plots","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-7","chapter":"118 Plotting graph with R v.s. Python","heading":"118.8.1 R","text":"lecture, learned two type R methods plot parallel coordinate plots. One uses ggparcoord produces static plot, one uses parcoords produce interactive plot.","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"ggparcoord","chapter":"118 Plotting graph with R v.s. Python","heading":"118.8.1.1 ggparcoord","text":"plot parallel poordinate plots using ggparcoord. example , used iris dataset. numerical columns used .data: dataset plot.column: columns dataset used plot.scale: method used scale variables. commonly used option uniminmax globalminmax.title: title graph.alphaLines: transparency line.splineFactor: indicating whether spline interpolation used. number multiplied number columns.","code":"\nggparcoord(data=iris, column = 1:4, scale = 'globalminmax', title =\"ggparcoord Parallel Coordinate Plots\", alphaLines = 0.2, splineFactor = 10)"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"parcoords","chapter":"118 Plotting graph with R v.s. Python","heading":"118.8.1.2 parcoords","text":"plot parallel poordinate plots using parcoords. example , used iris dataset. numerical columns used .data: dataset plot.`rowname``: columns dataset used plot.color: include list{colorScale=name d3-scale, colorBy=column used determine color, colorScheme=color scheme used}.alpha: thickness line.brushMode: desired brush behavior.withD3: whether include d3.js","code":"\nparcoords(data=iris, rowname=F, color=list(colorScale=\"scaleOrdinal\", colorBy=\"Species\", colorScheme=\"schemeCategory10\"), alpha=0.5, brushMode='1d', withD3=TRUE)"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-12","chapter":"118 Plotting graph with R v.s. Python","heading":"118.8.2 Python","text":"options/packages plot parallel poordinate plots python. ’ll use pandas plot parallel poordinate plots example. example , use preset dataset iris display parallel poordinate plots Python.\n* pd.plotting.parallel_coordinates: function plot heatmap\n* frame: dataset plot\n* class_column: column used determine color.\n* color: colors used plot.source: https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html","code":"fig,ax = plt.subplots(1, 1, figsize=(5,5));\niris = sns.load_dataset(\"iris\")\npd.plotting.parallel_coordinates(frame=iris, class_column=\"species\", color=('#556270', '#4ECDC4', '#C7F464'))\nax.set_title(\"Parallel Poordinate Plots with Python\")\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\nplt.show()"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"mosaic-plot-1","chapter":"118 Plotting graph with R v.s. Python","heading":"118.9 Mosaic Plot","text":"","code":""},{"path":"plotting-graph-with-r-v.s.-python.html","id":"r-8","chapter":"118 Plotting graph with R v.s. Python","heading":"118.9.1 R","text":"lecture, used vcd::mosaic plot mosaic plot. show simple mosaic plot two variables . dataset used mpg.formula: indicating variables data used create contingency table.direction: indicating direction variabledata: dataset plot.highlighting_fill: color filled tiles.","code":"\ndata(mpg)\nvcd::mosaic(formula=drv~class, direction = c(\"v\", \"h\"), data=mpg, highlighting_fill = c(\"lightblue\", \"lightpink\", \"lightyellow\"))"},{"path":"plotting-graph-with-r-v.s.-python.html","id":"python-13","chapter":"118 Plotting graph with R v.s. Python","heading":"118.9.2 Python","text":"options/packages plot mosaic plot python. ’ll use statsmodels.graphics.mosaicplot plot mosaic plot example. example , use preset dataset mpg display parallel poordinate plots Python.statsmodels.graphics.mosaicplot: function plot mosaic plot\ndata: dataset plot.\nindex: indicating variables/columns plotted.\ndata: dataset plot.index: indicating variables/columns plotted.source: https://data-science-master.github.io/lectures/09_python/09_matplotlib.html","code":"from statsmodels.graphics.mosaicplot import mosaic\nfig,ax = plt.subplots(1, 1, figsize=(5,5));\nmpg = r.mpg\nmosaic(data=mpg, index=['class', 'drv'])\nax.set_title(\"Parallel Poordinate Plots with Python\")\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\nplt.show()"},{"path":"ggplot2-in-python.html","id":"ggplot2-in-python","chapter":"119 ggplot2 in python","heading":"119 ggplot2 in python","text":"Luwei ZhangThis tutorial talks use ggplot visualization Python. introduces common functions examples covered previous lectures 5702 Python time.Please find code GitHub repository:\n[https://github.com/Helen-Luwei-Zhang/EDAV_CC]","code":""},{"path":"pygal-tutorial.html","id":"pygal-tutorial","chapter":"120 pygal tutorial","heading":"120 pygal tutorial","text":"Huanyu Jiang (UNI: hj2593)lots tools us visualize data. tutorial, showed use Python API Pygal. Pygal designed draw various kinds charts display data. drew Line charts, bar graphs, pie graphs examples show audience use pygal. think tutorial helpful formatted straightforward way: showed specific examples along code, also added audio explain code. process, got self familiar pygal got know explain something clear concise way. Next time, show audience applications pygal customized styles different type data.work: https://www.youtube.com/watch?v=AsqGm_acqzIReference: http://www.pygal.org/en/stable/documentation/ index.html","code":""},{"path":"integrate-r-with-python.html","id":"integrate-r-with-python","chapter":"121 Integrate R with Python","heading":"121 Integrate R with Python","text":"Weiwei Jiang","code":""},{"path":"integrate-r-with-python.html","id":"introduction-27","chapter":"121 Integrate R with Python","heading":"121.1 Introduction","text":"Python R important tools Data Science. Sometimes may familiar problem solution one tool rather . Sometimes may already developed well-defined pipeline model data analytic procedure one Python R. ’s tedious time-consuming rewrite code another form. Thus, bring lots benefits can combine Python R. documents foucs run python code R environment. packages enable run R scripts Python environment. rpy2 one packages still keeping updating official documents good start tutorial, put link Reference.","code":""},{"path":"integrate-r-with-python.html","id":"runing-python-code-in-r-scripts","chapter":"121 Integrate R with Python","heading":"121.2 Runing Python code in R scripts","text":"searching way call Python R, exists convenient CRAN package called “reticulate”. package enables easily transform R Python data type,using library Python, source Python scripts switch different versions Python.","code":""},{"path":"integrate-r-with-python.html","id":"installation-8","chapter":"121 Integrate R with Python","heading":"121.2.1 Installation","text":"“reticualte” included CRAN, can simply install use package follows.","code":"\n#install.packages(\"reticulate\")\nlibrary(reticulate)"},{"path":"integrate-r-with-python.html","id":"python-version","chapter":"121 Integrate R with Python","heading":"121.2.2 Python version","text":"default python version version declared PATH. package compatible version Python >= 2.7. can either change default python version call functions change python version need.way change default python version change default python version Rproj simply run scripts Sys.setenv(RETICULATE_PYTHON = PATH). PATH value specific path want use.three function reticulate help change python versions.use_python() : specify path certain version Pythonuse_virtualenv(): specify direction containing Python virtualenv.use_condaenv() specify name Conda Env.Example:also exists function called py_config() show current Python version Python version discovered system.","code":"\nreticulate::use_python( \"~/anaconda3/bin/python\")\npy_config()"},{"path":"integrate-r-with-python.html","id":"data-transformation-between-r-and-python","chapter":"121 Integrate R with Python","heading":"121.2.3 Data Transformation Between R and Python","text":"One important thing need bear mind data type consistence calling another programming language current programming environment. Luckily, reticluate package automatically convert python data types equivalent R data types calling Python R. can convert Python R data calling py_to_r r_to_py. simple example.default data transformation format listed :One important thing need bear mind Arrays R Python stored different order. Arrays stored Column-major order R Row-major order Python. representation n-dimension array might look differently R forms Python forms data. Python tend show data first dimension R tend use last dimension. example.","code":"\nlibrary(reticulate)\nnp <- import(\"numpy\", convert=FALSE)\npd <- import(\"pandas\",convert = FALSE)\n(x <- np$arange(1, 5)$reshape(2L, 2L))\n\n(y <- py_to_r(x))\n\n(UCB <- data.frame(UCBAdmissions))\nUCB_py <- r_to_py(UCB)\n(pd$DataFrame(UCB_py))\n(x <- np$arange(1, 9)$reshape(2L, 2L,2L))\n\n(y <- py_to_r(x))"},{"path":"integrate-r-with-python.html","id":"running-python-in-r","chapter":"121 Integrate R with Python","heading":"121.2.4 Running python in R","text":"several ways can call Python R.Running R Markdown cells refers data using py$Source exists Python scriptsExecuting python code functions.Create interaction Python console R Calling repl_python() function.","code":""},{"path":"integrate-r-with-python.html","id":"running-in-r-markdown-cells","chapter":"121 Integrate R with Python","heading":"121.2.4.1 Running in R Markdown cells","text":"reticulate packages includes Python engine R Markdown. Thus, can simply write python code R Markdown.can also easily reach variable r using r. running python environment. example. one thing extreme careful r interface object python. assign value name r lose interface object python environment. pay attention variable name Python environment.","code":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\nfig, (ax1) = plt.subplots(1, 1)\n# make a little extra space between the subplots\n\ndt = 0.01\nt = np.arange(0, 30, dt)\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nnse1 = np.random.randn(len(t))                 # white noise 1\nnse2 = np.random.randn(len(t))                 # white noise 2\nrp = np.exp(-t / 0.05)\n\ncnse1 = np.convolve(nse1, rp, mode='same') * dt   # colored noise 1\ncnse2 = np.convolve(nse2, rp, mode='same') * dt   # colored noise 2\n\n# two signals with a coherent part and a random part\ns1 = 0.01 * np.sin(2 * np.pi * 10 * t) + cnse1\ns2 = 0.01 * np.sin(2 * np.pi * 10 * t) + cnse2\n\ndf = pd.DataFrame({ \"t\":t,\n                    \"s1\":s1,\n                    \"s2\":s2})\nax1.plot(t, s1, t, s2)\nax1.set_xlim(0, 5)\nax1.set_xlabel('time')\nax1.set_ylabel('s1 and s2')\nax1.grid(True)\nplt.show()\nlibrary(reticulate)\nlibrary(ggplot2)\nggplot(py$df,aes(x=t)) + geom_line(aes(y=s1),color=\"dark blue\") +\n                         geom_line(aes(y=s2),color = \"dark orange\") + \n                         xlim(0,5) +\n                         labs(title =\"Drawing with data From Python Enviroment\",\n                              y = \"s1 and s2\")\ndata(ames,package = \"openintro\" )\namesprint(r.ames.head())"},{"path":"integrate-r-with-python.html","id":"source-exists-python-scripts","chapter":"121 Integrate R with Python","heading":"121.2.4.2 Source exists Python scripts","text":"can simply source exist Python script using Python code R cells. example.\ncontent plot ScriptThen use source_python() function source script can use defined Python function R cells.","code":"import numpy as np\nimport matplotlib.cbook as cbook\nimport matplotlib.image as image\nimport matplotlib.pyplot as plt\ndef plot_with_image(x,y):\n    with cbook.get_sample_data('logo2.png') as file:\n        im = image.imread(file)\n\n    fig, ax = plt.subplots()\n\n    ax.plot(x,y, '-o', ms=20, alpha=0.7, mfc='orange')\n    ax.grid()\n    fig.figimage(im, 10, 10, zorder=3, alpha=.5)\n\n    plt.show()\nsource_python(\"resources/r_with_python/plot.py\")\nplot_with_image(ames[1:10,]$area, ames[1:10,]$price)"},{"path":"integrate-r-with-python.html","id":"executing-python-code-with-functions.","chapter":"121 Integrate R with Python","heading":"121.2.5 Executing python code with functions.","text":"two functions called py_run_file py_run_string(). two functions can help execute python R scripts. exampleOr can simply import Python package R use format R. simple example.","code":"\npy_run_string(\"print('Hello, world! This is a message from Python')\")\n#  Content of Hello.py : print('This is a message from Python')\npy_run_file(\"resources/r_with_python/Hello.py\")\nnp <- import(\"numpy\", convert = TRUE)\nnp1 <- np$array(c(1:10))\nnp1"},{"path":"integrate-r-with-python.html","id":"executing-python-code-with-functions.-1","chapter":"121 Integrate R with Python","heading":"121.2.6 Executing python code with functions.","text":"Create interaction Python console R Calling repl_python() function. wish write python code console, can call repl_python() function. console RStudio change Python engine. can reach python variable simply adding prefix py$.","code":""},{"path":"integrate-r-with-python.html","id":"reference-7","chapter":"121 Integrate R with Python","heading":"121.3 Reference","text":"https://rstudio.github.io/reticulate/https://anderfernandez.com/en/blog/--use-python--r/https://rpy2.github.io/doc/v3.4.x/html/introduction.html#getting-started","code":""},{"path":"python-visualization-tutorial.html","id":"python-visualization-tutorial","chapter":"122 Python Visualization Tutorial","heading":"122 Python Visualization Tutorial","text":"Jingxiang ZhangThis python visualization tutorial using python language python packages\nincluding Matplotlib Seaborn. tutorial, cover contents like continuous\nvariables visualization categorical variables visualization different kinds\ngraphs like histogram, boxplot, bar chart, etc.Please check github repo see tutorial:https://github.com/zhangjx831/Python-Visualization/blob/main/Python-Visualization.ipynb","code":""},{"path":"python-altair-visualization-method-tutorial.html","id":"python-altair-visualization-method-tutorial","chapter":"123 Python Altair Visualization Method Tutorial","heading":"123 Python Altair Visualization Method Tutorial","text":"Anne Lin (Anqi Lin)Altair declarative statistical visualization library Python. offers powerful concise visualization grammar enables users build wide range statistical visualizations quickly simply.Therefore, created tutorial template common use Altair data visualization. link tutorial follows:https://nbviewer.org/github/anqilin11/Python-Altair-Tutorial-Template/blob/b50964211c1b2e6a4657dcb8c9672d96512eb1de/Community%20Contribution.ipynb(case abve link doesn’t work, entire GitHub page follows: https://github.com/anqilin11/Python-Altair-Tutorial-Template)","code":""},{"path":"r-to-python-easy-plot.html","id":"r-to-python-easy-plot","chapter":"124 R to python easy plot","heading":"124 R to python easy plot","text":"Sandy Chen Yixuan LiuOur notebook discusses way generate graphs class python. process, learned lot Python commands allow us produce similar graphs learned class. instance, explored functions like facet_wrap Python. also explored plot histograms, dot plot, box plot, density Plot, qq plot ridgeplot Python. link notebook rendered pdf: https://github.com/yixual5/R--Python-Plot-Tutorial","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"r-dplyr-vs-python-pandas","chapter":"125 R Dplyr vs Python Pandas","heading":"125 R Dplyr vs Python Pandas","text":"Yujia XieDplyr R Pandas Python two popular libraries working tabular/structured data many data scientists business analysts. People always arguing framework better. think perform well crucial choose best practices needs. choice among two likely depend skills available organization, infrastructure code base available, advanced models required used.\ndocument:Explores new tools can add repertoire data scientist.Explores new tools can add repertoire data scientist.Helps transition one language/framework .Helps transition one language/framework .Creates reference sheet go Dplyr Pandas back case forget syntax.Creates reference sheet go Dplyr Pandas back case forget syntax. ","code":"\n# install.packages('reticulate')\nlibrary(reticulate)\nlibrary(tidyverse)"},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-for-r-quick-overview","chapter":"125 R Dplyr vs Python Pandas","heading":"125.0.1 Dplyr for R Quick Overview","text":"one core packages tidyverse R programming language, dplyr primarily set functions designed enable dataframe manipulation intuitive, user-friendly way. dplyr actually includes several dozen functions enable various forms data manipulation, package features five primary verbs:filter(), extract rows dataframe, based conditions specified user;filter(), extract rows dataframe, based conditions specified user;select(), subset dataframe columns;select(), subset dataframe columns;arrange(), sort rows dataframe based attributes held particular columns;arrange(), sort rows dataframe based attributes held particular columns;mutate(), create new variables, altering /combining values existing columns;mutate(), create new variables, altering /combining values existing columns;summarize()/summarise(), collapse values dataframe single summary.summarize()/summarise(), collapse values dataframe single summary.can used conjunction group_by() changes scope function operating entire dataset operating group--group. six functions provide verbs language data manipulation. Together properties make easy chain together multiple simple steps achieve complex result.\nSources: https://r4ds..co.nz/transform.html#missing-values-1https://en.wikipedia.org/wiki/Dplyr ","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-for-python-quick-overview","chapter":"125 R Dplyr vs Python Pandas","heading":"125.0.2 Pandas for Python Quick Overview","text":"Pandas fast, powerful, flexible easy use open source data analysis manipulation tool, built top Python programming language. offers data structures operations manipulating numerical tables time series. Pandas allows various data manipulation operations merging, reshaping, selecting well data cleaning, data wrangling features.\nPandas open source, BSD-licensed library providing:high-performance, easy--use data structures data analysis toolsPrimary datastructures:\n-Series: 1D array flexible index\n-Dataframe: 2D matrix flexible index column namesSources: https://en.wikipedia.org/wiki/Pandas_(software)https://pandas.pydata.org/docs/reference/frame.html","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"write-python-in-r-studio","chapter":"125 R Dplyr vs Python Pandas","heading":"125.0.3 Write Python in R Studio","text":"begin:\nPython version choice installed.\nUpdate R Studio. able pass data structures R Python ’s best 1.2.1206 later installed.\nInstall/update R reticulate package load library.\nSecondly, point correct version Python.\nmay multiple versions Python installed. ’re using Mac, version Python 2 installed default OS uses. Typing -python python3 terminal (Mac OS) give list paths versions Python machine.R chunk, tell R use Python 3 interpreter, import sys Python module find version Python R using.Now ready write Python code creating Python chunk.Source: https://rpubs.com/onduuuu/python_in_r","code":"\nuse_python(\"/usr/local/bin/python3\", required = T)\nsys <- import(\"sys\")\nsys$version# Python in a Python chunk\nimport pandas as pd\nprint(\"We are calling Python from R in a Python Chunk!\")"},{"path":"r-dplyr-vs-python-pandas.html","id":"dataset-1","chapter":"125 R Dplyr vs Python Pandas","heading":"125.1 Dataset","text":"use 1000 random sample Records Yellowcab Taxi trips January 2017.\ninfo: https://www1.nyc.gov/site/tlc//tlc-trip-record-data.page\nFeatures:pickup_datetime: datetime driver picked passengerdropoff_time: datetime driver dropped passengertrip_distance: total distance traveled yellowcab carrying passengerfare_amount: fee needs pay passengertip_amount: tip amount given passenger driverpayment_type: passenger pay; either credit card cashday_of_week: day week trip occuredis_weekend: indicates whether trip happened weekend","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"the-tutorial-and-reference-sheet","chapter":"125 R Dplyr vs Python Pandas","heading":"125.2 The Tutorial and Reference Sheet","text":"","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"load-data","chapter":"125 R Dplyr vs Python Pandas","heading":"125.3 Load Data","text":"","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-1","chapter":"125 R Dplyr vs Python Pandas","heading":"125.3.1 Dplyr","text":"Use readr::read_csv()","code":"\nr_df <- readr::read_csv(\"resources/yujia_resources/yellowcab.csv\", \n                 skip = 1) #second line is the column names"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas","chapter":"125 R Dplyr vs Python Pandas","heading":"125.3.2 Pandas","text":"Use pd.read_csv()","code":"pd_df = pd.read_csv('resources/yujia_resources/yellowcab.csv',\n                 sep=',',\n                 header=1,\n                 parse_dates=['pickup_datetime','dropoff_datetime'])"},{"path":"r-dplyr-vs-python-pandas.html","id":"get-summary-info-for-dataframe","chapter":"125 R Dplyr vs Python Pandas","heading":"125.4 Get Summary Info for DataFrame","text":"","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-2","chapter":"125 R Dplyr vs Python Pandas","heading":"125.4.1 Dplyr","text":"","code":"\nr_df %>% str()"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-1","chapter":"125 R Dplyr vs Python Pandas","heading":"125.4.2 Pandas","text":"","code":"pd_df.info()"},{"path":"r-dplyr-vs-python-pandas.html","id":"selecting-columns","chapter":"125 R Dplyr vs Python Pandas","heading":"125.5 Selecting columns","text":"common datasets hundreds even thousands variables. like narrow variables really interested .","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-3","chapter":"125 R Dplyr vs Python Pandas","heading":"125.5.1 Dplyr","text":"select() can help us zoom useful subset using operations based names variables.\nSelect columns column nameSelect columns fare_amount day_of_week (inclusive)Select columns except fare_amount day_of_week (inclusive)number helper functions can use within select():starts_with(\"abc\"): matches names begin “abc”.starts_with(\"abc\"): matches names begin “abc”.ends_with(\"xyz\"): matches names end “xyz”.ends_with(\"xyz\"): matches names end “xyz”.contains(\"ijk\"): matches names contain “ijk”.contains(\"ijk\"): matches names contain “ijk”.matches(\"(.)\\\\1\"): selects variables match regular expression. one matches variables contain repeated characters. ’ll learn regular expressions strings.matches(\"(.)\\\\1\"): selects variables match regular expression. one matches variables contain repeated characters. ’ll learn regular expressions strings.num_range(\"x\", 1:3): matches x1, x2 x3.num_range(\"x\", 1:3): matches x1, x2 x3.Select columns end ‘datetime’select() can used conjunction everything() helper. useful handful variables ’d like move start data frame.Source: https://r4ds..co.nz/transform.html#missing-values-1","code":"\nr_df %>% select(pickup_datetime, fare_amount, is_weekend)\nr_df %>% select(fare_amount:day_of_week)\nr_df %>% select(-(fare_amount:day_of_week))\nr_df %>% select(ends_with(\"datetime\"))\nr_df %>% select(is_weekend, trip_distance, everything())"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-2","chapter":"125 R Dplyr vs Python Pandas","heading":"125.5.2 Pandas","text":"Pass columns listUse Filter FunctionSelect columns fare_amount day_of_week (inclusive).loc label-based, means specify name rows columns need filter .iloc integer index-based, specify rows columns integer index.loc inclusive sides iloc left closed right open., represents row; , represents columnsSelect columns except fare_amount day_of_week (inclusive)select columns end datetime python.Use either regular expression filter() function; info regular expression: https://docs.python.org/3/library/re.htmlOr use .loc function str.endswith()","code":"pd_df[['trip_distance', 'is_weekend']]\npd_df.loc[:, ['trip_distance', 'is_weekend']]pd_df.filter(items=['trip_distance', 'is_weekend'])pd_df.loc[:, 'fare_amount':'day_of_week']\npd_df.iloc[:, 3:7]pd_df.loc[:, ~pd_df.columns.isin(pd_df.columns[3:7])]pd_df.filter(regex='datetime$',axis=1)\npd_df.loc[:, pd_df.columns.str.endswith(\"datetime\")]"},{"path":"r-dplyr-vs-python-pandas.html","id":"selecting-rows","chapter":"125 R Dplyr vs Python Pandas","heading":"125.6 Selecting Rows","text":"Subset observations based values.\nLogical Operator:& “”| “”! “”.","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-4","chapter":"125 R Dplyr vs Python Pandas","heading":"125.6.1 Dplyr","text":"Mainly use filter() subsetting rows.want observe trips happened weekends.want observe trips happened weekends trip_distance greater 2 miles.want observe trips happened weekdays trip_distance less equal 2 miles.useful short-hand x %% y. select every row x one values ySource: https://r4ds..co.nz/transform.html#missing-values-1","code":"\nr_df %>% filter(is_weekend==TRUE)\nr_df %>% filter(is_weekend==TRUE & trip_distance > 2)\nr_df %>% filter(!(is_weekend==TRUE | trip_distance > 2))\nr_df %>% filter(is_weekend==FALSE, trip_distance <= 2)\nr_df %>% filter(as.Date(pickup_datetime) %in% c(as.Date('2017-01-15'), as.Date('2017-01-16')))\n# same as\nr_df %>% filter(as.Date(pickup_datetime)==as.Date('2017-01-15') | as.Date(pickup_datetime)==as.Date('2017-01-16'))"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-3","chapter":"125 R Dplyr vs Python Pandas","heading":"125.6.2 Pandas","text":"Pandas can either use indexing approachOr try handy query APIShow trips pickup_datetime Jan 15th Jan 16th.isin():“” use: something.isin(somewhere).“”: ~something.isin(somewhere).Show trips happened 3rd, 5th, 6th week.","code":"pd_df[(pd_df.is_weekend==True) & (pd_df.trip_distance > 2)]pd_df.query(\"is_weekend==True & trip_distance > 2\")pd_df.loc[(pd_df['pickup_datetime'] >= '2017-01-15') & (pd_df['pickup_datetime'] < '2017-01-17')]pd_df.loc[~pd_df.day_of_week.isin([3,5,6])]"},{"path":"r-dplyr-vs-python-pandas.html","id":"delete-add-columns","chapter":"125 R Dplyr vs Python Pandas","heading":"125.7 Delete / Add column(s)","text":"Sometimes like vertically subset dataframe add new columns dataframe","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-5","chapter":"125 R Dplyr vs Python Pandas","heading":"125.7.1 Dplyr","text":"use function mutate() add column(s).\nLets say want add column named total_payment sum fare_amount tip_amount new column total_payment_per_mile total_payment divided trip_distance.use function select() drop column(s).\nDrop newly added two columnsis_weekend pickup_datetime","code":"\nr_df <- r_df %>% mutate(total_payment = fare_amount + tip_amount,\n                        total_payment_per_mile = total_payment / trip_distance)\nr_df <- r_df %>% select(-c(total_payment, total_payment_per_mile))"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-4","chapter":"125 R Dplyr vs Python Pandas","heading":"125.7.2 Pandas","text":"Add column(s)Drop column(s) Pandas use drop(). axis=1 used indicate column-wise operations. Set inplace=True overwrite current dataframe.","code":"pd_df['total_payment'] = pd_df.fare_amount + pd_df.tip_amount\npd_df['total_payment_per_mile'] = pd_df.total_payment / pd_df.trip_distancepd_df.drop(['total_payment', 'total_payment_per_mile'], axis=1, inplace=True)"},{"path":"r-dplyr-vs-python-pandas.html","id":"rename-columns","chapter":"125 R Dplyr vs Python Pandas","heading":"125.8 Rename Columns","text":"want rename two features; one trip_distance trip_dist fare_amount fare_amt. Pandas supply dictionary says {'trip_distance': 'trip_dist', 'fare_amount': 'fare_amt'} Dplyr exact opposite way trip_dist=trip_distance fare_amt = fare_amount.","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-6","chapter":"125 R Dplyr vs Python Pandas","heading":"125.8.1 Dplyr","text":"use function rename() dplyr","code":"\nr_df <- r_df %>% rename(trip_dist = trip_distance, fare_amt = fare_amount)"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-5","chapter":"125 R Dplyr vs Python Pandas","heading":"125.8.2 Pandas","text":"use function rename() pandas","code":"pd_df.rename(columns = {'trip_distance': 'trip_dist', 'fare_amount': 'fare_amt'}, inplace = True)"},{"path":"r-dplyr-vs-python-pandas.html","id":"change-order-of-columns","chapter":"125 R Dplyr vs Python Pandas","heading":"125.9 Change order of columns","text":"","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-7","chapter":"125 R Dplyr vs Python Pandas","heading":"125.9.1 Dplyr","text":"use function relocate(), ..place column another specified column.\nMove column day_of_week dropoff_datetime","code":"\nr_df <- r_df %>% relocate(day_of_week, .after = dropoff_datetime)"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-6","chapter":"125 R Dplyr vs Python Pandas","heading":"125.9.2 Pandas","text":"Use function reindex() change order columns pandas","code":"pd_df.reindex(['pickup_datetime','dropoff_datetime','day_of_week','trip_distance','fare_amount','tip_amount','payment_type', 'is_weekend'], axis=1)"},{"path":"r-dplyr-vs-python-pandas.html","id":"change-cell-based-on-conditions","chapter":"125 R Dplyr vs Python Pandas","heading":"125.10 Change cell based on conditions","text":"want change fare_amount based trip_distance. Let’s say trip_distance < 2, fare_amount*1.2; 2 <= trip_distance < 5, fare_amount*1.5; 5 <= trip_distance < 10, fare_amount*1.7; else fare_amount*2","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-8","chapter":"125 R Dplyr vs Python Pandas","heading":"125.10.1 Dplyr","text":"use function mutate() along function case_when","code":"\nr_df <- r_df %>% mutate(fare_amount = case_when(trip_distance < 2 ~ fare_amount*1.2,\n                                                trip_distance >= 2 & trip_distance < 5 ~ fare_amount*1.5,\n                                                trip_distance >= 5 & trip_distance < 10 ~ fare_amount*1.7,\n                                                TRUE ~ fare_amount*2))"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-7","chapter":"125 R Dplyr vs Python Pandas","heading":"125.10.2 Pandas","text":"way python requires work.\nmask represents one condition, change cell values based conditions.","code":"mask1 = pd_df.trip_distance < 2\nmask2 = (pd_df.trip_distance >= 2) & (pd_df.trip_distance < 5)\nmask3 = (pd_df.trip_distance >= 5) & (pd_df.trip_distance < 10)\nmask4 = pd_df.trip_distance >= 10\npd_df.fare_amount[mask1] = (pd_df.fare_amount[mask1])*1.2\npd_df.fare_amount[mask2] = (pd_df.fare_amount[mask2])*1.5\npd_df.fare_amount[mask3] = (pd_df.fare_amount[mask3])*1.7\npd_df.fare_amount[mask4] = (pd_df.fare_amount[mask4])*2"},{"path":"r-dplyr-vs-python-pandas.html","id":"distinct-values-per-column","chapter":"125 R Dplyr vs Python Pandas","heading":"125.11 Distinct values per column","text":"Find distinct/unique values column.","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-9","chapter":"125 R Dplyr vs Python Pandas","heading":"125.11.1 Dplyr","text":"use function distinct()","code":"\nr_df %>% select(day_of_week) %>% distinct()"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-8","chapter":"125 R Dplyr vs Python Pandas","heading":"125.11.2 Pandas","text":"use unique() method","code":"pd_df.day_of_week.unique()"},{"path":"r-dplyr-vs-python-pandas.html","id":"sort-by-values","chapter":"125 R Dplyr vs Python Pandas","heading":"125.12 Sort by values","text":"Sort dataframe certain feature. default Dplyr Pandas ascending order.","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-10","chapter":"125 R Dplyr vs Python Pandas","heading":"125.12.1 Dplyr","text":"use function arrange().\nSort dataframe tip_amount descending order.","code":"\nr_df %>% arrange(desc(tip_amount))"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-9","chapter":"125 R Dplyr vs Python Pandas","heading":"125.12.2 Pandas","text":"use sort_values() method.","code":"pd_df.sort_values('tip_amount', ascending=False)"},{"path":"r-dplyr-vs-python-pandas.html","id":"count-number-of-records-per-group","chapter":"125 R Dplyr vs Python Pandas","heading":"125.13 Count number of records per group","text":"Finding count number entries certain groups","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-11","chapter":"125 R Dplyr vs Python Pandas","heading":"125.13.1 Dplyr","text":"first use group_by() count entries using count() tally()","code":"\nr_df %>% group_by(day_of_week) %>% count()\nr_df %>% group_by(day_of_week) %>% tally()\nr_df %>% group_by(day_of_week) %>% summarise(count = n())"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-10","chapter":"125 R Dplyr vs Python Pandas","heading":"125.13.2 Pandas","text":"use value_counts() method","code":"pd_df.value_counts('day_of_week')\npd_df.day_of_week.value_counts()"},{"path":"r-dplyr-vs-python-pandas.html","id":"summarize-aggregate","chapter":"125 R Dplyr vs Python Pandas","heading":"125.14 Summarize / Aggregate","text":"groupby operation involves combination splitting object, applying function, combining results. can used group large amounts data compute operations groups.","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-12","chapter":"125 R Dplyr vs Python Pandas","heading":"125.14.1 Dplyr","text":"Aggregate entire columns want create descriptive statistics one multiple columns. use summarise()Summarize / Aggregate group want aggregate statistics certain groups. use function group_by() along summarise() summarise_all().Aggregate group columns.\nFind mean max values every features day week. Remove NA calculation since otherwise may get NA result meaningful.Aggregate group specific column.\nFind mean max values trip_distance day week.\ngood idea always setting na.rm=TRUE know whether NA values column want find mean/max/min/… case really need na.rm=TRUE NA value feature trip_distance.","code":"\nr_df %>% summarise(across(everything(), mean))\nr_df %>% summarise(across(everything(), min))\nr_df %>% group_by(day_of_week) %>% summarise_all(list(mean, max), na.rm=TRUE)\nr_df %>% group_by(day_of_week) %>% summarise(mean=mean(trip_distance), max=max(trip_distance))"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-11","chapter":"125 R Dplyr vs Python Pandas","heading":"125.14.2 Pandas","text":"Get mean min columnAggregate group columns. use groupby() along agg()Aggregate group specific column. use groupby() along agg()","code":"pd_df.agg(['mean', 'min'])pd_df.groupby(['day_of_week']).agg(['mean', 'max'])pd_df.groupby(['day_of_week']).agg({'trip_distance':['mean', 'max']})"},{"path":"r-dplyr-vs-python-pandas.html","id":"slicing","chapter":"125 R Dplyr vs Python Pandas","heading":"125.15 Slicing","text":"Indexing Python starts 0 R 1.","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-13","chapter":"125 R Dplyr vs Python Pandas","heading":"125.15.1 Dplyr","text":"Slicing row know exact row number want extract.\nSelect 20th row 25th row (inclusive)Find first 3 records using function slice_head()Find last 1% total records using function slice_tail()Slicing top bottom records valueFind 5 records highest tip_amount using function slice_max()Find 1% total records lowest tip_amount using function slice_min()Slicing top bottom records value groupFind 2 records highest tip_amount per day_of_week using function slice_max() along group_by().with_ties specify records equal values included .Find 1% total records lowest tip_amount per day_of_week using function slice_min() along group_by()Sampling-Slicing random records (per group)\nSampling can done entire dataset equally distributed based group.Use function slice_sample() specifying n fixed amount prop proportion records. default, replace = FALSEReturn 20 random samples without replacementReturn 10% total records replacementReturn 10% total records split group replacement","code":"\nr_df %>% slice(c(20:25))\nr_df %>% slice_head(n = 3)\nr_df %>% slice_tail(prop = 0.01)\nr_df %>% slice_max(tip_amount, n = 5)\nr_df %>% slice_min(tip_amount, prop = 0.01)\nr_df %>% group_by(day_of_week) %>% slice_max(tip_amount, n = 2)\nr_df %>% group_by(day_of_week) %>% slice_min(tip_amount, prop = 0.01, with_ties = FALSE)\nr_df %>% slice_sample(n = 20)\nr_df %>% slice_sample(prop = 0.1, replace=TRUE)\nr_df %>% group_by(is_weekend) %>% slice_sample(prop = 0.1, replace=TRUE)"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-12","chapter":"125 R Dplyr vs Python Pandas","heading":"125.15.2 Pandas","text":"Slicing row using .iloc\nSelect 20th row 25th row (inclusive). Note Python index start 0.Use head() tail() get fixed amount records top bottom dataframe. Unlike dplyr, computations want extract proportionSlicing top bottom records value using nlargest() nsmallest().Unlike dplyr, first sort column computations want extract proportionSlicing top bottom records value group using groupby(), nlargest() nsmallest(), lambda function.\nFind 2 records highest tip_amount per day_of_weekFind 1% total records lowest tip_amount per day_of_weekSampling-Slicing random records (per group)","code":"pd_df.iloc[range(19, 25)]pd_df.head(n = 5)\npd_df.tail(n = len(pd_df)*0.01)pd_df.nlargest(5, 'tip_amount')\npd_df.sort_values('tip_amount', ascending=False).head(int(len(pd_df)*0.01))pd_df.groupby('day_of_week',group_keys=False).apply(lambda x: x.nlargest(2, 'tip_amount'))pd_df.groupby('day_of_week',group_keys=False).apply(lambda x: x.nsmallest(int(len(x) * 0.01), 'tip_amount'))pd_df.sample(n=20)\npd_df.sample(frac=0.1, replace=True)\npd_df.groupby('is_weekend').sample(frac=0.1, replace=True)"},{"path":"r-dplyr-vs-python-pandas.html","id":"join-two-tables-1","chapter":"125 R Dplyr vs Python Pandas","heading":"125.16 Join two tables","text":"Suppose now 2 dataframes B, common column called key","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-14","chapter":"125 R Dplyr vs Python Pandas","heading":"125.16.1 Dplyr","text":"inner_join(), left_join(), right_join(), full_join()","code":"\nA %>% inner_join(B, by=\"key\")\nA %>% left_join(B, by=\"key\")\nA %>% right_join(B, by=\"key\")\nA %>% full_join(B, by=\"key\")"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-13","chapter":"125 R Dplyr vs Python Pandas","heading":"125.16.2 Pandas","text":"merge function can perform join operations specifying (outer, inner, left, right) key","code":"pd.merge(A, B, how=\"inner\", on=\"key\")\npd.merge(A, B, how=\"left\", on=\"key\")\npd.merge(A, B, how=\"right\", on=\"key\")\npd.merge(A, B, how=\"outer\", on=\"key\")"},{"path":"r-dplyr-vs-python-pandas.html","id":"bind-rows-and-columns","chapter":"125 R Dplyr vs Python Pandas","heading":"125.17 Bind Rows and Columns","text":"Suppose now 2 dataframes B","code":""},{"path":"r-dplyr-vs-python-pandas.html","id":"dplyr-15","chapter":"125 R Dplyr vs Python Pandas","heading":"125.17.1 Dplyr","text":"bind_rows() bind_columns().\nAutomatically filled NA values don’t appear one dataframes.","code":"\nA %>% bind_rows(B)\nA %>% bind_columns(B)"},{"path":"r-dplyr-vs-python-pandas.html","id":"pandas-14","chapter":"125 R Dplyr vs Python Pandas","heading":"125.17.2 Pandas","text":"concat() method can concatenates dataframes rows (default axis = 0) columns (axis = 1).\nAutomatically filled NA values don’t appear one dataframes.","code":"pd.concat([A,B], axis=1)"},{"path":"an-introduction-to-pyecharts-package-in-python.html","id":"an-introduction-to-pyecharts-package-in-python","chapter":"126 An introduction to pyecharts package in Python","heading":"126 An introduction to pyecharts package in Python","text":"Zihao (Wayne) ZhangHere contribution related pyecharts, interactive data visualization package Python.https://nbviewer.org/github/WALLERR/EDAV_cc2021/blob/main/cc_pyecharts.ipynb","code":""},{"path":"visualization-in-r-v.s.-python.html","id":"visualization-in-r-v.s.-python","chapter":"127 Visualization in R v.s. Python","heading":"127 Visualization in R v.s. Python","text":"Zining ChenFor data scientists, R studio Python might two tools familiar . However, people previsouly really procifient Python (like ), can little tough unfamiliar grammar functioning R. Since R Python holding completely different packages plotting, introduce collection comman usage different code syntax used data visualization R Python. also works cheatsheet come Python get started R faster.","code":""},{"path":"visualization-in-r-v.s.-python.html","id":"basic-setup","chapter":"127 Visualization in R v.s. Python","heading":"127.1 Basic setup","text":"dataset retrived Kaggle. https://www.kaggle.com/anandhuh/covid--african-countries-latest-dataGenerally, R studio, packages used visualzation using ggplot. packages R using library(). example:python, plotting can done using matplotlib. importing packages like:","code":"\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(vcd)\ndf_r <- read_csv(\"resources/r_vs_python/covid_africa.csv\")"},{"path":"visualization-in-r-v.s.-python.html","id":"histogram-5","chapter":"127 Visualization in R v.s. Python","heading":"127.2 Histogram","text":"R:Python:","code":"\n#original histogram\nhist(df_r$`Total Deaths`, xlab = \"cases\", main = \"Total deaths histogram\")\n#basic histogram\nggplot(df_r, aes(x = `Total Deaths`)) + \n  geom_histogram(color = \"white\", fill = \"lightblue\") + \n  ggtitle(\"Total deaths histogram\") + labs(x = \"deaths\")"},{"path":"visualization-in-r-v.s.-python.html","id":"barplot-1","chapter":"127 Visualization in R v.s. Python","heading":"127.3 Barplot","text":"R:Python:","code":"\n#basic barplot\nbarplot(`Total Cases` ~ Country, data = df_r, las=2, main = \"Total cases barplot\")\n#ggplot barplot\nggplot(df_r, aes(x = Country, y = `Total Cases`)) +\n    geom_bar(stat='identity', fill = \"cornflowerblue\") +\n    ggtitle(\"Total cases barchart\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"},{"path":"visualization-in-r-v.s.-python.html","id":"boxplot-4","chapter":"127 Visualization in R v.s. Python","heading":"127.4 Boxplot","text":"R:Python:","code":"\n#basic boxplot\nboxplot(df_r$Population, main = \"Population boxplot\", xlab = \"population\")\n#ggplot boxplot\nggplot(df_r, aes(x = Population)) +\n    geom_boxplot() +\n    ggtitle(\"Population boxplot\") "},{"path":"visualization-in-r-v.s.-python.html","id":"scatterplot-2","chapter":"127 Visualization in R v.s. Python","heading":"127.5 Scatterplot","text":"R:Python:","code":"\n# with a regression line\nggplot(na.omit(df_r), aes(x = `Total Tests`, y =`Population`)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, color=\"blue\")"},{"path":"visualization-in-r-v.s.-python.html","id":"parallel-coordinates","chapter":"127 Visualization in R v.s. Python","heading":"127.6 Parallel Coordinates","text":"R:Python:","code":"\n#choose the first 10 countries for better\nGGally::ggparcoord(df_r[1:10,], columns =  c(2:5), scale = \"globalminmax\", groupColumn = \"Country\") + \n  xlab(\"country\") + ylab(\"count\")"},{"path":"visualization-in-r-v.s.-python.html","id":"heatmap-2","chapter":"127 Visualization in R v.s. Python","heading":"127.7 Heatmap","text":"use another dataset example.\ndataset retrived Kaggle. https://www.kaggle.com/sonukumari47/students-performance--examsR:Python:R:Python:","code":"\ndf_r2 <- read_csv(\"resources/r_vs_python/student_performance.csv\")\ndf_r2 <- df_r2[,-1]\nggplot(df_r2, aes(x = `parental level of education`, y = `race/ethnicity`,fill = `math percentage`)) + \n  geom_tile() +\n  scale_fill_viridis_c(direction = -1) + ggtitle(\"Square heatmap\") +\n  theme(axis.text.x = element_text(angle = 10))"},{"path":"visualization-in-r-v.s.-python.html","id":"mosaic-plot-2","chapter":"127 Visualization in R v.s. Python","heading":"127.8 Mosaic Plot","text":"R:Python:","code":"\nmosaic(`race/ethnicity`~ sex , df_r2, direction = c(\"v\", \"h\"))"},{"path":"understand-data-with-plots.html","id":"understand-data-with-plots","chapter":"128 Understand data with plots","heading":"128 Understand data with plots","text":"Xinyue Gui Wen ZhanExploratory Data Analysis (EDA) vital many data science related projects. Identifying trends patterns datasets initial crucial step building machine learning models, since helps us better understand data. blog (https://github.com/W3n2han/plots_python_seborn_matplotlib/blob/main/Understand%20Data%20with%20Plots.pdf), find frequently used graphs plots, summary use-cases example codes (language used Python). goal blog help quickly identify plot best satisfy needs, helping save time EDA procedure.can also see pdf code ‘resources/plots_python_seaborn_matplotlib’.","code":""},{"path":"r-and-python-visualization.html","id":"r-and-python-visualization","chapter":"129 R and Python visualization","heading":"129 R and Python visualization","text":"Gilberto Garcia PerezThese R libraries using plotting intermediate tasks.Correspondingly, Python packages use plotting intermediate tasks.","code":"\n# Required R packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(vcd)\nlibrary(RColorBrewer)\nlibrary(GGally)# Required Python libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm"},{"path":"r-and-python-visualization.html","id":"introduction-28","chapter":"129 R and Python visualization","heading":"129.1 Introduction","text":"document aims provide quick reference guide different data visualization options available R Python, way people familiarized one languages can easily find right syntax making specific kind plot language. manner, hope experienced programmers may benefit levering knowledge language reduce learning curve related syntactic differences, well identify limitations one encounters environment.Given widespread use ggplot2 library R Language, focus entirely , reader know plots can also generated Base R, although probably lot work. regards Python, making extensive use two related packages, matplotlib seaborn. former widely package used plotting Python, latter improving everyday generates nice looking lines code, just ggplot2 R universe. Nonetheless, matplotlib suited people wish control specific aspects charts.list meant exhaustive, many plots libraries exist language generate . course, needed, one can resort base R matplotlib create , take many lines code scope reference guide.Finally, order clutter much syntactic aspects code, refrain customizing lot plots (.e. adding many labels, titles, colors, minor aspects).","code":""},{"path":"r-and-python-visualization.html","id":"histograms-1","chapter":"129 R and Python visualization","heading":"129.1.1 Histograms","text":"worth noting can call variables R Python viceversa, functionality exploiting generate plots environments data.","code":"\n# R code\nset.seed(5) # set seed for reproducibility purposes\n\n# We generate a normal random sample\nnormal_sample <- data.frame(rnorm(1e3, mean=10, sd=2)) %>%\n  setNames(c('x'))\n\nnormal_sample %>% \n  ggplot(aes(x=x)) +\n  geom_histogram(bins = 30, closed='left') +\n  labs(x='Normal random sample',\n       y='Count',\n       title='Example of histogram') +\n  theme(plot.title = element_text(hjust = 0.5)) # Centers title# Python code\nnormal_sample = r.normal_sample # converts R dataframe to Python dataframe\nsns.reset_orig()\nplt.clf()\nplt.hist(normal_sample['x'], bins=30);\nplt.xlabel('Normal random sample')\nplt.ylabel('Count')\nplt.title('Example of histogram')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"boxplots","chapter":"129 R and Python visualization","heading":"129.1.2 Boxplots","text":"","code":"\n# R code\ndata(mtcars)\n\nmtcars %>% \n  ggplot(aes(x=as.factor(cyl), y=mpg)) + \n    geom_boxplot() + \n    labs(x='cyl',\n         y='mpg',\n         title='Example of boxplots') + \n    theme(plot.title = element_text(hjust = 0.5))# Python code\nmtcars = r.mtcars\nsns.set_style(\"darkgrid\")\nplt.clf()\nsns.boxplot(x=\"cyl\", y=\"mpg\", data=mtcars)\nplt.xlabel('cyl')\nplt.ylabel('mpg')\nplt.title('Example of boxplots')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"q-q-plots","chapter":"129 R and Python visualization","heading":"129.1.3 Q-Q plots","text":"","code":"\n# R code\nnormal_sample <- rnorm(1e3, 10, 2)\nqqnorm(normal_sample)\nqqline(normal_sample, col = \"red\")# Python code\nnormal_sample = np.array(r.normal_sample) # Converts it to numpy array\nsns.reset_orig()\nplt.clf()\nsm.qqplot(normal_sample, fit=True, line=\"45\")\nplt.title('Normal Q-Q Plot')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"bar-plots","chapter":"129 R and Python visualization","heading":"129.1.4 Bar plots","text":"Python plot, summarize data R routine automatically.","code":"\n# R code\ndata(\"Titanic\")\ntitanic <- as.data.frame(Titanic)\n\ntitanic %>% \n  ggplot(aes(x = Class, y = Freq)) + \n  geom_col() +\n  labs(title='Example of bar plot') + \n  theme(plot.title = element_text(hjust = 0.5)) # Centers title# Python code\ntitanic = r.titanic\ntitanic_sum = titanic.groupby('Class').sum().reset_index()\n\nsns.set_style(\"darkgrid\")\nplt.clf()\nsns.barplot(x=\"Class\", y=\"Freq\", data=titanic_sum, ci=None)\nplt.title('Example of bar plot')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"cleveland-dot-plots-1","chapter":"129 R and Python visualization","heading":"129.1.5 Cleveland dot plots","text":"knowledge, exist package Python universe generates Cleveland dot plots just geom_point . Fortunately, can modify dataframe use another function seaborn package create plots.","code":"\n# R code\ndata(seattlepets, package = \"openintro\")\nseattlepets <- seattlepets %>%\n  mutate(species = factor(species)) %>%\n  filter(species == \"Dog\") %>%\n  filter(!is.na(animal_name)) %>%\n  group_by(animal_name) %>%\n  summarize(freq = n()) %>%\n  top_n(freq, n=20) %>%\n  slice(1:20)\n\nggplot(seattlepets, aes(x = freq, y = fct_reorder(animal_name, freq))) +\n  geom_point(size = 3, color = \"SteelBlue\") +\n  labs(x=\"Count\",\n       y = \"Name\",\n       title=\"Example of Cleveland dot plot\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank()) +\n  theme(plot.title = element_text(hjust = 0.5))# Python code\nseattlepets = r.seattlepets\nseattlepets.sort_values(by='freq', ascending=False, inplace=True)\n\nplt.clf()\nax = sns.catplot(x=\"freq\", y=\"animal_name\", data=seattlepets, color='b')\nplt.title('Example of Cleveland dot plot')\naxes = plt.gca()\naxes.yaxis.grid(True)\naxes.xaxis.grid(False)\nplt.tight_layout()\nplt.show()"},{"path":"r-and-python-visualization.html","id":"scatter-plot-3","chapter":"129 R and Python visualization","heading":"129.1.6 Scatter plot","text":"","code":"\n# R code\ndata(\"iris\")\n\nscatter <- ggplot(data=iris, aes(x = Sepal.Length, y = Sepal.Width)) \nscatter + geom_point(aes(color=Species)) +\n  xlab(\"Sepal Length\") +  ylab(\"Sepal Width\") +\n  ggtitle(\"Example of scatter plot\") +\n  theme(plot.title = element_text(hjust = 0.5))# Python code\niris = r.iris\n\nplt.clf()\nsns.scatterplot(data=iris, x=\"Sepal.Length\", y=\"Sepal.Width\", hue=\"Species\")\nplt.title('Example of scatter plot')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"scatter-plot-with-kernel-density-estimation","chapter":"129 R and Python visualization","heading":"129.1.6.1 Scatter plot with kernel density estimation","text":"","code":"\n# R code\ndata(\"faithful\")\n\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n geom_point() +\n geom_density_2d() +\n xlim(0.5, 6) +\n ylim(30, 110) +\n ggtitle(\"Example of scatter plot with contour lines\") +\n theme(plot.title = element_text(hjust = 0.5))# Python code\nfaithful = r.faithful\n\nplt.clf()\nsns.kdeplot(data=faithful, x=\"eruptions\", y=\"waiting\", color='r')\nplt.scatter(data=faithful, x=\"eruptions\", y=\"waiting\")\nplt.title('Example of scatter plot with contour lines')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"scatter-plot-with-hex-heatmap","chapter":"129 R and Python visualization","heading":"129.1.6.2 Scatter plot with hex heatmap","text":"","code":"\n# R code\ndata(\"faithful\")\n\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n geom_hex() +\n scale_fill_gradientn(colors = brewer.pal(3,\"Blues\")) +\n ggtitle(\"Example of scatter plot with hex heatmaps\") +\n theme(plot.title = element_text(hjust = 0.5))# Python code\nsns.reset_orig()\nplt.clf()\nplt.hexbin(faithful['eruptions'], faithful['waiting'], gridsize=(25,25), cmap=plt.cm.Blues)\nplt.xlabel('eruptions')\nplt.ylabel('waiting')\nplt.title('Example of scatter plot with hex heatmaps')\nplt.show()"},{"path":"r-and-python-visualization.html","id":"scatter-matrix","chapter":"129 R and Python visualization","heading":"129.1.7 Scatter matrix","text":"","code":"\n# R code\nggpairs(iris[,1:4])# Python code\nplt.clf()\nsns.set_style(\"darkgrid\")\ng = sns.PairGrid(iris, diag_sharey=False);\ng.map_lower(sns.scatterplot);\ng.map_upper(sns.kdeplot);\ng.map_diag(sns.kdeplot);\nplt.tight_layout()\nplt.show()"},{"path":"r-and-python-visualization.html","id":"heatmaps-1","chapter":"129 R and Python visualization","heading":"129.1.8 Heatmaps","text":"Python function include option appropriately re-scale data. Therefore, forced make changes directly dataframe. Additionally, chose similar colormaps, identical, something clear different coloring maps. can also observe Python routine inverted labels y-axis.","code":"\ndata <- as.matrix(mtcars)\nheatmap(data, Colv = NA, Rowv = NA, scale=\"column\",\n        col= colorRampPalette(rev(brewer.pal(10, \"Blues\")))(25))plt.clf()\nmtcars_norm = (mtcars - mtcars.min())/(mtcars.max() - mtcars.min())\nsns.heatmap(mtcars_norm, cmap='Blues')\nplt.tight_layout()\nplt.show()"},{"path":"r-and-python-visualization.html","id":"final-thoughts","chapter":"129 R and Python visualization","heading":"129.2 Final thoughts","text":"First , learned integrating Python code R Markdown file straightforward initially thought. took lot time configure R library charge loading Python kernel. due fact loading correct Python version, consequence able use packages already installed usual Python environment.spite , R Markdown files remarkable job integrating embedding Python code graphics single file. particular interest, learned call variables environments, capability exploit future projects. One can seamlessly switch data back forth environments, way can decide use library package achieve best results specific task.Finally, consider ggplot2 library much developed counterpart Python universe, seaborn package. reflected quality plots can generate different customization availible ggplot2. However, Python community improving seaborn package rapidly think increasingly achieve capabilities ggplot2 library.","code":""},{"path":"data-visualization-in-python-vs-r.html","id":"data-visualization-in-python-vs-r","chapter":"130 Data Visualization in Python vs R","heading":"130 Data Visualization in Python vs R","text":"Xingyu Wei","code":""},{"path":"data-visualization-in-python-vs-r.html","id":"description-2","chapter":"130 Data Visualization in Python vs R","heading":"130.1 Description:","text":"show plot graphs Python compare data visualization Python (‘Matplotlib.pyplot’ & ‘Seaborn’) R (‘ggplot2’), illustrated example. use data set types graphs show code output differences Python R.","code":""},{"path":"data-visualization-in-python-vs-r.html","id":"python-part","chapter":"130 Data Visualization in Python vs R","heading":"130.2 Python part:","text":"Link: https://github.com/xxxxy9/5702CC","code":""},{"path":"data-visualization-in-python-vs-r.html","id":"r-part","chapter":"130 Data Visualization in Python vs R","heading":"130.3 R part:","text":"","code":"\n# install.packages(\"jsonlite\", repos=\"https://cran.rstudio.com/\")\nlibrary(\"jsonlite\") # must be installed from source\nlibrary(ggplot2) # For visualization"},{"path":"data-visualization-in-python-vs-r.html","id":"data-1","chapter":"130 Data Visualization in Python vs R","heading":"130.3.1 Data:","text":"Source: https://datahub.io/machine-learning/iris#data","code":"\n# Clean the environment\nrm(list=ls())\n\njson_file <- 'https://datahub.io/machine-learning/iris/datapackage.json'\njson_data <- fromJSON(paste(readLines(json_file), collapse=\"\"))\n\nfor(i in 1:length(json_data$resources$datahub$type)){\n  if(json_data$resources$datahub$type[i]=='derived/csv'){\n    path_to_file = json_data$resources$path[i]\n    df <- read.csv(url(path_to_file))\n  }\n}\n\ndf['index']<-1:150"},{"path":"data-visualization-in-python-vs-r.html","id":"the-same-five-graphs-as-python-part-section-1.4","chapter":"130 Data Visualization in Python vs R","heading":"130.3.2 The same five graphs as Python Part, Section 1.4","text":"Heatmap\nSource: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software--data-visualizationScatterpot including 3 variables\nuse color size scatter point represent two variablesRegression graph multiple regression linesMultiple plots sharing axesBoxplot \nSource: https://www.r-graph-gallery.com/265-grouped-boxplot--ggplot2.html","code":"\n#create correlation matrix\nlibrary(reshape)\ncormatrix <- round(cor(df[1:4]),2)\nmelted_cormat <- melt(cormatrix)\n\n#plot heatmap\nggplot(data = melted_cormat, aes(x=X1, y=X2, fill=value)) + \n  geom_tile()+\n   scale_fill_viridis_c(alpha = 1) +\n  theme(axis.text.x = element_text(angle=30, vjust=0.6))+ggtitle(\"Correlation Matrix\")\nggplot(df, aes(x=index, y=sepallength, color=sepalwidth)) + geom_point(aes(size = petallength), alpha=0.7) + ggtitle(\"epal length for each iris plant \")+ scale_color_gradient(low=\"blue\", high=\"yellow\")+ theme_bw()\ntheme_set(theme_bw())\ng <- ggplot(data=df, aes(x=sepallength, y=sepalwidth, color=class)) + \n  geom_point(aes(shape=class), size=1) + xlab(\"Sepal Length\") + ylab(\"Sepal Width\") + \n  ggtitle(\"Iris Species classification\")\n\n# generalised additive model\ng + geom_smooth(method=\"gam\", formula= y~s(x, bs=\"cs\"))\np <-ggplot(df, aes(x=index)) +    \n    geom_line(aes(y = sepallength), color = \"blue\") +\n    geom_line(aes(y = petallength), color = \"orange\") + \n    xlab(\"ID\") + ylab(\"Length\")+\n    ggtitle('Sepal Length and Petal Length for each iris plant')\n\np + theme(\nplot.title = element_text(color=\"navy\", size=10, face=\"bold\"),\naxis.title.x = element_text(color=\"navy\", size=10, face=\"bold\"),\naxis.title.y = element_text(color=\"navy\", size=10, face=\"bold\")\n,legend.title = element_blank()) \np1<- ggplot(df, aes( y=sepallength, fill=class))+ \n    geom_boxplot() +\n    facet_wrap(~class)+\n    ylim(c(0,8))\np2<- ggplot(df, aes( y=petallength, fill=class))+ \n    geom_boxplot() +\n    facet_wrap(~class)+\n    ylim(c(0,8))\nlibrary(ggpubr)\nggarrange(p1, p2, widths = c(4,4), labels = c('a', 'b'))"},{"path":"python-visualization-tutorial-1.html","id":"python-visualization-tutorial-1","chapter":"131 Python Visualization Tutorial","heading":"131 Python Visualization Tutorial","text":"Zixiang Tang(zt2292) & Chenxi Jiang(cj2706)task create Jupyter notebook plot tutorial. tutorial mainly focuses plotting commonly used graphs visualization purposes. tutorial includes graphs learned EDAV classes.Tutorial can found via https://github.com/shawntzx/Python_Visualizaiton_Tutorial-EDAV_CC_Group15-.git","code":""},{"path":"introduction-to-analytics-consulting-at-accenture.html","id":"introduction-to-analytics-consulting-at-accenture","chapter":"132 Introduction to analytics consulting at Accenture","heading":"132 Introduction to analytics consulting at Accenture","text":"Gabrielle NyirjesyI conducted zoom session Analytics Consulting shared learned 5 years working Accenture. topics discussed included types Data Science problems, working real-world data, considerations delivering solutions, team structure career progression. presentation can found following link: https://drive.google.com/drive/folders/1DH9pn7gAZpjYXgjMJQwFwBbvTYCnan0K?usp=sharing","code":""},{"path":"map-of-the-r-world.html","id":"map-of-the-r-world","chapter":"133 Map of the R world","heading":"133 Map of the R world","text":"Guangyu Wu (gw2415)newcomer R world, created map help people like navigate rich ecosystem R packages. Popular packages mapped onto 2D 3D space based semantic information colored auto-generated functionality groups.Please visit project page https://github.com/TimWGY/r_world high-resolution map interactive visualization, well methodology documentation.","code":""},{"path":"alluvial-diagrams-and-their-implementation-using-ggalluvial-in-r.html","id":"alluvial-diagrams-and-their-implementation-using-ggalluvial-in-r","chapter":"134 Alluvial diagrams and their implementation using GGalluvial in R","heading":"134 Alluvial diagrams and their implementation using GGalluvial in R","text":"Arnav Saxena (as6456)blog, ’ve talked alluvial diagrams. basic idea focus introducing concept alluvial plots audience unaware . contains following questions:alluvial diagram?Use cases alluvial diagramComponents alluvial diagramHow implement using ggalluvial package R?Sources building diagrams using Python MS ExcelAlluvial diagrams implementation using GGalluvial R","code":""},{"path":"building-a-dashboard-in-r-for-data-analysis-and-visualization-using-shiny-package.html","id":"building-a-dashboard-in-r-for-data-analysis-and-visualization-using-shiny-package","chapter":"135 Building a Dashboard in R for Data Analysis and Visualization using shiny package","heading":"135 Building a Dashboard in R for Data Analysis and Visualization using shiny package","text":"Srividya InampudiI written blog medium regarding shiny package R Community Contribution assignment. Please find link attached belowlink: https://medium.com/@isrivi/building--dashboard--r--data-analysis--visualization-using-shiny-package-5d864732563fThis Blog go basic steps creating Dashboard using shiny package R. Dashboard can used Visualize Analyze different Datasets.","code":""},{"path":"rgl-package.html","id":"rgl-package","chapter":"136 RGL Package","heading":"136 RGL Package","text":"Nishi Modi Vishwas Reddy ThunikiRGL package data visualization device system R. uses OpenGl WebGL backend. provides R programming interface along interactive viewpoint navigation. contains low level structure high level graphic commands.RGL Graphical Elements:\n• various primitive shapes “points3d”, “lines3d”, “segments3d”, “quads3d” add graphical elements currently active plot.\n• various constructed shapes “text3d”, “arc3d”, “planes3d”, “pch3d” construct objects primitives.\n• “Light3d” function help controlling position characteristics light. helps improving appearance object different backgrounds.\n• “texture” function helps changing texture polygon enhance look particular code.\n• Font can changed using “rglFonts” low level function “rglExtraFonts” high level function.RGL?\nRGL provides real time 3D plots using medium high level functions. created produce 3D models easily R. listed elements RGL explained detailed talk examples. can help everyone improving visualization effects code. several advanced features well try highlight talk.","code":""},{"path":"github-initial-setup.html","id":"github-initial-setup","chapter":"137 Github initial setup","heading":"137 Github initial setup","text":"Joyce Robbins","code":""},{"path":"github-initial-setup.html","id":"create-new-repo","chapter":"137 Github initial setup","heading":"137.1 Create new repo","text":"Create new repository copying template: http://www.github.com/jtr13/cctemplate following instructions README.","code":""},{"path":"github-initial-setup.html","id":"pages-in-repo-settings","chapter":"137 Github initial setup","heading":"137.2 Pages in repo settings","text":"Change source gh-pagesMay trigger GHA get work","code":""},{"path":"github-initial-setup.html","id":"add-packages-to-description-file","chapter":"137 Github initial setup","heading":"137.3 Add packages to DESCRIPTION file","text":"Need better process…Downloaded submissions CourseWorksCreate DESCRIPTION file. Add add dependencies projthis::proj_update_deps()https://twitter.com/ijlyttle/status/1370776366585614342Add Imports real DESCRIPTION file.Found problematic packages looking reverse dependencies packages failed install:devtools::revdep()Also used pak::pkg_deps_tree()Problems:magickrJava dependency qdap","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"tutorial-for-pull-request-mergers","chapter":"138 Tutorial for pull request mergers","heading":"138 Tutorial for pull request mergers","text":"","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"general","chapter":"138 Tutorial for pull request mergers","heading":"138.1 General","text":"following checklist steps perform merging pull request. point, ’re sure , request review one PR leaders.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-branch","chapter":"138 Tutorial for pull request mergers","heading":"138.2 Check branch","text":"PR submitted non-main branch.PR submitted main branch, provide instructions fix problem:Close PR.Close PR.Follow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesFollow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesIf trouble 2., delete local folder project, delete fork GitHub, start .trouble 2., delete local folder project, delete fork GitHub, start .Open new PR.Open new PR.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"examine-files-that-were-added-or-modified","chapter":"138 Tutorial for pull request mergers","heading":"138.3 Examine files that were added or modified","text":"ONE .Rmd file.ONE .Rmd file.additional resources resources/<project_name>/ folder.additional resources resources/<project_name>/ folder.files root directory besides .Rmd file.files root directory besides .Rmd file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-filename","chapter":"138 Tutorial for pull request mergers","heading":"138.4 Check .Rmd filename","text":".Rmd filename words joined underscores, white space. (Update: need branch name.).Rmd filename can contain lowercase letters. (Otherwise filenames sort nicely repo home page.)","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-file-contents","chapter":"138 Tutorial for pull request mergers","heading":"138.5 Check .Rmd file contents","text":"file contain YAML header --- line.first line start single hashtag #, followed single whitespace, title.second line blank, followed author name(s).additional single hashtag headers chapter. (, new chapters created.)hashtag headers followed numbers since hashtags create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading.file contains setup chunk .Rmd file, contain setup label. (bookdown render fail duplicate chunk labels.)\n.e. use {r, include=FALSE} instead {r setup, include=FALSE}.\nSee sample .RmdLinks internal files must contain resources/<project_name>/ path, : ![Test Photo](resources/sample_project/election.jpg)file contain install.packages(), write functions, setwd(), getwd().’s anything else looks odd ’re sure, assign jtr13 review explain issue.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"request-changes","chapter":"138 Tutorial for pull request mergers","heading":"138.6 Request changes","text":"problems checks listed , explain pull request merged request changes following steps:, add changes requested label pull request.job pull request done now. contributors fix requests, review either move forward merge explain changes still need made.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-the-pull-request","chapter":"138 Tutorial for pull request mergers","heading":"138.7 Merge the pull request","text":"good go, ’s time merge pull request. several steps.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-chapter-filename-to-_bookdown.yml-in-prs-branch","chapter":"138 Tutorial for pull request mergers","heading":"138.7.1 Add chapter filename to _bookdown.yml in PR’s branch","text":"access PR branch:Make sure PR branch checking PR branch name shown (main):Open _bookdown.yml file.Open _bookdown.yml file.delete everything file beginning rmd_files: [ add name new file single quotes followed comma:delete everything file beginning rmd_files: [ add name new file single quotes followed comma:? easier fix merge conflicts way. (better way merge main PR branch adding new file can’t done GitHub. ’s interest explain locally.)Save edited version.Save edited version.Click resolve conflicts button:Click resolve conflicts button:Cut new filename paste proper location. delete lines <<<<<<< xxxx, ======= >>>>>>>> main. short, file look correct ’re done. Click “Marked resolved” button green “Commit merge” button.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"pr-leaders-only-add-part-names-to-.rmd-for-every-first-article-in-part","chapter":"138 Tutorial for pull request mergers","heading":"138.7.2 PR Leaders only: Add part names to .Rmd for every first article in part","text":"adding first chapter PART.every first article part, add chapter name top .Rmd file, propose changes. example like .\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-pr-and-leave-a-comment","chapter":"138 Tutorial for pull request mergers","heading":"138.7.3 Merge PR and leave a comment","text":"Now comes final step.’re sure things correctly, assign one PR merge leaders @jtr13 review merge PR.Go back conversation tab pull requests page, example:https://github.com/jtr13/cc20/pull/23#issuecomment-728506101Leave comments congratulations 🎉 (type :tada:) click green button merge.\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-updated-version","chapter":"138 Tutorial for pull request mergers","heading":"138.7.4 Check updated version","text":"successful merge means addition file files added project merge conflicts. mean book render deploy GitHub pages without issues. merge, take 5-10 minutes GitHub Actions render book deploy updated version. ’s problem notified email address . words, job done. However ’re interested, can check progress clicking Actions top repo.","code":""}]
